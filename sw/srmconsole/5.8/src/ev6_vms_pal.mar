;+
; ev6_vms_pal.mar
;-
	.title	"EV6_VMS_PAL"		; informational only
	.ident	"V1.84"			; informational only
;+
; Last Edit:	14-Aug-00
;
; Edit History
;
; Who	Rev	When		What
; ---	---	----		----
; ES	1.0	14-Jun-96	Put MCES into p_misc shadow.
; ES		17-Jul-96	Switch to r4-r7, r20-r23 shadow set.
;				Unalign store -- Fix turning off shadow mode.
; ES		24-Jul-96	MFPR_ASN -- Fix cleaning ASN.
; ES		30-Jul-96	SWPPAL -- First pass. Added PAL__ENTER_OSF.
;		01-Aug-96	POWERUP and WAKEUP -- Separate them.
;				Use PAL__TEMPS_BASE for pal temps base.
;				Use trap__lock_cell for clearing lock. 
; ES		07-Aug-96	Move ALIGN_CACHE_BLOCK before sys__mchk.
;				Fix loop control at sys__mchk_fetch_cbox
;					and sys__crd_fetch_cbox.
;				In trap__save_float, add NOPs to separate
;					mtpr and ftoit instructions
;				In trap__reset, add NOPs to seperate
;					mtpr and addt/mult instructions
; ES		13-Aug-96	In trap__foe, we don't need to check TNV,
;					since TNV check happened already.
; ES		15-Aug-96	In call_pal__swppal, use shadow registers
;					instead of r2 and r3, and copy r16
;					to p5 for address flow.
; ES		20-Aug-96	In trap__mt_fpcr, delete extra NOP.
;				In trap__pal_os_bugcheck, and
;					trap__pal_exc_bugcheck, merge sll.
; ES		11-Sep-96	In sys__enter_console, add more code.
;				In trap__save_state, take out pal base write,
;					as enter_console does it.
;				In trap__reset, form pal base more safely.
;				Move trap__wakeup before free code.
;				In trap__update_pcb..., fix a label.
; ES		17-Sep-96	In save and restore state, add VPTB.
;				In default call_pal entry, added PVC_VIOLATE.
; ES		18-Sep-96	Added reference_platform conditional.
;				In call_pal__swppal, add a PVC_VIOLATE.
;				Added trap__cserve and sys__cserve, putting in
;					cserve__start and cserve__callback.
;				In pal__restore_state, fix typo, so that
;					pal mode is or'ed in when turning
;					off shadow mode.
; ES		24-Sep-96	In pal__restore_state, when setting up
;					DTB_ASNx, don't trash r1.
;				Eliminate PT__M_CTL and PT__DC_CTL. Work
;					only from CNS__M_CTL and CNS__DC_CTL.
;					It's simpler and fixes save/restore bugs.
;				Add some PVC_VIOLATEs for hw_jmp_stalls.
; ES		25-Sep-96	In trap__interrupt, streamline checks.
; ES		27-Sep-96	In trap__opcdec, put NOPs in first first block
;					to avoid PVC restriction on traps.
;				In trap__interrupt, avoid cbr in 1st fetch block.
;				In trap__mt_fpcr, make 1st fetch block clean.
; ES		30-Sep-96	In trap__itb_miss, avoid trappable instructions
;					out of 1st fetc block.
;				In call_pal__amovrx, need PVC_JSR labels.
; ES		09-Oct-96	In trap__unalign, check for unaligned_from_pal
;					before saving scratch registers.
; ES		17-Oct-96	Added queue instructions.
; ES		24-Oct-96	In trap__arith, need to check for fpcr update
;					for integer overflow.
; ES		01-Nov-96	Some interrupt work. Added IPL_DEFS.
;				Added explicit icache flushes as ITB_IA and
;					ITB_IAP no longer flush the icache.
;					Affects MTPR_TBIA, MTPR_TBIAP, IMB.
;				In sys__crd, use correct register to or
;					mchk code into p_misc. And use
;					MCES__PCE__S to test second error.
;				Re-arranged ev6_vms_system_pal.mar.
; ES		07-Nov-96	In sys__cflush, add the code.
; ES		12-Nov-96	In powerup, clear the fpcr to make the
;					simulator happy.
; ES		27-Nov-96	Re-worked machine check handling in
;					ev6_vms_pal.mar, ev6_vms_system_pal.mar.
; ES		02-Dec-96	Add MTPR_PERFMON.
; ES		04-Dec-96	In ev6_vms_callpal.mar, don't trash r17
;					in call_pal__perfmon_wr_1.
; ES		09-Dec-96	Add ev6_p1, ev6_p2.
; ES		12-Dec-96	In ev6_vms_pal.mar, IMPORTANT CHANGE!!!
;					ev6__i_ctl<single_issue_l>
;						is really
;					ev6__i_ctl<single_issue_h>.
;				To avoid ISP/Behavorial mismatch on
;					hw_mfpr EV6__VA
;					when synching for asnx and isx, do a
;					hw_mfpr <EV6__PAL_BASE ! ^xF0>,
;					which scoreboards 4-7.
; ES		18-Dec-96	Re-worked reset. Moved more to the system
;					palcode. Need to do exact mapping
;					sequence with source map enable.
;					See SROM for retirator macro.
; ES		09-Jan-97	Added scoreboard bit 4 for i_stat.
; ES		22-Jan-97	Removed MAP_SHADOW_REGISTERS. See SROM
;					for use of this macro.
;					IMPORTANT: It is assumed that the
;					SROM does a MAP_SHADOW_REGISTERS!
; ES		06-Feb-97	Fixed PVC violations.
;				(1) In powerup, separate mt_fpcr from mxpr.
;				(2) In powerup, pvc$violate 21 on asn clear.
;				(3) In trap__pal_mm_dispatch, pvc_violate
;					on jmp to special handler.
;				(4) In sys__cbox, 1006. Also use hw_ret.
;				(5) In sys__mchk_scrub, pvc_violate 1006.
;				(6) In pal__save_state, pvc_violate 12.
;				(7) In pal__save_state, use hw_ret.
;				(8) In pal__restore_state, use hw_ret.
;				(9) In trap__unalign, pvc_violate 29.
;				(10)In illegal call_pal, use hw_jmp.
;				(11)In swppal, use hw_jmp.
; ES		11-Feb-97	In wrperfmon, zap low nibbles of counts for p1. 
; ES	1.1	24-Feb-97	(1) Start version numbers.
; 				(2) In restore state, restore the vptb portion
;					of I_CTL and VA_CTL from CNS__VPTB.
;					Also restore PT__VA_CTL.
;				(3) In sys__enter_console, Go to 48-bit mode
;					in VA_CTL so that 1-1 console can
;					access I/O.
;				(4) Add dtbm_double_4.
;				(5) In pal__restore_state, write i_ctl
;					twice to cause a stall in case
;					we are toggling 48-bit mode bit.
; ES	1.2	19-Mar-97	Floating point emulation and other p1 stuff.
;				(1) Put IPL table at ^x0D00. Change
;					ev6_vms_callpal.mar to use ipl_offset
;					instead of ipl_table_base, which is
;					a consistent name with the rest of
;					the code.
;				(2) FEN -- add emulation code.
;				(3) OPDEC -- work out jump and merge back.
;				(4) ARITH -- skip fpcr update.
;				(5) QUEUE -- change '17' to 'r17'.
;				(6) CALL_PAL_00AC -- to return to pal mode.
;				(7) MTPR_FEN -- save state in FPE_STATE.
;				(8) MFPR_FEN -- get state from FPE_STATE.
;				(9) CLRFEN -- save state in FPE_STATE.
;				(10)RESET -- skip fpcr init, set FPE_STATE=1.
;				(11)SWPCTX -- save FPE_STATE.
;				(12)Added fault and alignment handling.
;
;				In sys__reset and sys__exit_console,
;					use ldq instead of stq to
;					clear lock. Eliminate hw_ret_stall
;					that is unnecessary.
;				In sys__reset, don't set dc_ctl<dcdat_err_en>
;					in p1.
;				In sys__reset, put an mb (pvc restriction 28
;					before the ldq that clears the lock
; ES	1.3	02-Apr-97	(1)Bug in trap__fen. Forgot to branch to code
;					which choses between opcdec and fen.
;				(2)In trap__pal_mm_dispatch, zap the offset
;					calculation to avoid sign extension.
;					This is a p1 change, as the offsets
;					are smaller without the fp
;					emulation code.
;				(3)Change 'trap' labels in call_pal__post_xm
;					to 'call_pal'
;				(4) Implement new palcode restriction,
;					separating retires of flush and
;					hw_ret_stall.
;				(5) When doing a hw_jmp_stall, combine offset
;					calculation and or-ing of palmode bit.
;				(6) Anywhere we write DTB_TAGx/DTB_PTEx, make
;					sure no scoreboard bits are set
;					before their issue.
;				(7) In sys__exit_console, sys__enter_console,
;					refine jmp_stall, scoreboard strategies.
;				(8) In call_pal__rei, fix pvc #4 in kernel
;					to non-kernel flow. Separate PS and
;					PAL_BASE mxprs.
; ES	1.4	08-Apr-97	(1) In pass1, dc_ctl<dctag_par_en> must be 0.
;				(2) In sys__cbox, add mb, eliminate mb's of
;					calling routines. Add clearing of
;					i_ctl<sbe> bits and ic_flush to
;					quiet the istream.
;				(3) In trap__d1to1, hold up return until
;					TB write has retired. Used to
;					avoid speculative reads on powerup.
;				(4) In fp emulation, go off to emulator in
;					kernel mode, and use priv call_pal
;					to return to pal mode. Use ^x3B.
;				(5) Change cbox read chain names to match spec.
;				(6) Add PAL_BASE, I_CTL, PROCESS_CONTEXT to
;					mchk logout frame.
;				(7) Add check_interrupt_pending conditional
;					for deasserting interrupts.
; ES	1.5	09-Jun-97	(1) Because of cbox triplicate tags, can
;					not do dc_ctl<flush> in reset. 
;					BIST cleans it anyway.
;				(2) In 1-1 dtb miss single, separate pte and
;					mm_stat writes.
;				(3) In check_interrupt_pending code, need
;					to order ISUM read and PS,IER writes.
;				(4) Re-order mchk logout frame. Add VA_CTL.
; ES	1.6	26-Jun-97	(1) BUG FIX:
;					In sys__crd_second and
;					sys__crd_skip_frame, recover
;					exc_addr from PT__FAULT_PC
;					because p23 has been trashed.
;				(2) Turn icache fill mchks into crd mchks.
;				(3) Turn recoverable dc_tag_perr into crd mchk.
;				(4) Eliminate VA and VA_CTL from mchk frame.
;				(5) Add mchk and mchk_crd revision.
; ES	1.7	01-Aug-97	(1) Add sample tsunami/DP264 clock interrupt
;					clear.
;				(2) On DFAULT for DC_TAG_PERR, need to turn
;					off DCTAG_PAR_EN to avoid a mchk on
;					the ECB.
;				(3) Add sample tsunami/DP264 clear interrupt
;					pending code.
; 				(4) Incorrect displacements in unalign
;					store long and word of second half. 
;				(5) For p1, in restore_state, restore
;					CNS__FPE_STATE from CNS__PCTX to
;					keep it coherent with CNS__PCTX.
;				(6) For system partners that want unix
;					to use VMS enter_console, add
;					restore of PAL_BASE during save_state.
;					Also, eliminate double mm_stat save.
;				(7) Go back to having PT__M_CTL as well as
;					CNS__M_CTL so we have a live and
;					saved version.
;				(8) Add va_48 conditional for xx_CTL__INIT
;					values.
;				(9) Add tb_mb_en conditional.
;				(10)Add mchk_en conditional.
;				(11)Dismiss missing/faulting WH64/ECB.
;				(12)Handle I_CTL<SDE> with 5 fetch blocks
;					instead of jmp_stall to avoid
;					a conjectured race condition.
;				(13)In emulator, load PT_WHAMI off of p_temp,
;					NOT p4.
;				(14)In emulator code, map 4MB region
;					depending on location of emulator.
;					Also make stack space = ^x400.
;				(15)In IMB, add an MB.
;				(16)Eliminate unnecesssary jmp_stall in
;					powerup.
;				(17)Add clearing SIRR in enter_console.
;				(18)In emulator code, do load of instruction
;					physically so that we don't run
;					into FOR problems. And in
;					mm_in_float_emul_instr, eliminate
;					unnecessary restore of r25, r26.
;				(19)Handle FPE with 5 fetch blocks.
;				(20)Now that the PALcode has grown, we
;					need a ZAPNOT in the non-p1
;					mm_dispatch as well.
; ES	1.8	25-Aug-97	(1)Add dcache_set_en conditional. In p1,
;					default to 1. In p2, default to 3.
;					Use in dc_tag_perr processing and
;					reset. Also add an MB in dc_tag_perr.
; ES	1.9	28-Aug-97	(1)In trap__post_km_ps, trap__post_km_r45,
;					and call_pal__post_xm_cont,
;					add a CRITICAL missing
;					bis r31, r31, r31 to
;					finish out the fetch block.
; ES	1.10	11-Sep-97	(1)In sys__reset, initialize x31_EMUL,
;					not x0_EMUL!!!!
;				(2)Changes to DTBM_SINGLE, DTBM_DOUBLE_3
;					and DTBM_DOUBLE_4. Get VA
;					before read of VPTE. Use p7<0>
;					as a double miss flag. On double
;					miss, put mm_stat from p5 into
;					<31:15> of p7, which contains
;					exc_sum and double miss flag.
;					Use p5 for exc_addr of double miss.
;					Restore mm_stat into p5 on
;					invalid_dpte and double3_pte_inv.
;				(2)In ITB_MISS, don't use p5 until
;					after the VPTE fetch.
;				(3)Need CONT_HW_VECTOR on double_4 flow.
;				(4)In double3_pte_inv, save exc_addr to p4
;					so we can restore mm_stat to p5.
;					Later load p20 from p4.
; ES	1.11	17-Sep-97	(1)Bug in emulation code. Have stores load
;					PT__TRAP (erroneously skipped).
;					Fixed by moving up
;					trap__emul_parse label.
; ES	1.12	18-Sep-97	(1)Emulator called in kseg. Need matching
;					emulator built for kseg.
;				(2)Xor in dtb miss before load virtual.
;					Ensures va is in hand.
; ES	1.13	25-Sep-97	(1)Add counter for fp emulates. Modifications
;					in swppal, fen, new call_pal ^xAD, to
;					get count.
;				(2)Pass serial line interrupts off to
;					platform specific call.
;				(3)Take TNV on mm_in_float_emul_instr rather
;					than halting.
;				(4)Handle 1-1 case in emulator when reading
;					instruction.
;				(5)Add a hw_ret after sys__int_err for pvc.
; ES	1.14	08-Oct-97	(1)Change I_CTL__RSV1 to I_CTL__CHIP_ID.
;				(2)In sys__enter_console, update PT__VA_CTL
;				   	to reflect change to VA_CTL, so that
;				   	PALcode running while console is
;					running doesn't have problems
;					flipping modes.
; ES	1.15	10-Oct-97	(1)Add clrmap callpal and conditional.
; ES	1.16	29-Oct-97	(1)Fix typo in remqhilr, so we branch to
;					call_pal__remqhilr_retry instead of
;					call_pal__remqhiqr_retry
;				(2)Put <> around 8192 in cflush to make
;					hal happy (hal will be fixed soon).
; ES	1.17	30-Oct-97	(1)In trap__reset, use r26 when branching to
;					sys__reset, so we don't step on
;					dp264 srom parameters.
;				(2)Add SROM parameter passing for DP264.
;				(3)Change beh_model default to 0.
; ES	1.18	31-Oct-97	(1)In sys__mchk_arrange_cbox, save dc0 to
;					dc0_syndrome, not dc1_syndrome.
;				(2)Save mm_stat in short frame. Before
;					merge to sys__crd_merge, each
;					flow must write MCHK_CRD__MM_STAT.
;					The dc_tag_perr error writes a
;					useful mm_stat, showing the error.
;				(3)On dc_tag_perr, only halt on dc_tag_perr
;					while in pal_mode. Add a new halt
;					code HALT__DC_TAG_PERR_FROM_PAL.
;					On mchk in progress, just dismiss
;					after fixing.
;				(4)In swppal, use a quadword for PAL__ENTER_OSF.
; ES	1.19	04-Nov-1997	(1)In exit_console, make all RWE for
;					load/unlock.
;				(2)Add pvc labels for bsr emul_restore
;					so that pvc doesn't terminate
;					permutation on subroutine return.
; ES	1.20	17-Dec-1997	(1)Update DC_STAT field names.
; 				(2)In exit_console, need to zap sign extension
;					when making all RWE for load/unlock.
; ES	1.21	18-Dec-97	(1)Don't zap low nibble of counters on writing
;					performance counters.
;				(2)In machine flow, move read and store of isum
;					earlier so we only read isum for system
;					machine interrupt once.
;				(3)Put ev6_p1 conditional around MB in double
;					flow.
;				(4)Reverse order of clrmap and ev6_p1
;					conditional in CLRMAP,
;					returning if not ev6_p1, and
;					make CLRMAP = 1 by default.
;				(5)In floating point emulation, use buffer
;					 blocks	instead of jmp_stall when
;					enabling kseg -- avoids possible
;					SPE propagation delay problems.
;				(6)Add some PVC_VIOLATE <35>'s. (HW_INT_CLR)
;					Re-arrange sys__crd_skip_frame a bit.
; ES	1.22	12-Jan-98	(1)In arithmetic exception, ignore SWC bit
;					when checking whether to take an
;					exception.
; ES	1.23	21-Jan-98	(1)Change <ev6_p1 ! ev6_p2> conditional to
;					just ev6_p1 in cbox read chain code.
; ES	1.24	04-Feb-98	(1)Read ISUM twice in interrupt to minimize the
;					possibility of a read/write conflict
;					causing a read of 0.
;				(2)In perfmon, zap counter on disable for
;					both pass1 AND pass2.
;				(3)Add check for double bit error before
;					istream error in mchk.
;				(4)Add check for istream mchk in crd flow.
;				(5)In reset, read the cbox error read chain
;					to clear it out.
;				(6)In reset, comment out clearing fpcr. It's
;					unpredictable anyway, and it's possibly
;					set up by the srom.
;				(7)In sys__mchk and sys__mchk_clear_crd,
;					change r5 to p5 (just a name change).
; ES	1.25	12-Mar-98	(A)Large rewrite of error handling.
;				(1)sys__int_err -- does more logging and then
;					branches to sys__mchk_header.
;				(2)sys__crd
;					(a)Now scrubs under certain conditions.
;						Does it before merge point from
;						630's.
;					(b)Now logs i_stat, dc_stat before merge
;						from 630's. Only clear crd-type
;						errors in dc_stat in case there
;						is a delayed mchk in the wings.
;					(c)Crd from speculative mchk now handled
;						directly in this code.
;					(d)New merge point is sys__crd_header.
;					(e)Dpc and pce flows also scrub
;						as necessary.
;					(f)Post now sets PS<IP>.
;				(3)sys_crd_scrub -- new routine to scrub. Also
;					includes a store to make block dirty.
;				(4)sys__mchk_dc_tag_perr
;					(a)Does more logging before merging at
;						sys__crd_header.
;					(b)Has its own dpc and pce flows.
;				(5)sys__mchk
;					(a)Now logs i_stat, dc_stat before merge
;						from 660's.
;					(b)New merge point is sys__mchk_header.
;					(c)Istream mchk to crd now does more
;						logging before branching to
;						sys__crd_header.
;					(c)Post now sets PS<IP>
;				(6)sys__mchk_scrub -- includes a store to make
;					block dirty.
;				(7)trap__pal_bugcheck -- does more logging
;					before branching to sys__mchk_header.
;				(8)perfmon -- minor change to an ASSUME.
;
;				(B)Turn hw_jmp's, jmp's into hw_ret, ret's to
;				avoid issuing instructions in PALmode on random
;				cache line predictions.
; ES	1.26	31-Mar-98	In sys__crd_scrub, need to save off p7
;					before calling sys__cbox, and then
;					restore on return.
; ES	1.27	29-Apr-98	(1)In trap__save_state and trap__restore_state,
;					don't save and restore pctr_ctl
;					(which causes the counter
;					to increment since we get a
;					post-incremented copy from the chip
;					even if counting is disabled).
;				(2)In trap__fst_table, and trap__fld_table,
;					change repeat 31 to repeat 32!!
;				(3)Add comments and definitions relating to
;					ev6_p3 changes of I_STAT.
; ES	1.28	15-May-98	(1)In double flows, force a stall until
;					the pte writes retire to avoid
;					issue of tag write in single flow
;					before retire of pte write in double
;					flow.
; ES	1.29	18-May-98	(1)In sys__reset, just check for 'DEC', not
;					'DECB'.
;				(2)In reference platform code, add use of
;					WRITE_MANY_CHAIN in an impure area
;					location.
;				(3)In queue and amovrx, add stall after
;					dtb pte write.
; ES	1.30	29-Jun-98	(1)Add sleep mode support (for unix).
;				(2)In arith exception, zap the upper portion
;					of exc_sum so we don't step on
;					fpcr.
; ES	1.31	09-Jul-98	(1)In sys__mchk_scrub, map
;					address and then scrub using
;					ldq_l, stq_c. Also move mb.
;				(2)In sys__mchk_clear_crd, also clear
;					dc_stat, and force ipr writes
;					to finish.
;				(3)In sys__crd, look for a double
;					bit error that may have
;					occurred just after the single
;					bit error.
;				(4)In sys__crd, don't scrub for
;					dstream_dc_err.
;				(5)In sys__crd_merge, also
;					clear dc_stat and force
;					ipr writes to finish.
;				(6)In sys__crd_skip_frame, don't
;					scrub for dstream_dc_err.
;				(7)In sys__crd_skip_frame_merge, also
;					clear dc_stat and force
;					ipr writes to finish.
;				(8)In sys__crd_scrub, map
;					address and then scrub using
;					ldq_l, stq_c. Also move mb.
;				(9)Add pte eco.
; ES	1.32	28-Jul-98	Work around rdblkmodify nxm bug.
; ES	1.33	03-Aug-98	Take out istream kseg hack.
; ES	1.34	06-Aug-98	(1)Add kseg_hack conditional.
;				(2)In enter console, add mm_stat stall at the
;					end to interlock everything.
;				(3)In rei, take out stores to clear lock.
;					Any load does it.
; ES	1.35	25-Aug-98	I'm skipping a number and hope to keep this
;					two in synch from now on.
; ES	1.36	25-Aug-98	Avoid speculative tag overwrite by holding it
;					up until previous PTE write retires.
;					Keeping the tag and PTE write in a
;					fetch block holds up speculative
;					set in the mapper, but add insurance
;					with scoreboard bits. The ALIGN
;					macro is marked with 1.36.
;				Also take cycle counts out of miss flows.
; ES	1.37	27-Aug-98	Keep in synch with unix
; ES	1.38	08-Sep-98	(1)MTPR_PERFMON -- add subfunctions for
;					profileme.
;					Eliminate return status in r0<0>.
;					Add some waits for retires (though it
;					probably doesn't really matter).
;					Change how nibbles are zapped on
;					disable.
; ES	1.39	16-Sep-98	On stack builders, we must avoid the possibility
;				of a late MTPR PTE/TAG retire popping
;				our TB entry out midstream because the
;				stack builders run out of shadow mode part
;				of the time, and we wouldn't be able to
;				recover from a TB miss. So do a scoreboard
;				stall to avoid this possibility (or MB if more
;				convenient). Changes are marked with 1.39.
;				Affects in ev6_vms_pal:
;				trap__post_km_ps, trap__post_km
;				Affects in ev6_vms_callpal:
;				rei, call_pal__post_xm
; ES	1.40	13-Oct-98	There were a number of versions at 1.40. All
;				traces of them have been removed.
; ES	1.41	13-Oct-98	(1) spinlock_hack workaround. Look for 1.41.
;				ev6_vms_pal changes:
;					new conditional spinlock_hack
;					trap__interrupt
;				ev6_vms_callpal changes:
;					swpctx
;					mtpr_ipl
;					perfmon
;					rei
;				ev6_vms_system_pal changes:
;					ipl table
;					sys__interrupt_pc
;					reset
;					i_ctl__init value
;				(2)In sys__reset and sys__enter_console, get
;					ipl31 from ipl table, rather than
;					writing 0 to IER_CM (necessary for
;					spinlock and it's the correct code
;					anyway). Marked with 1.41.
;				(3)In sys__reset, add a hw_int_clr for
;					perf counter restriction. Marked with
;					1.41.
;				(4)In sys__enter_console, add some NOP between
;					ipr writes. Marked with 1.41.
;				(5)New ev6_pal_temps.mar, new ev6 prefix files
;					with spinlock_hack = 1, new ev6_defs.
;				(5)In trap__interrupt_sw_found, added
;					some NOPs between IPR writes. Marked
;					with 1.41.
; ES	1.42	14-Oct-98	(1)Look for 1.42. Fix bug in sys__interrupt_pc,
;					fix potential bug in mtpr_ipl. Add turng
;					off pc spinlock hack when going to unix.
;					Affects all 3 modules.
;				(2)ev6_vms_system_pal.mar -- in reset, save m_ctl
;					into pt__m_ctl!!!! Bug has been there
;					forever.
; ES	1.43	15-Oct-98	Fix bugs in transparent handling.
; ES	1.44	22-Oct-98	(1)In trap__mchk, add some code that helps
;				eliminate double mchk's.
;				(2)In trap__mchk, add fetch of ISUM. Should
;				have added that when mchk was reworked a
;				while back.
;				(3)On istream mchk, check on cmov at pc-4 and
;				punt if so. Bug in EV6 -- the cmov may or
;				may not have been executed.
;				(4)STx_C erroneous succeed problem -- A
;				ldx_l/stx_c sequence with an exception
;				in the middle that doesn't do a virtual
;				operation (which breaks the lock) and that
;				can have unknown bad path code that pulls back
;				in the locked block after it has been
;				pulled away from another processor. Fixed
;				as I100 in ev67, where a hw_ret mispredict
;				will cause the lock to be cleared. Handled here
;				with FORCE_PATH conditional in the
;				following locations:
;					dtb single miss
;					itb single miss
;					dtb double miss
;					interrupt passive release
;					crd passive relesae
;				Fixed with a hw_jmp followed by a
;					infinite loop branch in the
;					same fetch block. None of the
;					various type of predictors will
;					predict other than back to the jmp,
;					allowing no bad path code.
;				(5)Some additional NOPs in enter_console code.
; ES	1.45	04-Nov-98	(1)In ev6_vms_system_pal, change
;					assume_fetch_block <...> to
;					align_fetch_block <...>.
;				(2)In ev6_vms_system_pal, fix pte
;					creation of scrub address.
;				(3)In force_path hack, use pvc_violate <1008>
;					on all but 1 of each branch to
;					sys__cbox and sys__crd_scrub. The
;					pvc label skips the branch on goes
;					on to the next instruction. This will
;					speed up pvc considerably.
; ES	1.46	09-Nov-98	Make the new pvc happy with pvc_violate <1008>
;					sections by enforcing ipr operations
;					in different fetch blocks.
; ES	1.47	19-Nov-98	New force_path2 hack. Leave force_path=0.
;				In perf counter interrupt on force_path=0
;					dismiss, branch to
;					trap__interrupt_dismiss.
;				ITB_MISS -- do a new hack
;					to force mispredict before a load
;					can fire off
;				DTB_MISS -- do the new hack. Seem to be
;					some problems with the unix-style
;					ldbu hack
;				DOUBLE -- do the new hack
;				1to1 mapping cases -- ignore
;				interrupt dismiss -- do the new hack
; ES	1.48	01-Dec-98	In cmov hack in mchk, do a physical lookup
;				to avoid problems with FOR pages.
; ES	1.49	11-Jan-99	(1)Take out extraneous I/O bit line in
;				DTB_MISS.
;				(2)Do a hw_ret, not a hw_jmp in
;				pc spinlock hack.
; ES	1.50	12-Jan-99	(1)On call_pals, ev6 erroneously pushes
;				the prediction stack twice.
;				Add add_extra_ret conditional that
;				adds extra ret to call_pal macro, but
;				leave it turned off. If the stack is not
;				deep, the mispredict that it causes
;				is more harmful. We leave it around for
;				documentation of the problem.
;				(2)Fix comments on 2.3 perfmon.
;				Retired branch mispredicts are not
;				counted.
; ES	1.51	05-Feb-99	Augment 1.49 fix. Do the mulq hack on
;				the hw_ret.
; ES	1.52	12-Feb-99	In spinlock hacks in rei flow, change
;				label call_pal__mtpr_ipl_spin0 to
;				call_pal__rei_spin0 and call_pal__rei_spin1.
;				Without this change, the "check_interrupt_
;				pending code" is not run, but otherwise
;				there should be no other implications.
; ES	1.53	08-Mar-99	(1)New PALcode restriction. On writing any
;				ebox ipr (va_ctl,cc,cc_ctl), it is necessary
;				to force its retire before any close-in-
;				program-order mispredict from a branch,
;				hw_ret_stall, or dtb miss.
;				The younger mispredict can actually stop the
;				ipr write even though the instruction is older
;				and retires.
;				(2)Use ldbu hack in dtb miss instead of mulq.
; ES	1.54	12-Mar-99	Take out ldbu hack. Not ready for prime time.
;				Need to ensure ldbu doesn't issue speculatively.
;				Will be fixed in next release.
; ES	1.55	24-Mar-99	Add an settle tb interlock to stacking code at
;				trap__post_km_r45 and trap__post_km.
; ES	1.56	07-Apr-99	(1)Ldbu hack enhanced once last time.
;				(2)Add a couple of other lines to be more
;				compatible with the non-generic versions.
;				(3)Eliminate hw_jmp in sys__cbox.
; ES	1.57	27-Apr-99	In swppal, in spinlock hack, used p6 instead
;				of r31 in two places where I was creating
;				constants. Depending on value of p6, could
;				clear some bits like mchk_en. Bug found by
;				Dave Mayo.
; ES	1.58	28-Apr-99	Add backwards-compatible ev67 code.
;				With the ev6 workaround conditionals
;				enabled (spinlock_hack and force_path2),
;				the resulting PALcode will work on ev6
;				and ev67.
;
;				*****HOWEVER*****, there should be a separate
;				PALcode build for ev67 which turns off
;				the workarounds not necessary for ev67.
;
;				(1)At trap___conditional_flags, location
;				PALbase + 0x10, there is a flags word
;				where bit<1> indicates spinlock_hack on,
;				and bit<0> indicates force_path2 on.
;				This way, if the cpu is an ev6 and those
;				bits are clear, the console can complain.
;
;				(2)Create a PT__I_STAT_MASK to clean
;				I_STAT appropriately before writing to
;				logout frames. Just do ev6 style w1c since
;				the profileme bits are readonly and are
;				not affected.

;				(3)Delete ev6_p3. Code should continue to
;				be built with ev6_p2 conditional set even
;				on ev67.

;				(4)In bugcheck, change mchk_crd offsets to
;				regular mchk offsets. Long-time bug.

;				(5)Add setting of SMC bits in m_ctl, which
;				turns off speculative stores temporarily
;				on certain conditions. See description
;				in reset flows. (Note that no work is
;				being done in the kseg_hack conditional code).
;				NO ONE SHOULD ASSUME M_CTL HAS JUST
;				THE SPE BITS IN IT ANYMORE!!!!!!!!!!!!!
;				If macros fiddle with M_CTL, they should always
;				RESTORE FROM PT__M_CTL! ANY CODE hacking
;				with CNS__M_CTL or PT__M_CTL BETTER SET the
;				SMC BITS!!! Restore_state always sets the
;				SMC bits if they are 0. If they are non-zero,
;				the SMC bits are left as is.
;				Since these bits have no meaning on ev6,
;				they can be set for both ev6 and ev67, and
;				they MUST BE SET once there is a separate
;				ev67 PALcode!!!!!!!!!!!!!!!!!!!!!!!!!!!
;
;				(6) Add ASSUME kseg_hack eq 0 line.
;
; ES	1.59	28-May-99	In swppal, fix bug introduced by
;				nonzero_console_base macro used by some
;				platforms. Make start of the continuation area
;				compatible with insertion of the macro. Make
;				flow for r16=addr branch to an explicit label
;				rather than assuming what code is at the
;				start of the continuation area.
; ES	1.60	15-Jun-99	Add ebox ipr corruption checker. Conditional
;				is check_ebox_iprs. Make egore conditional
;				default to 0.
; 	1.61	28-Jun-99	Fix 1.60.
; ES	1.62	13-Jul-99	Separate Page Table Structures, ECO 121.
; ES	1.63	19-Aug-99	Rearrange dtb miss flow for applu.
;				Questions to denis foley.
; ES	1.64	30-Aug-99	(1)Fix bug in save_state -- save process
;				context before setting fpe to save fp regs.
;				Make this dismiss and others go through
;				trap__interrupt_dismiss.
;				(2)On crd's, if c_stat is 0, dismiss with
;				mulq hack to avoid crd's on speculative loads.
;				(3)Make other dismisses go through
;				trap_interrupt_dismiss.
; ES	1.65	12-Oct-99	Two bugs found by Stephen Shirron
;				(1) In enter_console, clear ASTRR.
;				(2) Fix bug introduced by 1.64. Save
;				p23 to pal_temp at beginning of sys__crd
;				so restore from the pal_temp at merge
;				point works properly.
; ES	1.66	13-Oct-99	In restore_state, apply restriction
;				described in 1.53 to the va_ctl write.
;				Might be falling over that problem.
; ES	1.67	21-Oct-99	(1) When scrubbing from a c_addr<42:6>, do
;				sign extension from <42> to produce a
;				proper va. This issue for any platform that
;				(2) In various crd skip routines, add
;				branch on c_stat=0 back in to allow
;				system crd routines to merge and use that
;				as a flag not to scrub.
;				uses bit 42 (e.g. wildfire).
; ES	1.68	26-Oct-99	In sys_crd, check for i_stat<par> (affects
;				ev67 only) and dc_stat<ecc_err_st> and don't
;				check for c_stat=0 dismiss case if on of
;				these cases.
;				On ev6, there are no i_stat crd's -- the
;				bug would just keep us from logging
;				dc_stat<ecc_err_st> errors, but that's all.
;				On ev67, we wouldn't clear the i_stat crd, so
;				we would keep getting it, so it is a must
;				fix for ev67.
; ES	1.69	27-Oct-99	In reset, check for ev67/8 chip id by
;				comparing against ^x18 instead of ^x8 to
;				catch new chip ids.
; ES	1.70	28-Oct-99	(1)Switched to platform-partners'
;				ev6_vms_pal.mar.
;				(2)Switched to platform-partners'
;				ev6_vms_callpal.mar. But had to add
;				separate_page_tables call_pal's back in,
;				which were in the generic, but not the
;				platform-partners'.
;				(3)Two restrictions due to applu bug:
;					(a)Fetch block previous to a mtpr must
;					not contain mis-predictable/trappable
;					instructions such as ld/st, cbr,jsr/ret.
;					Under "right" set of stall conditions,
;					scoreboard bits not killed along with
;					the mtpr.
;					(b)It has also been suggested that
;					no killable instructions be in the mtpr
;					block. This restriction is a little
;					more problematic and impossible to
;					follow in the "mulq hack".
;				The general change is a change from hw_mtpr to
;				EV6_MTPR macro with prealign and postalign
;				parameters, whose defaults are to do the
;				alignment. The change is done manually in
;				the miss flows to keep them more obvious.
;
; ES	1.71	11-Nov-99	(1) Fix applu cases caught by new pvc version
;				(still in development)
;				(2) Change ldbu r31 to ldbu p7 in single miss.
;				Avoids possible case of ldbu r31 issuing
;				after a replayed ldx_l instruction and
;				causing a stx_c to incorrectly succeed.
;				See comments there for explanation.
;				LOOK for "1.71(2)"
;
; ES	1.72	03-Dec-99	(1) PVC_VIOLATES as necessary for ev6 PVC #42,
;				required virtual op between exception and ret
;				(2) In sys__cflush, allow for 16MB cache.
; ES	1.73	13-Dec-99	(1) Change applu_fix use in the EV6_MTPR macro.
;				The ev6/ev67/ev68 restriction that there be
;				no trapable instruction in the same fetch
;				block as a MTPR is no longer conditionalized
;				on applu_fix and is solely controlled by the
;				prealign and postalign parameters. The
;				ev6/ev67 restriction that there be a
;				non-trappable fetch block preceding a
;				block with a MTPR is controlled by both
;				the prealign parameter and the applu_fix
;				conditional. If prealign = 1 and applu_fix = 1,
;				the fetch block is added. If only prealign = 1,
;				there is just an align_fetch_block inserted.
;				(2) Change applu_fix in the source code to
;				apply to the additional block restriction only.
;				(3) Another ldbu change. Do a XOR ^x08 of
;				the address to avoid performance hit on
;				same address.
;				(4) In force_path2 = 0 code, add
;				pvc_violate <44> for single cpu ev6
;				systems that do not set force_path2.
;				(5) In egore code (does not affect anyone
;				else), add a scoreboard bit to fulfil the
;				va_ctl write restriction 42.
;				(6) In sys_reset, do a change for va_ctl
;				restriction 42.
; ES	1.74	26-Jan-00	Bug fix and optimization to separate page
;				table code.
; ES	1.75	28-Jan-00	Modify itb_miss to allow for separate page
;				table check in double flow.
; ES	1.76	10-Feb-00	(1) In cflush, use hw_ldl/p instead of hw_ldq/p.
;				The hw_ldq is a "prefetch, evict next", which
;				won't change dcache sets.
;				(2) Since platform partners use the same
;				crd frame for sc and pc errors, don't
;				touch frame if sce is set, i.e., merge
;				with dpc flow if sce is set. Note: the
;				platform-specific code ought to be nice and
;				do the same thing, i.e., act like a DSC if
;				PCE is set.
; ES	1.77	11-Feb-00	(1) Bug fix -- typo in mchk cmov check.
;				Change INT_CMOV_CMP to FP_CMOV_CMP for fp
;				compare.
;				(2) In sys__cbox, restore i_ctl from
;				CNS__I_CTL so that we can preserve SBE value.
;				In reset, save i_ctl to CNS__I_CTL before
;				calling sys__cbox.
; ES	1.78	15-Feb-00	(1) In sys__cbox, restore only the SBE bits
;				from CNS__I_CTL. Get the other bits from
;				the real I_CTL so we preserve vptb field.
; ES	1.79	20-Mar-00	In crd handling, suppress reporting of
;				icache parity error unless more than 1
;				occurs within a clock interrupt interval.
;				(See Denis Foley for an explanation of why
;				we are doing this. In any case, it does
;				no harm to suppress the reporting).
; ES	1.80	28-Mar-00	(1)Incrementally evict the cache on c_stat = 0
;				errors to avoid possible livelock on
;				floating point store errors.
;				(2)In reset, fix test of chip_id for
;				setting i_stat mask.
; ES	1.81	09-May-00	(1)In mchk, clear mchk_d in hw_int_clr so
;				that double halts and mchk_while_in_pal
;				don't fall over it when turning control
;				over to the console.
;				(2)Fix comments on implicitly-written
;				iprs (by request).
; ES	1.82	09-Jun-00	(1)Add p_misc__ev68p2__s for ev68 p2.
;				(2)On istream mchk, if p_misc__ev68p2__s is
;				set, don't check for pc near cmov.
;				(3)Report i_stat<tpe>,<dpe>,<par> errors
;				that are not ev68 p2 <lam> errors. If
;				<lam> error, report depending on value
;				of CNS__REPORT_LAM -- always if set,
;				depending on frequency is clear.
;				(4)IMB eco. Set flag in PCB for non-kernel
;				call_pal IMB.
; ES	1.83	10-Jul-00	Update of chip_id longword in reset.
;				EV67 Pass 2.4.1 is 0x13.
; ES	1.84	14-Aug-00	Version 1.82 undid the frequency check
;				on <PAR>, forgetting that LAM is OR'ed in
;				on some passes of ev67. So add the check
;				back in on ev67.
;-
	vmaj = 1
	vmin = 84
	vms_pal = 1
	osf_pal = 2
	pal_type = vms_pal
	pal_version_l = <<pal_type@16> ! <vmaj@8> ! <vmin@0>>
;
; EV6 hardware definitions
;
	PAL_SHADOW_DEFS
	MCES_DEFS
	MM_DEFS
	MM_STAT_DEFS
	HALT_DEFS
	MCHK_DEFS
	CBOX_IPR_DEFS
;
; VMS definitions
;
	PAL_FUNC_DEFS
	PTE_DEFS
	PAL_P_MISC_DEFS
	PAL_PS_DEFS
	SCB_DEFS
	IPL_DEFS
	PCB_DEFS
	FRM_DEFS
;
; Configuration options
;
.iif ndf ev6_p1, ev6_p1 = 0
.iif ndf ev6_p2, ev6_p2 = 1

ASSUME <ev6_p1+ev6_p2> eq 1

.iif ndf beh_model, beh_model = 0
.iif ndf egore, egore = 0					; 1.60
.iif ndf focus, focus = 0
.iif ndf console, console = 0
.iif ndf srom, srom = 1
.iif ndf reference_platform, reference_platform = 0
.iif ndf pte_eco, pte_eco = 1
.iif ndf check_interrupt_pending, check_interrupt_pending = 0
.iif ndf mchk_on_wakeup, mchk_on_wakeup = 0
.iif ndf nonzero_console_base, nonzero_console_base = 0
.iif ndf galaxy, galaxy = 0
.iif ndf intercept_interrupt_return, intercept_interrupt_return = 0
.iif ndf intercept_change_ipl, intercept_change_ipl = 0
.iif ndf turbo_pcia_intr_fix, turbo_pcia_intr_fix = 0
.iif ndf tsunami_platform, tsunami_platform = 0
.iif ndf tl6_ibox_timeout, tl6_ibox_timeout = 0

.iif ndf va_48, va_48 = 0
.iif ndf tb_mb_en, tb_mb_en = 0
.iif ndf mchk_en, mchk_en = 1
.iif ndf fp_count, fp_count = 0
.iif ndf clrmap, clrmap = 1

.iif ndf kseg_hack, kseg_hack = 0
ASSUME <kseg_hack> eq 0						; 1.58

.iif ndf spinlock_hack, spinlock_hack = 0			; 1.41

;
; Turn off force_path. Use force_path2 conditionally.		; 1.47
;
force_path = 0							; 1.47
.iif ndf force_path2, force_path2 = 0				; 1.47

;
; Ebox ipr corruption checker for ev6 pass 2.3
;
.iif ndf check_ebox_iprs, check_ebox_iprs = 0			; 1.60

.if ne ev6_p1
	.iif ndf dcache_set_en, dcache_set_en = 1
.iff
	.iif ndf dcache_set_en, dcache_set_en = 3
.endc

.iif ndf separate_page_tables, separate_page_tables = 0		; 1.62

.iif ndf applu_fix, applu_fix = 1				; 1.70

.iif ndf evict_loop_count, evict_loop_count = 2	; 1.80 evict 1KB at a time
ASSUME evict_loop_count le <^x7fff>

.if ndf	max_cpuid
	max_cpuid = 2
.endc
.if ndf	svmin			; platform specific palcode version number
	svmin = 0
.endc

pal_version_h = <<max_cpuid@16> ! <svmin@0>>

;
; EV6 shadow register usage 
;
;	p4	= p4	reserved for itb/dtb miss
;	p5	= p5	reserved for itb/dtb miss
;	p6	= p6	reserved for itb/dtb miss
;	p7	= p7	reserved for itb/dtb miss
;
;	p20	= r20	reserved for call_pal and non-miss handling routines
;	p21	= r21	address of pal temps in memory (p_temp)
;	p22	= r22	p_misc (phys_mode in <63> and ps in <15:0>)
;	p23	= r23	call_pal linkage register
;

;
; Notes on speculative execution, non-renamed hardware registers, and
; branch prediction.
;
; 1.81
; EXC_ADDR, IVA_FORM, EXC_SUM, are sourced by
; non-renamed hardware registers that need to be available for subsequent traps.
; Hardware protects the values from overwrite for the first fetch block
; (4 instructions) of a pal flow. During this fetch block, the PALcode should
; save whatever registers it will need.
;
; In addition, VA, VA_FORM, DC_CTL and MM_STAT can have issue order problems
; with subsequent loads that issue out of order in respect to the mxfr from/to
; these registers. A MB before the load can be used to enforce the issue order.
; 
; Note that branch prediction is turned off in PALmode. The fall-through
; path should be the common path. Also note that in the exception flows,
; it takes a fetch block to turn off branch prediction, so therefore
; there should be no conditional branches in the first fetch block.
;
; We can also use this fact to help us avoid the overwrite problems
; For example, a dismiss of a dtb miss is down a branch path, and thus is not
; speculatively issued. If the taken branch depends on these registers, we
; have enforced issue order.
;

;+
; POWERUP - offset 0
;
; Entry:
;	Jumped to by the SROM code on reset. Vector 780 will be used for
;	wakeup. It is assumed that the retirator, the integer/float map
;	and the shadow registers have been initialized!
;
; Note that the SROM standard as of 25-Jan-1996 uses r15-r21 for information
; to be passed up to the PALcode. Also, whami might be passed in r0.
;-

	EV6__POWERUP_ENTRY = ^x0
TRAP__START:					; used by vector macros
trap__pal_base:
	START_HW_VECTOR <POWERUP>

	or	r31, r31, r16			; 1.56 clear fault_reset flag
	br	r26, sys__reset			; off to sys__reset
						; 1.56 delete .long 0
	.long	pal_version_l			; location PALbase + 0x8
	.long	pal_version_h			; location PALbase + 0xC
;
; Flag whether ev6 workarounds are assembled in
; = 00	none
; = 01	force_path2
; = 10	spinlock_hack
; = 11	both
;
trap__conditional_flags:				; 1.58 PALbase + 0x10
	.quad	<<spinlock_hack@1>!<force_path2>>	; 1.58

	.align	quad
trap__lock_cell:
	.quad	0

	END_HW_VECTOR


;+
; DTBM_DOUBLE_3 - offset 100
;
; Entry:
;	Vectored into via hardware trap on TB miss on level 3
;	page table entry. Use 3-level flow, optimizing out 1st level
;	lookup.
;
; Function:
;	Translate level 3 PTE va.
;	If valid, load TB and return to redo the ld_vpte, which will now hit.
;	If not valid, then take TNV/ACV exception.
;
; Current state:
;	p4	va of level 3 page table entry
;	p5	mm_stat (from dtbm_single)
;	p6	original va
;	p7	exc_sum (from dtbm_single)
;	p23	pc of instruction causing the TB miss
;
;	VA	original va for dstream TB miss
;
; Register Use:
;	p5	exc_addr, pc of ld_vpte in TB miss flows
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;
;	r25	saved, used as scratch, restored on hw_ret
;	r26	saved, used as scratch, restored on hw_ret
;
; Exit state:
;	On exit to trap__double3_pte_inv
;	p5	scratch
;	p6	original va
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction causing TB miss
;	r25	pte
;-
	START_HW_VECTOR <DTBM_DOUBLE_3>
;
; Entered on a miss on va of level 3 page table entry
; 	level 1 = vptb
; 	level 2 = level 1 of original va
; 	level 3 = level 2 of original va
; byte_within_page = level 3 of original va as offset into level 3 page table
;
; Optimize by starting with the fetch of level 2 pte
;
ASSUME P_MISC__PHYS__S eq 63

	zapnot	p7, #3, p7			; clean exc_sum to 16 bits
	sll	p5, #16, p5			; shift mm_stat into place
	bis	p7, p5, p7			; combine 
	hw_mfpr	p5, EV6__EXC_ADDR		; (0L) save exc_addr

	bis	p7, #1, p7			; double miss flag

	hw_stq/p r25, PT__R25(p_temp)		; get a scratch register
	hw_stq/p r26, PT__R26(p_temp)		; get a scratch register
	hw_ldq/p r25, PT__PTBR(p_temp)		; get phys page table addr

	blt	p_misc, trap__double3_1to1	; 1-to-1 => branch

	CONT_HW_VECTOR <DTBM_DOUBLE_3>		; 1.70 punt into free code

.if ne check_ebox_iprs				; 1.60

	hw_ldq/p r25, PT__VPTB(p_temp)		; 1.60 get vptb
	srl	p4, #33, r26			; 1.60 get level1 value
	sll	r26, #33, r26			; 1.60 clean
	xor	r25, r26, r25			; 1.60 compare
	bne	r25, trap__dbm_double3_crash1	; 1.60
	br	r25, trap__dbm_double3_okay1	; 1.60

trap__dbm_double3_crash1:
	hw_ldq/p r25, PT__R25(p_temp)		; 1.60 restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; 1.60 restore r26
	lda	p20, ^x0A(r31)			; 1.60 crash code
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; 1.60
	br	r31, trap__halt_after_fix	; 1.61

trap__dbm_double3_okay1:
	lda	r25, ^xAA(r31)			; 1.60 get a constant
	insbl	r25, #0, r25			; 1.60 swapped on big-end
	cmpeq	r25, #^xAA, r26			; 1.60 compare
	beq	r26, trap__dbm_double3_crash2	; 1.60
	br	r26, trap__dbm_double3_okay2	; 1.60

trap__dbm_double3_crash2:
	hw_ldq/p r25, PT__R25(p_temp)		; 1.60 restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; 1.60 restore r26
	lda	p20, ^x0B(r31)			; 1.60 crash code
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; 1.60
	br	r31, trap__halt_after_fix	; 1.61
						; 1.70 delete CONT

trap__dbm_double3_okay2:			; 1.61
	rpcc	r26, (p7)			; 1.61 depend on p7
	addq	p7, r26, r25			; 1.61 depend on r26
	addq	r25, #1, r25			; 1.61 depend on r25
	addq	r25, #1, r25			; 1.61 depend on r25
	rpcc	r25, (r25)			; 1.61 depend on r25
  .if ne egore					; 1.61
	bis	r31, #1, r26			; 1.61 isp has no cc
	bis	r31, #2, r25			; 1.61 isp has no cc
  .endc						; 1.61
	xor	r26, r25, r26			; 1.61 see if cc incremented
	beq	r26, trap__dbm_double3_crash3	; 1.61 branch if not
	br	r26, trap__dbm_double3_okay3	; 1.61 otherwise okay

trap__dbm_double3_crash3:
	hw_ldq/p r25, PT__R25(p_temp)		; 1.61 restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; 1.61 restore r26
	lda	p20, ^x0C(r31)			; 1.61 crash code
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; 1.61
	br	r31, trap__halt_after_fix	; 1.61

trap__dbm_double3_okay3:			; 1.61
	hw_ldq/p r25, PT__PTBR(p_temp)		; 1.61 get phys page table addr

.endc						; 1.60 check_ebox_iprs
;
; 1.74 Bug fix and optimization.
; The bug is that the test for "VIRBND not in use" checked
; for negative VIRBND, instead of VIRBND = -1. The optimization is
; that instead of checking VIRBND in use and VA lssu VIRBND, we check
; VA lequ VIRBND, which essentially does both the checks. That is, if
; VIRBND is -1, then every VA is less-than-or-equal to it, so the old way
; (one page table) is used.
;
.if ne separate_page_tables			; 1.62
	hw_ldq/p r26, PT__VIRBND(p_temp)	; get virbnd
	cmpule	p6, r26, r26			; 1.74 is va lequ virbnd?
	beq	r26, trap__double3_sysptbr	; if not, use sysptbr
	br	r31, trap__double3_cont		; is so, use ptbr

trap__double3_sysptbr:				; use sysptbr
	hw_ldq/p r25, PT__SYSPTBR(p_temp)	; get sysptbr
	br	r31, trap__double3_cont		; continue
						; 1.70 delete CONT
trap__double3_cont:				; continue

.endc						; 1.62 separate_page_tables

	sll	p4, #<64-<<2*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 2 into offset
	addq	r25, r26, r25			; pa for level 2 pte
	hw_ldq/p r25, 0(r25)			; get level 2 pte
	sll	p4, #<64-<<1*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 3 into offset
	blbc	r25, trap__double3_pte_inv	; branch => invalid pte
	srl	r25, #32, r25			; extract pfn from pte
	sll	r25, #13, r25			; get into position
	addq	r25, r26, r25			; pa for level 3 pte
	hw_ldq/p r25, 0(r25)			; get level 3 pte
	blbc	r25, trap__double3_pte_inv	; branch => invalid pte

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
.endc						; 1.70
	srl	r25, #7, r26			; get mb bit

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p4, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p4, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	r25, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	r25, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1
						; 1.70 move hw_ldq
;
; We need to wait until the pte writes retire. We need to avoid the
; case where the tag writes in the single flow issue before the
; pte writes in this flow issue and retire. With changes in 1.36, we
; can probably remove this stall, but at this point in the project,
; let's just leave it in.
;
	EV6_MTPR r31, <EV6__MM_STAT ! ^x80>, prealign=0	
						; 1.70 wait for pte write
						; 1.70 delete CONT
ASSUME <tb_mb_en + pte_eco> ne 2

.if eq force_path2				; 1.47 force_path2 = 0

  .if ne pte_eco
	blbc	r26, trap__dtbm_double3_mb	; branch for mb
	hw_ldq/p r25, PT__R25(p_temp)		; 1.70 restore register
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	PVC_VIOLATE <44>			; 1.73
	hw_ret	(p5)				; (0L) return

trap__dtbm_double3_mb:
	mb
	hw_ldq/p r25, PT__R25(p_temp)		; 1.70 restore register
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	PVC_VIOLATE <44>			; 1.73
	hw_ret	(p5)				; (0L) return
  .iff
	hw_ldq/p r25, PT__R25(p_temp)		; 1.70 restore register
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	PVC_VIOLATE <44>			; 1.73
	hw_ret	(p5)				; (0L) return
  .endc						; pte_eco

.iff						; force_path2 = 1
;
; We need to avoid the situation where a ldx_l has acquired a lock,
; another processor has taken it, and a bad path before a stx_c has
; a load which pulls the data back in and makes it look like the
; lock has succeeded. Hold up loads by writing to MM_STAT with
; scoreboard bit 2 (and 6 -- 6 is required or we hang).
;
	hw_ldq/p r25, PT__R25(p_temp)		; 1.70 restore register
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	mb					; allow hw_ret to fire

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
	bis	r0, r0, r0			; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
.endc						; 1.70

	ALIGN_FETCH_BLOCK <^x47FF041F>
	mulq	p6, #1, p6			; hold up loads
	mulq	p6, #1, p6			; hols up loads
	hw_mtpr p6, <EV6__MM_STAT ! ^x44>	; hold up loads
	PVC_VIOLATE <43>			; 1.71
	hw_ret	(p5)				; return
.endc						; 1.47 force_path2


;+
; trap__double3_pte_inv
;
; Entry:
; 	Double TB miss flow found an invalid level 2/3 page table entry,
;	which is equivalent to invalid level 1/2 PTE of original va.
;
; Function:
;	Prepare to take a TNV or ACV exception. Access violation takes
;	priority over translation not valid.
;
; Current state:
;	p4	available
;	p5	pc of ld_vpte instruction in TB miss flow
;	p6	original va
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction causing TB miss
;
;	r25	pte
;	r26	scratch
;
;	PT__R25	saved r25
;	PT__R26	saved r26
;
; Register use:
;	p20	available if original exception not from pal
;
; Exit state:
;	On exit to trap__post_km_r45
;	r25			restored
;	r26			restored
;	PT__FAULT_PC		pc of instruction causing TB miss
;	PT__FAULT_SCB		scb offset
;	PT__FAULT_R4		fault va
;	PT__FAULT_R5		memory management flags
;
;	On exit to trap__tnv_in_pal
;	p4			pte with <17:16> = ^b00 (level 1 or 2)
;	p5			mm_stat (from dtbm_single)
;	p6			fault va
;	p23			pc of instruction causing TB miss
;
;	r25			saved and available
;	r26			saved and available
;-

;
; Dependent on trap__itb_miss_vpte being in low pal space, which it is.
;
itbmiss = <trap__itb_miss_vpte - trap__pal_base>

trap__double3_pte_inv:
	bis	p5, r31, p4			; save pc of ld_vtpe in p4
	srl	p7, #16, p5			; recover mm_stat

	hw_mfpr	r26, EV6__PAL_BASE		; (4,0L)
	blbs	p23, trap__double3_tnv_in_pal	; originally from pal?

	bic	p4, #3, p20			; clean pc, free up p4
	lda	p4, <1@PTE__KRE__S>(r31)	; kre bit
	and	p4, r25, p4			; get kre bit value
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store fault pc

	subq	r26, p20, p20			; pal base - offset
	lda	r26, <itbmiss>(p20)		; see if from itb miss
	beq	r26, trap__double3_pte_inv_i	; if match => from itb miss

	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	blbc	p5, trap__double3_pte_inv_check	; load => check x31

	srl	p5, #EV6__MM_STAT__OPCODE__S, p7	; get opcode
	and	p7, #EV6__MM_STAT__OPCODE__M, p7	; clean it
	cmpeq	p7, #^x18, p7				; ECB/WH64?
	bne	p7, trap__double3_dismiss		; dismiss if so

	br	r31, trap__double3_pte_inv_cont	; store
;
; Check for load x31
;
trap__double3_pte_inv_check:
	srl	p7, #EV6__EXC_SUM__REG__S, p7	; get ra
	and	p7, #EV6__EXC_SUM__REG__M, p7	; clean ra
	cmpeq	p7, #^x1F, p7			; check for x31
	bne	p7, trap__double3_dismiss	; branch => dismiss
;
; Post the trap.
;
trap__double3_pte_inv_cont:
	lda	p20, SCB__TNV(r31)		; assume TNV
	sll	p5, #63, p5			; set up mmf for read/write
	cmoveq	p4, #SCB__ACV, p20		; ACV over TNV
	hw_stq/p p5, PT__FAULT_R5(p_temp)	; store mmf
	hw_stq/p p6, PT__FAULT_R4(p_temp)	; store fault va
	hw_stq/p p20, PT__FAULT_SCB(p_temp)	; store scb offset
	br	r31, trap__post_km_r45
;
; Istream. Post the trap.
;
trap__double3_pte_inv_i:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	lda	p20, SCB__TNV(r31)		; assume TNV
	bis	r31, #1, p5			; get a 1 for mmf
	hw_stq/p p23, PT__FAULT_R4(p_temp)	; store fault va=pc
	cmoveq	p4, #SCB__ACV, p20		; take ACV over TNV
	hw_stq/p p5, PT__FAULT_R5(p_temp)	; store mmf
	hw_stq/p p20, PT__FAULT_SCB(p_temp)	; store scb
	br	r31, trap__post_km_r45
;
; Dismiss the load x31.
; We will be returning past the load x31 instruction, but we have two
; entries on the prediction stack. So we need to do two returns.
;
; We don't need a MB to hold up loads that may affect MM_STAT.
; To get here, we have taken conditional branches that depend on the
; mfpr MM_STAT being issued.
;
pop_offset = <trap__double3_dismiss_pop_done - trap__double3_dismiss_pop>

trap__double3_dismiss:
	br	p7, trap__double3_dismiss_pop
trap__double3_dismiss_pop:
	addq	p7, #<pop_offset+1>, p7		; return past ret in palmode
	PVC_JSR	double3_dismiss
	hw_ret	(p7)				; pop prediction stack
	PVC_JSR	double3_dismiss, dest=1

trap__double3_dismiss_pop_done:
	addq	p23, #4, p23			; increment past load x31
	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p23)				; return
;
; The originator of the miss was PALcode.
;
trap__double3_tnv_in_pal:
	ldah	p4, <<3@PTE__SOFT__S>+32768>@-16(r31)
	bic	r25, p4, p4			; clear level 3 marker
	br	r31, trap__tnv_in_pal		; merge
;+
; Do a 1-to-1 mapping
; Current state:
;	p5	exc_addr of ld_vpte
;	r25	saved, needs to be restored
;	r26	saved, needs to be restored
;-
trap__double3_1to1:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26
	addq	p5, #4, p5			; return past the ld_vpte

    .if eq force_path				; 1.44
	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p5)				; do the ret
    .iff
	ALIGN_FETCH_BLOCK <^x47FF041F>		; align
	PVC_VIOLATE <1007>
	PVC_VIOLATE <1020>			; stop permutation
	hw_jmp	(p5)				; return with jmp
	br	r31, .-4			; stop predictor
    .endc					; 1.44

	END_HW_VECTOR

;+
; DTBM_DOUBLE_4 - offset 180
;
; Entry:
;	Vectored into vis hardware trap on TB miss on level 3
;	page table entry. Use 4-level flow.
;
; Function:
;	Translate level 3 PTE va.
;	If valid, load TB and return to redo the ld_vpte, which will now hit.
;	If not valid, then take TNV/ACV exception.
;
; Current state:
;	p4	va of level 3 page table entry
;	p5	mm_stat (from dtbm_single)
;	p6	original VA
;	p7	exc_sum (from dtbm_single)
;	p23	pc of instruction causing the TB miss
;
;	VA	original VA for dstream TB miss
;
; Register Use:
;	p5	exc_addr, pc of ld_vpte in TB miss flows
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;
;	r25	saved, used as scratch, restored on hw_ret
;	r26	saved, used as scratch, restored on hw_ret
;
; Exit state:
;	On exit to trap__double3_pte_inv
;	p5	scratch
;	p6	original va
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction causing TB miss
;	r25	pte
;-

	START_HW_VECTOR <DTBM_DOUBLE_4>

;
; Entered on a miss on va of level 3 page table entry.
;	level 0 = VPTB
;	level 1 = level 0 of original va
;	level 2 = level 1 of original va
;	level 3 = level 2 of original va
; byte_within_page = level 3 or original va as offset in level 3 page table
;
; Optimize by starting with the fetch of level 1 pte
;
ASSUME P_MISC__PHYS__S eq 63

	zapnot	p7, #3, p7			; clean exc_sum to 16 bits
	sll	p5, #16, p5			; shift mm_stat into place
	bis	p7, p5, p7			; combine 
	hw_mfpr	p5, EV6__EXC_ADDR		; (0L) save exc_addr

	bis	p7, #1, p7			; double miss flag

	hw_stq/p r25, PT__R25(p_temp)		; get a scratch register
	hw_stq/p r26, PT__R26(p_temp)		; get a scratch register
	hw_ldq/p r25, PT__PTBR(p_temp)		; get phys page table addr

	blt	p_misc, trap__double3_1to1	; 1-to-1 => branch

	CONT_HW_VECTOR<DTBM_DOUBLE_4>		; 1.70 punt into free code
;
; 1.74 Bug fix and optimization.
; The bug is that the test for "VIRBND not in use" checked
; for negative VIRBND, instead of VIRBND = -1. The optimization is
; that instead of checking VIRBND in use and VA lssu VIRBND, we check
; VA lequ VIRBND, which essentially does both the checks. That is, if
; VIRBND is -1, then every VA is less-than-or-equal to it, so the old way
; (one page table) is used.
;
.if ne separate_page_tables			; 1.62
	hw_ldq/p r26, PT__VIRBND(p_temp)	; get virbnd
	cmpule	p6, r26, r26			; 1.74 is va lequ virbnd?
	beq	r26, trap__double4_sysptbr	; if not, use sysptbr
	br	r31, trap__double4_cont		; is so, use ptbr

trap__double4_sysptbr:				; use sysptbr
	hw_ldq/p r25, PT__SYSPTBR(p_temp)	; get sysptbr
	br	r31, trap__double4_cont		; continue
						; 1.70 delete CONT
trap__double4_cont:				; continue

.endc						; 1.62 separate_page_tables
	sll	p4, #<64-<<3*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 1 into offset
	addq	r25, r26, r25			; pa for level 1 pte
	hw_ldq/p r25, 0(r25)			; get level 1 pte

	sll	p4, #<64-<<2*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 2 into offset
	blbc	r25, trap__double3_pte_inv	; branch => invalid pte
	srl	r25, #32, r25			; extract pfn from pte
	sll	r25, #13, r25			; get into position
	addq	r25, r26, r25			; pa for level 2 pte
	hw_ldq/p r25, 0(r25)			; get level 2 pte

	sll	p4, #<64-<<1*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 3 into offset
	blbc	r25, trap__double3_pte_inv	; branch => invalid pte
	srl	r25, #32, r25			; extract pfn from pte
	sll	r25, #13, r25			; get into position
	addq	r25, r26, r25			; pa for level 3 pte
	hw_ldq/p r25, 0(r25)			; get level 3 pte
	blbc	r25, trap__double3_pte_inv	; branch => invalid pte

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
.endc						; 1.70
	srl	r25, #7, r26			; get mb bit

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p4, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p4, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	r25, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	r25, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1
						; 1.70 move hw_ldq
;
; We need to wait until the pte writes retire. We need to avoid the
; case where the tag writes in the single flow issue before the
; pte writes in this flow issue and retire. With changes in 1.36, we
; can probably remove this stall, but at this point in the project,
; let's just leave it in.
;
	EV6_MTPR r31, <EV6__MM_STAT ! ^x80>, prealign=0
						; 1.70 wait for pte write
ASSUME <tb_mb_en + pte_eco> ne 2

.if eq force_path2				; 1.47 force_path = 0

  .if ne pte_eco
	blbc	r26, trap__dtbm_double4_mb	; branch for mb
	hw_ldq/p r25, PT__R25(p_temp)		; 1.70 restore register
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	PVC_VIOLATE <44>			; 1.73
	hw_ret	(p5)				; (0L) return

trap__dtbm_double4_mb:
	mb
	hw_ldq/p r25, PT__R25(p_temp)		; 1.70 restore register
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	PVC_VIOLATE <44>			; 1.73
	hw_ret	(p5)				; (0L) return
  .iff
	hw_ldq/p r25, PT__R25(p_temp)		; 1.70 restore register
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	PVC_VIOLATE <44>			; 1.73
	hw_ret	(p5)				; (0L) return
  .endc						; pte eco

.iff						; force_path2 = 1
;
; We need to avoid the situation where a ldx_l has acquired a lock,
; another processor has taken it, and a bad path before a stx_c has
; a load which pulls the data back in and makes it look like the
; lock has succeeded. Hold up loads by writing to MM_STAT with
; scoreboard bit 2 (and 6 -- 6 is required or we hang).
;
	hw_ldq/p r25, PT__R25(p_temp)		; 1.70 restore register
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	mb					; allow hw_ret to fire

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
	bis	r0, r0, r0			; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
.endc						; 1.70

	ALIGN_FETCH_BLOCK <^x47FF041F>
	mulq	p6, #1, p6			; hold up loads
	mulq	p6, #1, p6			; hols up loads
	hw_mtpr p6, <EV6__MM_STAT ! ^x44>	; hold up loads
	PVC_VIOLATE <43>			; 1.71
	hw_ret	(p5)				; return
.endc						; 1.47 force_path2

	END_HW_VECTOR

;+
; FEN - offset 200
;
; Entry:
;	Vectored into via hardware trap on floating point disabled fault.
;
; Function:
;	Prepare to take a floating disabled exception via trap__post_km.
;	
; Note:
;	Unlike previous processors, this entry point is *only* for
;	floating point disabled fault, so we don't need to check fen
;	to see if we need to generate an OPCDEC instead.
;
; Register use:
;	p20		flag for unaligned case
;	p23		exc_addr
;
; Exit state:
;	On exit to trap__post_km
;	PT__FAULT_PC		pc of instruction
;	PT__FAULT_SCB		scb offset
;-
	START_HW_VECTOR <FEN>
;+
; Pass1 -- without floating point
;-
.if ne ev6_p1
	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) save exc_addr
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	hw_ldq/p p5, CNS__FPE_STATE(p4)		; get real fpe state
	blbc	p5, trap__emul_fen		; if clear, take fen trap

trap__emul_merge:				; merge from opcdec

	hw_stq/p p23, CNS__FP_PC(p4)		; save user pc
	bis	r31, r31, p20			; clear unaligned flag

    .if ne fp_count
	hw_ldq/p p6, PT__RSV_FOR_PAL(p_temp)	; get fp counter
	addq	p6, #1, p6			; add 1
	hw_stq/p p6, PT__RSV_FOR_PAL(p_temp)	; write it back
	br	r31, trap__fen_cont		; continue
    .endc

	CONT_HW_VECTOR <FEN>			; continue in free space
;
; Fen with FPE_STATE set
;	or
; Opcdec with unknown FPE_STATE
;
;	Save GPRs.
;	Get the instruction and parse for load/store.
;	Execute locally for ld/st.
;	Go to emulator for operates and branches.
;	Restore GPRs.
;	HW_RET to correct PC. 
;
ASSUME EV6__I_CTL__SDE__S eq 6

	ALIGN_FETCH_BLOCK <^x47FF041F>		; align with nops

	hw_stq/p r0, CNS__R0_EMUL(p4)		; save r0
	hw_stq/p r1, CNS__R1_EMUL(p4)		; save r1
	hw_stq/p r2, CNS__R2_EMUL(p4)		; save r2
	hw_mfpr	r2, EV6__I_CTL			; (4,0L) get i_ctl

	bis	p4, r31, r1			; impure base into r1
	bic	r2, #<2@EV6__I_CTL__SDE__S>, r2	; zap sde
	hw_stq/p r3, CNS__R3_EMUL(p4)		; save r3
	hw_stq/p r8, CNS__R8_EMUL(r1)		; save gpr

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) write i_ctl
	hw_stq/p r9, CNS__R9_EMUL(r1)		; save gpr
	hw_stq/p r10, CNS__R10_EMUL(r1)		; save gpr
	hw_stq/p r11, CNS__R11_EMUL(r1)		; save gpr

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) stall outside IQ
	hw_stq/p r12, CNS__R12_EMUL(r1)		; save gpr
	hw_stq/p r13, CNS__R13_EMUL(r1)		; save gpr
	hw_stq/p r14, CNS__R14_EMUL(r1)		; save gpr

	hw_stq/p r15, CNS__R15_EMUL(r1)		; buffer block 1 -- save gpr
	hw_stq/p r16, CNS__R16_EMUL(r1)		; save gpr
	hw_stq/p r17, CNS__R17_EMUL(r1)		; save gpr
	hw_stq/p r18, CNS__R18_EMUL(r1)		; save gpr

	hw_stq/p r19, CNS__R19_EMUL(r1)		; buffer block 2 -- save gpr
	hw_stq/p r24, CNS__R24_EMUL(r1)		; save gpr
	hw_stq/p r25, CNS__R25_EMUL(r1)		; save gpr
	hw_stq/p r26, CNS__R26_EMUL(r1)		; save gpr

	hw_stq/p r27, CNS__R27_EMUL(r1)		; buffer block 3 -- save gpr
	hw_stq/p r28, CNS__R28_EMUL(r1)		; save gpr
	hw_stq/p r29, CNS__R29_EMUL(r1)		; save gpr
	hw_stq/p r30, CNS__R30_EMUL(r1)		; save gpr

	hw_stq/p r4, CNS__R4_EMUL(r1)		; save gpr
	hw_stq/p r5, CNS__R5_EMUL(r1)		; save gpr
	hw_stq/p r6, CNS__R6_EMUL(r1)		; save gpr
	hw_stq/p r7, CNS__R7_EMUL(r1)		; save gpr

	hw_stq/p r20, CNS__R20_EMUL(r1)		; save gpr
	hw_stq/p r21, CNS__R21_EMUL(r1)		; save gpr
	hw_stq/p r22, CNS__R22_EMUL(r1)		; save gpr
	hw_stq/p r23, CNS__R23_EMUL(r1)		; save gpr

	hw_stq/p r31, CNS__R31_EMUL(r1)		; save gpr
;
; Now turn shadow mode back on
;
	bis	r2, #<2@EV6__I_CTL__SDE__S>, r2	; enable sde
	hw_mtpr	r2, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31			; finish fetch block

	ALIGN_FETCH_BLOCK <^x47FF041F>		; align with nops

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now start parsing the instruction.
;	r1	pointer to impure area
;	p23	pc of instruction
; Get the instruction physically so we don't fall over a FOR problem.
; If we get tnv, halt for now...
;
	bic	p23, #3, r2			  ; clean inst pc to r2
trap__emul_get_instr:
	blt	p_misc, trap__emul_get_instr_1to1 ; skip walk for 1to1
	hw_ldq/p p6, PT__PTBR(p_temp)		  ; get phys page table addr

        sll     r2, #<64-<<3*level_bits>+13>>, p5
        srl     p5, #<61-level_bits>, p5        ; get level 1 into offset
        addq    p5, p6, p6                      ; pa for level 1 pte
        hw_ldq/p p6, 0(p6)                      ; get level 1 pte

        sll     r2, #<64-<<2*level_bits>+13>>, p5
        srl     p5, #<61-level_bits>, p5        ; get level 2 into offset
        blbc    p6, trap__mm_in_float_emul_instr ; branch => invalid pte
        srl     p6, #32, p6                     ; extract pfn from pte
        sll     p6, #13, p6                     ; get into position
        addq    p5, p6, p6                      ; pa for level 2 pte
        hw_ldq/p p6, 0(p6)                      ; get level 2 pte

        sll     r2, #<64-<<1*level_bits>+13>>, p5
        srl     p5, #<61-level_bits>, p5        ; get level 3 into offset
        blbc    p6, trap__mm_in_float_emul_instr ; branch => invalid pte
        srl     p6, #32, p6                     ; extract pfn from pte
        sll     p6, #13, p6                     ; get into position
        addq    p5, p6, p6                      ; pa for level 3 pte
        hw_ldq/p p6, 0(p6)                      ; get level 3 pte

        blbc    p6, trap__mm_in_float_emul_instr ; branch => invalid pte

        sll     r2, #<64-13>, p5
        srl     p5, #<64-13>, p5                ; get byte offset
        srl     p6, #32, p6                     ; extract pfn from pte
        sll     p6, #13, p6                     ; get into position
        addq    p5, p6, r2                      ; pa for fl. pt. instr into r2
trap__emul_get_instr_1to1:
        hw_ldl/p p5, 0(r2)                      ; get fl. pt. instr using r2

	zap	p5, #^xF0, p5			; clean to 32 bits
	srl	p5, #26, p6			; get opcode
	and	p6, #^x30, p7			; just get <5:4> of opcode
	cmpeq	p7, #^x20, p7			; check against ^x20
	beq	p7, trap__emul_emul		; branch => pass to emulator
;
; Load/Store
;	r1	pointer to impure area
;	p5	instruction
;	p6	opcode
;
	sll	p5, #43, r2			; Rb.ab
	srl	r2, #59, r2			; Rb.ab
	s8addq	r2, r1, r2			; impure area + offset for RA
	hw_ldq/p r3, CNS__R0_EMUL(r2)		; get contents of register

	sll	p5, #48, r8			; get bit 15 to 63
	sra	r8, #48, r8			; now have sign ext displacement
	addq	r8, r3, r8			; (reg)+disp = mem addr

	sll	p5, #38, r9			; Fa
	srl	r9, #59, r9			; Fa
	bis	r9, r31, r10			; Fa for F31 check
	s8addq	r9, r1, r9			; impure area + offset for FA
;
; We now have the operand info.
;	r1	pointer to impure area
;	r8	memory address
;	r9	impure area + offset to FA
;	r10	FA for F31 check
;
;	p5	instr
;	p6	opcode
;
	and	p6, #^x04, r2			; check for load or store
	bne	r2, trap__emul_parse		; branch for stores
	cmpeq	r10, #^x1F, r3			; check for load to f31
	bne	r3, trap__emul_ldst_done	; done if load to f31

trap__emul_parse:
;
; Set up trap handler for load/storeoperand problem.
;
load_store_offset = <<trap__emul__ldst_problem - trap__pal_base>>

	ldah	p4,<<load_store_offset>+32768>@-16(r31)
	lda	p4,<<load_store_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for problem

	cmpeq	p6, #^x20, r2
	bne	r2, trap__emul_ldf		; branch for ldf
	cmpeq	p6, #^x21, r3
	bne	r3, trap__emul_ldg		; branch for ldg
	cmpeq	p6, #^x22, r2
	bne	r2, trap__emul_lds		; branch for lds
	cmpeq	p6, #^x23, r3
	bne	r3, trap__emul_ldt		; branch for ldt

	cmpeq	p6, #^x24, r2
	bne	r2, trap__emul_stf		; branch for stf
	cmpeq	p6, #^x25, r3
	bne	r3, trap__emul_stg		; branch for stg
	cmpeq	p6, #^x26, r2
	bne	r2, trap__emul_sts		; branch for sts
	cmpeq	p6, #^x27, r3
	beq	r3, trap__pal_exc_bugcheck	; bugcheck if not stt
	br	r31, trap__emul_stt		; fall through for stt

;
; Do load or store.
;	r8	memory address
;	r9	impure area + fpr offset
;

;
; LDF
;
trap__emul_ldf:
	and	r8, #^x3, r2		; check for alignment
	bne	r2, trap__emul_ldf_una	; branch for unaligned

	ldl	r0, 0(r8)		; load from memory (may fault)
trap__emul_ldf_ld_done:
	extwl   r0, #2, r1		; <31:16> -> r1
	inswl	r0, #2, r0		; <15:0> << 16 -> r0
        addq	r0, r1, r0
	sll	r0, #41, r1		; shift out all but fraction bits
	srl	r1, #12, r1		; shift fraction bits inplace
        addq	r31, #^x070, r16
        srl	r0, #30, r19		; shift out all but sign and bias
        sll	r19, #62, r19		; move into correct place
        srl	r0, #23, r0
        addq	r1, r19, r1		; insert sign and bias
        and	r0, #255, r0		; clear all but exp
	cmoveq	r0, #0, r16		; clear map
	cmpult	r0, #^x80, r19		; test for small exp
	cmpeq	r0, #^xff, r17		; check for Nan and Inf
	bic	r19, r17, r19		; r19 and ~r17
	cmoveq	r19, #0, r16		; clear map
        and	r0, #^x7f, r0		; clear all but exp
        s8addq	r16, r0, r16
	sll	r16, #52, r16
        addq	r16, r1, r0		; merge it
	hw_stq/p r0, CNS__F0_EMUL(r9)	; write to our register file
	br	r31, trap__emul_ldst_done

trap__emul_ldf_una:				; unaligned load
	LOAD_UNALIGNED_LONG r0, r8, r1, r2	; do the load
	bis	r31, #1, p20			; set unaligned flag
	hw_stq/p r8, PT__FAULT_R4(p_temp)	; save the address
	br	r31, trap__emul_ldf_ld_done	; re-enter
;
; LDG
;
trap__emul_ldg:
	and	r8, #^x7, r2		; check for alignment
	bne	r2, trap__emul_ldg_una	; branch for unaligned

	ldq	r0, 0(r8)		; load from memory (may fault)
trap__emul_ldg_ld_done:
	sll	r0, #48, r1		; <15:0> -> <63:48>
	srl	r0, #48, r16		; <63:48> -> <15:0>
	extwl	r0, #2,	r17		; <31:16> -> r17
	addq	r1, r16, r1		; <63:48> + <15:0>
	inswl	r17, #4, r16		; <31:16> -> <47:32>
	extwl	r0, #4,	r17		; <47:32> -> r17
	addq	r1, r16, r1		; <63:48> + <47:32> + <15:0> 
	inswl	r17, #2, r0		; <47:32> -> <31:16>
	addq	r1, r0, r0		; <63:48> + <47:32> + <31:16> + <15:0>
	hw_stq/p r0, CNS__F0_EMUL(r9)	; write to our register file
	br	r31, trap__emul_ldst_done

trap__emul_ldg_una:				; unaligned load
	LOAD_UNALIGNED_QUAD r0, r8, r1, r2	; do the load
	bis	r31, #1, p20			; set unaligned flag
	hw_stq/p r8, PT__FAULT_R4(p_temp)	; save the address
	br	r31, trap__emul_ldg_ld_done	; re-enter
;
; LDS
;
trap__emul_lds:
	and	r8, #^x3, r2		; check for alignment
	bne	r2, trap__emul_lds_una	; branch for unaligned

	ldl	r0, 0(r8)		; load from user memory
trap__emul_lds_ld_done:
	sll	r0, #41, r1		; shift out all but fraction bits
	srl	r1, #12, r1		; shift fraction bits inplace
	addq	r31, #^x70, r16	
	srl	r0, #30, r19		; shift out all but sign and bias
	sll	r19, #62, r19		; move into correct place
	srl	r0, #23, r0
	addq	r1, r19, r1		; insert sign and bias
	and	r0, #255, r0		; clear all but exp
	cmoveq	r0, #0, r16		; clear map
	cmpult	r0, #^x80, r19		; test for small exp
	cmpeq	r0, #^xff, r17		; check for Nan and Inf
	bis	r17, r19, r19		; or result
	cmoveq	r19, #0, r16		; clear map
	and	r0, #^x7f, r0		; clear all but exp
	s8addq	r16, r0, r16
	sll	r16, #52, r16
	addq	r16, r1, r0		; merge it
	hw_stq/p r0, CNS__F0_EMUL(r9)	; write to our register file
	br	r31, trap__emul_ldst_done

trap__emul_lds_una:				; unaligned load
	LOAD_UNALIGNED_LONG r0, r8, r1, r2	; do the load
	bis	r31, #1, p20			; set the unaligned flag
	hw_stq/p r8, PT__FAULT_R4(p_temp)	; save the address
	br	r31, trap__emul_lds_ld_done	; re-enter
;
; LDT
;
trap__emul_ldt:
	and	r8, #^x7, r2		; check for alignment
	bne	r2, trap__emul_ldt_una	; branch for unaligned

	ldq	r0, 0(r8)		; load from memory (may fault)
trap__emul_ldt_ld_done:
	hw_stq/p r0, CNS__F0_EMUL(r9)	; write to our register file
	br	r31, trap__emul_ldst_done

trap__emul_ldt_una:				; unaligned load
	LOAD_UNALIGNED_QUAD r0, r8, r1, r2	; do the load
	bis	r31, #1, p20			; set unaligned flag
	hw_stq/p r8, PT__FAULT_R4(p_temp)	; save the address
	br	r31, trap__emul_ldt_ld_done	; re-enter
;
; STF
;
trap__emul_stf:
	hw_ldq/p r0, CNS__F0_EMUL(r9)	; read from our register file
	sll	r0, #5, r1		; shift out all but fraction bits
	srl	r1, #34, r1		; shift fraction bits inplace
	srl	r0, #62, r0		; shift out all but fraction bits
	sll	r0, #30, r0
	addq	r0, r1, r0
	extwl	r0, #2, r1		; <31:16> -> r1
	inswl	r0, #2, r0		; <15:0> << 16 -> r0
	addq	r0, r1, r0

	and	r8, #^x3, r2		; check for alignment
	bne	r2, trap__emul_stf_una	; branch for unaligned

	stl	r0, 0(r8)		; store it in to memory (may fault)
	br	r31, trap__emul_ldst_done

trap__emul_stf_una:				; unaligned store
	STORE_UNALIGNED_LONG r0, r8, r1,r2, r3	; do the store
	bis	r31, #3, p20			; set unaligned store flag
	hw_stq/p r8, PT__FAULT_R4(p_temp)	; save the address
	br	r31, trap__emul_ldst_done	; done
;
; STG
;
trap__emul_stg:
	hw_ldq/p r0, CNS__F0_EMUL(r9)	; read from our register file
	sll	r0, #48, r1		; <15:0> -> <63:48>
	srl	r0, #48, r16		; <63:48> -> <15:0>
	extwl	r0, #2,	r17		; <31:16> -> r17
	addq	r1, r16, r1		; <63:48> + <15:0>
	inswl	r17, #4, r16		; <31:16> -> <47:32>
	extwl	r0, #4,	r17		; <47:32> -> r17
	addq	r1, r16, r1		; <63:48> + <47:32> + <15:0> 
	inswl	r17, #2, r0		; <47:32> -> <31:16>
	addq	r1, r0, r0		; <63:48> + <47:32> + <31:16> + <15:0>

	and	r8, #^x7, r2		; check for alignment
	bne	r2, trap__emul_stg_una	; branch for unaligned

	stq	r0, 0(r8)		; store it in memory (may fault)
	br	r31, trap__emul_ldst_done

trap__emul_stg_una:				; unaligned store
	STORE_UNALIGNED_QUAD r0, r8, r1, r2, r3	; do the store
	bis	r31, #3, p20			; set unaligned store flag
	hw_stq/p r8, PT__FAULT_R4(p_temp)	; save the address
	br	r31, trap__emul_ldst_done	; done
;
; STS
;
trap__emul_sts:
	hw_ldq/p r0, CNS__F0_EMUL(r9)	; read from our register file
	sll	r0, #5, r1		; shift out all but fraction bits
	srl	r1, #34, r1		; shift fraction bits in place
	srl	r0, #62, r0		; shift out all but fraction bits
	sll	r0, #30, r0
	addq	r0, r1, r0

	and	r8, #^x3, r2		; check for alignment
	bne	r2, trap__emul_sts_una	; branch for unaligned

	stl	r0, 0(r8)		; store in user memory (may fault)
	br	r31, trap__emul_ldst_done

trap__emul_sts_una:				; unaligned store
	STORE_UNALIGNED_LONG r0, r8, r1, r2, r3	; do the store
	bis	r31, #3, p20			; set unaligned store flag
	hw_stq/p r8, PT__FAULT_R4(p_temp)	; save the address
	br	r31, trap__emul_ldst_done	; done
;
; STT
;
trap__emul_stt:
	hw_ldq/p r0, CNS__F0_EMUL(r9)	; read from our register file

	and	r8, #^x7, r2		; check for alignment
	bne	r2, trap__emul_stt_una	; branch for unaligned

	stq	r0, 0(r8)		; store in user memory (may fault)
	br	r31, trap__emul_ldst_done

trap__emul_stt_una:				; unaligned store
	STORE_UNALIGNED_QUAD r0, r8, r1, r2, r3	; do the store
	bis	r31, #3, p20			; set unaligned store flag
	hw_stq/p r8, PT__FAULT_R4(p_temp)	; save the address
	br	r31, trap__emul_ldst_done	; done

;
; Load/Store Done.
;
trap__emul_ldst_done:
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	hw_ldq/p p6, CNS__FP_PC(p4)		; get pc back
	addq	p6, #4, p6			; point to next instruction
	hw_stq/p p6, CNS__FP_PC(p4)		; write back to save location
	br	r31, trap__emul_done

;+
; Done emulation. Restore and go back to user.
; Current state:
;	p4	base of impure area
;	p20	unaligned flag
;
;	Get out of shadow mode.
;	Restore overshadowed registers.
;	Get back into shadow mode.
;	Restore the rest of the registers.
;	Return to user.
;-
trap__emul_done:
	bis	p4, r31, r1			; impure base into r1
	PVC_JSR	emul_restore, bsr=1
	bsr	r3, trap__emul_restore		; restore GPRs except r0-r3
;
; Now restore the rest of the GPRs.
;
	hw_ldq/p r0, CNS__R0_EMUL(p4)		; restore r0
	hw_ldq/p r1, CNS__R1_EMUL(p4)		; restore r1
	hw_ldq/p r2, CNS__R2_EMUL(p4)		; restore r2
	hw_ldq/p r3, CNS__R3_EMUL(p4)		; restore r3
;
; Return to user.
;
	hw_ldq/p p23, CNS__FP_PC(p4)		; get nextpc
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap handler
	bne	p20, trap__emul_test_dat	; test for unaligned
	hw_ret	(p23)				; return
;
; Unaligned. See if we need to report the unaligned event.
;
trap__emul_test_dat:
	hw_ldq/p p4, PT__PCBB(p_temp)		; get PCB base
	hw_ldq/p p4, PCB__FEN(p4)		; read DAT/PME/FEN quadword
	srl	p4, #PCB__DAT__S, p4		; isolate DAT bit
	blbc	p4, trap__emul_report		; if clear, report the trap
	hw_ret	(p23)				; return to user
;
; Report unaligned access on ld/st.
;	PT__FAULT_R4	contains va already
;	p20		1 => read
;			3 => write
;	p23		next pc
;
trap__emul_report:
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; next pc
	lda	p4, SCB__UNALIGN(r31)		; scb offset
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; write scb offset
	srl	p20, #1, p20			; get r/w bit
	hw_stq/p p20, PT__FAULT_R5(p_temp)	; r/w bit
	br	r31, trap__post_km_r45		; post

;+
; Subroutine to restore GPRs.
;	p4	pointer to impure area
;	r1	pointer to impure area
;	r3	return address
;
; Do not touch shadow registers.
;-
	ALIGN_FETCH_BLOCK

trap__emul_restore:
	hw_mfpr	r2, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r2, #<2@EV6__I_CTL__SDE__S>, r2	; zap sde
	hw_ldq/p r8, CNS__R8_EMUL(r1)		; restore gpr
	hw_ldq/p r9, CNS__R9_EMUL(r1)		; restore gpr

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) write i_ctl
	hw_ldq/p r10, CNS__R10_EMUL(r1)		; restore gpr
	hw_ldq/p r11, CNS__R11_EMUL(r1)		; restore gpr
	hw_ldq/p r12, CNS__R12_EMUL(r1)		; restore gpr

	hw_mtpr	r2, EV6__I_CTL			; stall outside IQ
	hw_ldq/p r13, CNS__R13_EMUL(r1)		; restore gpr
	hw_ldq/p r14, CNS__R14_EMUL(r1)		; restore gpr
	hw_ldq/p r15, CNS__R15_EMUL(r1)		; restore gpr

	hw_ldq/p r16, CNS__R16_EMUL(r1)		; buffer block 1 -- restore gpr
	hw_ldq/p r17, CNS__R17_EMUL(r1)		; restore gpr
	hw_ldq/p r18, CNS__R18_EMUL(r1)		; restore gpr
	hw_ldq/p r19, CNS__R19_EMUL(r1)		; restore gpr

	hw_ldq/p r24, CNS__R24_EMUL(r1)		; buffer block 2 -- restore gpr
	hw_ldq/p r25, CNS__R25_EMUL(r1)		; restore gpr
	hw_ldq/p r26, CNS__R26_EMUL(r1)		; restore gpr
	hw_ldq/p r27, CNS__R27_EMUL(r1)		; restore gpr

	hw_ldq/p r28, CNS__R28_EMUL(r1)		; buffer block 3 -- restore gpr
	hw_ldq/p r29, CNS__R29_EMUL(r1)		; restore gpr
	hw_ldq/p r30, CNS__R30_EMUL(r1)		; restore gpr
	bis	r31, r31, r31

	hw_ldq/p r4, CNS__R4_EMUL(r1)		; restore gpr
	hw_ldq/p r5, CNS__R5_EMUL(r1)		; restore gpr
	hw_ldq/p r6, CNS__R6_EMUL(r1)		; restore gpr
	hw_ldq/p r7, CNS__R7_EMUL(r1)		; restore gpr

	hw_ldq/p r20, CNS__R20_EMUL(r1)		; restore gpr
	hw_ldq/p r21, CNS__R21_EMUL(r1)		; restore gpr
	hw_ldq/p r22, CNS__R22_EMUL(r1)		; restore gpr
	hw_ldq/p r23, CNS__R23_EMUL(r1)		; restore gpr
;
; Now turn shadow mode back on.
;
	bis	r2, #<2@EV6__I_CTL__SDE__S>, r2	; enable sde
	hw_mtpr	r2, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r3, #1, r3			; return in pal mode
	PVC_JSR emul_restore, bsr=1, dest=1
	hw_ret_stall (r3)			; stall for pvc

;+
; Load/Store operand problem.
;
; We need to restore GPRs. Then we need to set up for an exception.
;
; Current register state:
;	p5		mm_stat
;	p6		va
;	r25		restored as part of general restore
;	r26		restored as part of general restore
;
;	PT__FAULT_SCB	scb offset
;	PT__TRAP	needs to be cleared
;-
trap__emul__ldst_problem:
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher

	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	hw_ldq/p p23, CNS__FP_PC(p4)		; get back the original pc

	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store pc
	sll	p5, #63, p5			; mmf r/w bit
	hw_stq/p p6, PT__FAULT_R4(p_temp)	; store fault va
	hw_stq/p p5, PT__FAULT_R5(p_temp)	; store mmf
;
; Restore GPRs.
;
	bis	p4, r31, r1			; impure base into r1
	PVC_JSR	emul_restore, bsr=1
	bsr	r3, trap__emul_restore		; restore all but r0-r3
;
; Now restore the rest of the GPRs.
;
	hw_ldq/p r0, CNS__R0_EMUL(p4)		; restore r0
	hw_ldq/p r1, CNS__R1_EMUL(p4)		; restore r1
	hw_ldq/p r2, CNS__R2_EMUL(p4)		; restore r2
	hw_ldq/p r3, CNS__R3_EMUL(p4)		; restore r3
;
; Now post the exception.
;
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap handler
	br	r31, trap__post_km_r45		; post

;+
; Operates and branches are passed to emulator.
; Current state:
;	GPRs have been saved.
;	CNS__FP_PC	pc
;	p5		instruction
;
; Map a 4MB chunk of virtual to physical.
; NOTE: The impure area has to be in the same naturally aligned 4mb chunk!!!
;-
.if ndf PAL__EMUL_BASE
	PAL__EMUL_BASE = ^xE0000
.endc
	ALIGN_FETCH_BLOCK
trap__emul_emul:
	subq	r31, #1, r8			; get a -1
	srl	r8, #42, r8			; shift off low bits of kseg addr
	sll	r8, #42, r8			; shift back into position

	br	r1, trap__emul_entry
	.long	PAL__EMUL_BASE			; transfer address
trap__emul_entry:
	hw_ldl/p r27, 0(r1)			; get transfer address
	bis	r27, r8, r27			; transfer address as kseg

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mfpr	r16, EV6__IER_CM		; (4,0L) get old IER_CM
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get impure base
	hw_stq/p r16, CNS__IER_CM(p4)		; save old IER_CM
	bis	p4, r8, r17			; impure area as kseg

	ASSUME_FETCH_BLOCK
	hw_mtpr r31, EV6__IER_CM		; (4,0L) ints off, cm=kernel
	bis	p5, r31, r16			; instruction
	bis	r31, r31, r31
	bis	r31, r31, r31

	ASSUME_FETCH_BLOCK
	lda	p6,^x400(r31)			; get a ^x400
	hw_ldq/p p5, PT__WHAMI(p_temp)		; get whami
	mulq	p5, p6, p6			; whami * ^x400
	subq	r27, p6, r30			; set top of stack to be kseg
						;   xfer addr - (whami * ^x400)
;
; Now we need to turn on kseg mode
;
ASSUME EV6__M_CTL__SPE__S eq 1

	ASSUME_FETCH_BLOCK
	hw_mfpr	p6, EV6__I_CTL			; (4,0L) get i_ctl
	GET_32CONS p5, <^x804000>, r31		; isp bug workaround
	bic	p6, p5, p6			; clear unpredicatables

	ASSUME_FETCH_BLOCK
	hw_stq/p p6, CNS__I_CTL(p4)		; save away
	bis	r31, #2, p4			; get a 2
	sll	p4, #EV6__I_CTL__SPE__S, p5	; 43 bit mode kseg
	bis	r31, r31, r31

	ASSUME_FETCH_BLOCK
	bis	p6, p5, p6			; or in kseg mode
	hw_mtpr	p6, EV6__I_CTL			; (4,0L) write i_ctl
	sll	p4, #EV6__M_CTL__SPE__S, p4	; 43 bit mode kseg
	hw_mtpr p4, EV6__M_CTL			; (6,0L) write m_ctl

	ASSUME_FETCH_BLOCK
	hw_mtpr	p6, <EV6__I_CTL ! ^x40>		; (4&6,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now kseg mode is turned on
;	
kseg_offset = <trap__emul_kseg_done - trap__emul_kseg>

	ASSUME_FETCH_BLOCK
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
	br	r1, trap__emul_kseg

	ASSUME_FETCH_BLOCK
trap__emul_kseg: 
	addq	r1, #<kseg_offset>, r1
	bis	r1, r8, r1			; ret as native mode kernel kseg
	bsr	r31, .				; push prediction stack
	PVC_JSR emul_kseg
	hw_ret_stall (r1)			; pop prediction stack
	PVC_JSR emul_kseg, dest=1

trap__emul_kseg_done:
;
; Now we are in native kernel kseg
;
	ASSUME_FETCH_BLOCK
	PVC_VIOLATE <29>
	PVC_VIOLATE <1007>
	jsr	r26, (r27)			; jsr to emulator
;
; Return from emulation.
; We are still in native kernel kseg mode.
;	r0		return status
;	CNS__FP_PC	updated pc
;	CNS__FPCR	updated
;	CNS__EXC_SUM	exception information
;
trap__emul_return:				; return in non-pal mode
	lda	r16, ^xBAC(r31)			; flag to call_pal
	.long	^x3B				; pop back into pal mode
;
; Now turn kseg back off
;
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	hw_ldq/p p6, CNS__I_CTL(p4)		; get old I_CTL back
	hw_mtpr	p6, EV6__I_CTL			; (4,0L) write i_ctl

	hw_ldq/p p6, PT__M_CTL(p_temp)		; get M_CTL back
	hw_mtpr p6, EV6__M_CTL			; (6,0L) write m_ctl

kseg_off_offset = <trap__emul_kseg_off_done - trap__emul_kseg_off>

	br	r1, trap__emul_kseg_off
trap__emul_kseg_off:
	addq	r1, #<kseg_off_offset+1>, r1	; jump past in palmode
	bsr	r31, .				; push prediction stack
	PVC_JSR kseg_off
	hw_ret_stall (r1)			; pop prediction stack
	PVC_JSR kseg_off, dest=1
trap__emul_kseg_off_done:

;
; Now restore IER_CM
;
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	hw_ldq/p r16, CNS__IER_CM(p4)		; get old IER_CM back
	bis	r31, r31, p20			; zap unalign flag
	hw_mtpr	r16, EV6__IER_CM		; (4,0L) restore IER_CM
;
; Check r0 for status.
;	r0	 0	success
;		-1	arith trap
;		-2	opcdec
;		-4	fen (FEN from OPCDEC with FPE_STATE clear)
;
	bne	r0, trap__emul_arith		; branch for arith or opcdec
	br	r31, trap__emul_done		; restore registers and return

;+
; Arithmetic exception or opcdec or fen during emulation.
;	Restore state. Merge with appropriate flow.
;	Emulator left FP_PC as current pc.
;-
trap__emul_arith:
	bis	r0, r31, p5			; save status
	bis	p4, r31, r1			; impure base into r1
	PVC_JSR	emul_restore, bsr=1
	bsr	r3, trap__emul_restore		; restore all but r0-r3
;
; Now restore the rest of the GPRs.
;
	hw_ldq/p r0, CNS__R0_EMUL(p4)		; restore r0
	hw_ldq/p r1, CNS__R1_EMUL(p4)		; restore r1
	hw_ldq/p r2, CNS__R2_EMUL(p4)		; restore r2
	hw_ldq/p r3, CNS__R3_EMUL(p4)		; restore r3
;
; Current state:
;	p4	base of impure area
;	p5	status
;		-1	arith trap
;		-2	opcdec
;		-4	fen (FPE_STATE clear)
;
	hw_ldq/p p23, CNS__FP_PC(p4)		; get PC back
	blbc	p5, trap__emul_opcdec_or_fen	; branch for opcdec or fen

	hw_ldq/p p7, CNS__EXC_SUM(p4)		; get emulated exc_sum
	br	r31, trap__arith_merge		; merge with arithmetic
;
; 
; Opcdec
;	or
; Fen (FEN from OPCDEC with FPE_STATE clear)
;	p5	-2	opcdec
;		-4	fen (FEN from OPCDEC with FPE_STATE clear)
;
trap__emul_opcdec_or_fen:
	srl	p5, #1, p5			; shift to determine which
	blbc	p5, trap__emul_fen		; branch for fen
	br	r31, trap__opcdec_merge		; take opcdec

;+
; Take fen because CNS__FPE_STATE is clear.
;-
trap__emul_fen:
	lda	p20, SCB__FEN(r31)		; scb offset
	hw_stq/p p20, PT__FAULT_SCB(p_temp)	; store scb
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store pc

	blbs	p23, trap__pal_exc_bugcheck	; mchk if fen within pal
	br	r31, trap__post_km		; post the fen exception
;+
; Pass2 -- floating point
;-
.iff
	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) save exc_addr
	lda	p20, SCB__FEN(r31)		; scb offset
	hw_stq/p p20, PT__FAULT_SCB(p_temp)	; store scb
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store pc

	blbs	p23, trap__pal_exc_bugcheck	; mchk if fen within pal
	br	r31, trap__post_km		; post the fen exception
.endc
	END_HW_VECTOR

;+
; UNALIGN - offset 280
;
; Entry:
;	Vectored into via hardware trap on unaligned Dstream reference.
;
; Function:
;	Attempt to fix up the unaligned reference. If indicated, take
;	an exception via trap__post_km_r45.
;	
; Register use:
;	p4 - p7	scratch before doing first trappable memory reference
;	p20		scratch
;
;	r1		scratch
;	r2		scratch
;	r3		scratch
;	r8		scratch
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;
; Exit state:
;	On exit to trap__post_km_r45
;	PT__FAULT_PC		pc of instruction
;	PT__FAULT_SCB		scb offset
;	PT__FAULT_R4		fault va
;	PT__FAULT_R5		memory management flags
;	
;-
	START_HW_VECTOR <UNALIGN>
;
; We need to look at a number of IPRs. We can use p4-p7 up to the time we
; do our first virtual reference. By then, we need to have saved away important
; information like pc, opcode, register, etc. We need to get exc_addr and
; exc_sum in the first fetch block.
;
	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) get exc_addr
	hw_mfpr	p7, EV6__EXC_SUM		; (0L) get register field
	NOP					; avoid cbr
	NOP
;	
; Test for special unaligned_from_pal queue case.
;
	blbs	p23, call_pal__queue_unaligned_from_pal
;
; Now we can proceed.
;
	hw_stq/p r1, PT__R1(p_temp)		; get scratch registers
	hw_stq/p r2, PT__R2(p_temp)		; get scratch registers
	hw_stq/p r3, PT__R3(p_temp)		; get scratch registers
	hw_stq/p r8, PT__R8(p_temp)		; get scratch registers
	hw_stq/p r9, PT__R9(p_temp)		; get scratch registers
;
; Get opcode from mm_stat and save PT__OPCODE for later use. Get va.
; Then test for _C, _L illegal unaligned cases.
;
	hw_mfpr	p6, EV6__MM_STAT			; (0L) opcode, r/w bit
	hw_mfpr	p20, EV6__VA				; (4-7,1L) get va
	srl	p6, #EV6__MM_STAT__OPCODE__S, p5	; shift opcode down
	and	p5, #EV6__MM_STAT__OPCODE__M, p5	; clean opcode
	hw_stq/p p5, PT__OPCODE(p_temp)			; save opcode

	and	p5, #^b1010, p4				; test for _C, _L
	cmpeq	p4, #^b1010, p4				; test for _C, _L
	bne	p4, trap__unaligned_illegal		; illegal if _C, _L
;
; Set up PT__TRAP for dfaults that may occur during virtual references.
; Save the next pc away in PT__FAULT_PC. Save register number away in PT__REG.
; Branch off for stores. For loads, dismiss loads to r31/f31.
;
unaligned_dfault_offset = <<trap__unaligned_dfault - trap__pal_base>>

	ldah	p4,<<unaligned_dfault_offset>+32768>@-16(r31)
	lda	p4,<<unaligned_dfault_offset> & ^xFFFF>(p4)
	hw_stq/p p4, PT__TRAP(p_temp)			; offset for dfault

	addq	p23, #4, p23				; next pc for unalign
	hw_stq/p p23, PT__FAULT_PC(p_temp)		; save pc

	srl	p7, #EV6__EXC_SUM__REG__S, p7		; shift register number
	and	p7, #EV6__EXC_SUM__REG__M, p7		; clean register number
	hw_stq/p p7, PT__REG(p_temp)			; save away register

	CONT_HW_VECTOR <UNALIGN>			; continue in free space
;
; Current state:
;	p5		opcode
;	p6		mm_stat
;	p7		register number
;	p20		va
;
;	r1		available
;	r2		available
;	r3		available
;	r8		available
;	r9		available
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;	PT__OPCODE	opcode
;	PT__TRAP	set up for trap
;	PT__FAULT_PC	next pc
;	PT__REG		register number
;
; Make store the fall-through path, so we ensure the issue of
; mfpr mm_stat before any following loads.
;
	blbc	p6, trap__unaligned_check	; load => check x31
	br	r31, trap__unaligned_store	; mm_stat<0>=1 => store

trap__unaligned_check:
	cmpeq	p7, #^x1F, p7			; check for x31
	bne	p7, trap__unaligned_dismiss	; dismiss load x31
;
; Unaligned load quad, long, or word. Branch out for quad and word.
; Once we do the virtual reference, we no longer can depend on p4-p7 because
; either/both loads could trap!
;
; Do a MB to ensure we have all the mfpr instructions issued.
;
	mb

	srl	p5, #5, p4			; to test for word
	blbc	p4, trap__unaligned_ldwu	; branch for ldwu
	blbs	p5, trap__unaligned_ldq		; branch for ldq
;
; Do unaligned long.
;
	ldq_u	r2, (p20)			; r2 = CBAx xxxx (first half)
	ldq_u	r3, 3(p20)			; r3 = yyyy yyyD (second half)
	extll	r2, p20, r2			; r2 = 0000 0CBA (extract)
	extlh	r3, p20, r3			; r3 = 0000 D000 (extract)
	or	r2, r3, r3			; r3 = 0000 DCBA (combine)
	addl	r3, r31, r3			; r3 = ssss DCBA (sign-extend)
	br	r31, trap__unaligned_load_merge	; merge with other flows
;
; Unaligned load word. Once we do the virtual reference, we can't depend
; on p4-p7. Either/both loads could trap! Note that the load is zero-extended.
;
trap__unaligned_ldwu:
	ldq_u	r2, (p20)			; r2 = yBAx xxxx(first half)
	ldq_u	r3, 1(p20)			; r3 = yBAx xxxx(second half)
	extwl	r2, p20, r2			; r2 = 0000 00BA (extract)
	extwh	r3, p20, r3			; r3 = 0000 0000 (extract)
	or	r2, r3, r3			; r3 = 0000 00BA (combine)
	br	r31, trap__unaligned_load_merge	; merge to continue
;
; Unaligned load quad. Once we do the virtual reference, we can't depend
; on p4-p7. Either/both loads could trap!
;
trap__unaligned_ldq:
	ldq_u	r2, (p20)			; r2 = CBAx xxxx (first half)
	ldq_u	r3, 7(p20)			; r3 = yyyH GFED (second half)
	extql	r2, p20, r2			; r2 = 0000 0CBA (extract)
	extqh	r3, p20, r3			; r3 = HGFE D000 (extract)
	or	r2, r3, r3			; r3 = HGFE DCBA (combine)
	br	r31, trap__unaligned_load_merge	; merge to continue
;
; Merge from quad, long, word.
; Current state:
;
;	r1		available
;	r2		available
;	r3		load data
;	r8		available
;	r9		available
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;	PT__OPCODE	opcode
;	PT__FAULT_PC	next pc
;	PT__REG		register number
;
; Determine whether integer or float. If integer, need to turn off
; shadow mode.
;
; Note: The ev4 and ev5 PALcodes do something really bogus here. They check
; the current sp for accessibility before writing to the register that is
; the target of the load (i.e., changing state). I believe this check is
; being done on the behalf of VMS, which will be posting the condition back
; to the application using the current mode stack. *NOWHERE* is this stated
; in the SRM. Since an unaligned store is allowed to do a partial result,
; this check does not have to be done for stores.
;

EV6__I_CTL__SDE7__S = 7				; sde bit

ild_table_offset = <<trap__ild_table - trap__pal_base>>


trap__unaligned_load_merge:
	bic	r30, #63, r1			; align the current stack
	stq	r31, -64(r1)			; probe the current stack

	hw_ldq/p p5, PT__OPCODE(p_temp)		; get back opcode
	hw_ldq/p r2, PT__REG(p_temp)		; get back register number
	and	p5, #8, p4			; test for float
	beq	p4, trap__unaligned_float_load	; go off for float

	hw_mfpr	p6, EV6__PAL_BASE		; (4,0L) need pal base
	ldah	p6,<<ild_table_offset>+32768>@-16(p6)
	lda	p6,<<ild_table_offset> & ^xFFFF>(p6)
	sll	r2, #3, r2			; table is 2 inst per register
	addq	r2, p6, r2			; index into load table
;
; Save p_temp in r9, so we can get to pal temps when we turn off shadow mode.
; Turn off shadow mode.
;
	ALIGN_FETCH_BLOCK <^x47FF041F>

	bis	p_temp, r31, r9			; save p_temp
	hw_mfpr	r1, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r1, #<1@EV6__I_CTL__SDE7__S>, r1; zap sde
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now shadow mode is off. Go load the integer register file.
;
trap__unaligned_ild_jmp:
	bis	r2, #1, r2			; PALmode
	bsr	r31, .				; push prediction stack
	PVC_VIOLATE <29>
	PVC_JSR ild_table			; go off to load
	hw_ret	r31, (r2)			; pop prediction stack

	ALIGN_FETCH_BLOCK <^x47FF041F>

trap__unaligned_ild_return:
	hw_mfpr	r1, EV6__I_CTL			; (4,0L) get i_ctl back
	bis	r1, #<1@EV6__I_CTL__SDE7__S>, r1; or in sde
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now shadow mode is back on.
;
; Current state:
;	p20		va
;
;	PT__FAULT_PC	next pc
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;
; See if we need to report the unaligned event.
;

trap__unaligned_fld_return:
	hw_ldq/p p4, PT__PCBB(p_temp)		; get PCB base
	hw_ldq/p p23, PT__FAULT_PC(p_temp)	; get exc_addr back

	hw_ldq/p r1, PT__R1(p_temp)		; restore r1
	hw_ldq/p r2, PT__R2(p_temp)		; restore r2
	hw_ldq/p r3, PT__R3(p_temp)		; restore r3
	hw_ldq/p r8, PT__R8(p_temp)		; restore r8
	hw_ldq/p r9, PT__R9(p_temp)		; restore r9

	hw_ldq/p p4, PCB__FEN(p4)		; read DAT/PME/FEN quadword
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	srl	p4, #PCB__DAT__S, p4		; isolate DAT bit
	blbc	p4, trap__unaligned_load_report	; if clear, report the trap
	hw_ret_stall (p23)			; return (stall for pvc)
;
; Report unaligned load.
;
; Current state:
;	p20		va
;	p23		next pc
;
;	PT__FAULT_PC	next pc
;
trap__unaligned_load_report:
	lda	p4, SCB__UNALIGN(r31)		; scb offset
	hw_stq/p p20, PT__FAULT_R4(p_temp)	; write va
	hw_stq/p r31, PT__FAULT_R5(p_temp)	; indicate read operation
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; write scb offset
	br	r31, trap__post_km_r45		; post
;+
; Integer load table. Branched to from unaligned load integer to
; load the correct integer register.
;
; Current state:
;	r3		actual data
;	r9		p_temp
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;-

;
; For now, put pvc labels on all the entries. If pvc runs too slowly,
; take them out.
;
	.align quad
trap__ild_table:
	PVC_JSR ild_table, dest=1
	bis	r3, r31, r0
	br	r31, trap__unaligned_ild_return

	PVC_JSR ild_table, dest=1
	hw_stq/p r3, PT__R1(r9)
	br	r31, trap__unaligned_ild_return

	PVC_JSR ild_table, dest=1
	hw_stq/p r3, PT__R2(r9)
	br	r31, trap__unaligned_ild_return

	PVC_JSR ild_table, dest=1
	hw_stq/p r3, PT__R3(r9)
	br	r31, trap__unaligned_ild_return

	PVC_JSR ild_table, dest=1
	bis	r3, r31, r4
	br	r31, trap__unaligned_ild_return

	PVC_JSR ild_table, dest=1
	bis	r3, r31, r5
	br	r31, trap__unaligned_ild_return

	PVC_JSR ild_table, dest=1
	bis	r3, r31, r6
	br	r31, trap__unaligned_ild_return

	PVC_JSR ild_table, dest=1
	bis	r3, r31, r7
	br	r31, trap__unaligned_ild_return

	PVC_JSR ild_table, dest=1
	hw_stq/p r3, PT__R8(r9)
	br	r31, trap__unaligned_ild_return

	PVC_JSR ild_table, dest=1
	hw_stq/p r3, PT__R9(r9)
	br	r31, trap__unaligned_ild_return

.macro ILD_JUMP_TABLE t
	PVC_JSR ild_table, dest=1
	bis	r3, r31, r't'
	br	r31, trap__unaligned_ild_return
.endm	ILD_JUMP_TABLE

	t=10
	.repeat 22
	ILD_JUMP_TABLE	\t
	t=t+1
	.endr

;+
; Unaligned float load.
;
; Current state:
;	r1		available
;	r2		register number
;	r3		load data
;	r8		available
;	r9		available
;
;	p5		opcode
;	p20		va
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;	PT__OPCODE	opcode
;	PT__FAULT_PC	next pc
;	PT__REG		register number
;
; Even though I can use itofs and itoft for s and t, it's work to
; separate out the cases, and probably not worth the bother.
;-

fld_table_offset = <<trap__fld_table - trap__pal_base>>

	ALIGN_CACHE_BLOCK
trap__unaligned_float_load:
	and	p5, #3, p5			; get ieee, quad flags
	sll	r2, #5, r2			; 4*2 instructions per register
	s8addq	p5, r2, r2			; 2 instructions per data type

	lda	r1, -8(r30)			; allocate a quadword buffer
	bic	r1, #7, r1			; align buffer
	stq	r3, (r1)			; put data on stack (can trap)

	hw_mfpr	p6, EV6__PAL_BASE		; (4,0L) need pal base
	ldah	p6,<<fld_table_offset>+32768>@-16(p6)
	lda	p6,<<fld_table_offset> & ^xFFFF>(p6)
	addq	r2, p6, r2			; address of subroutine

	bis	r2, #1, r2			; PALmode
	bsr	r31, .				; push prediction stack
	PVC_VIOLATE <29>
	PVC_JSR	fld_table			; go off to load
	hw_ret	r31, (r2)			; pop prediction stack
						; return to check DAT code
						; at trap__unaligned_fld_return
;+
; Float load table. Branched to from unaligned load float to
; load the correct float register.
;
; 0*2i	vax F
; 1*2i	vax G
; 2*2i	ieee S
; 3*2i	ieee T
;
; Current state:
;	r1		address of data
;
; We don't need an entry for f31, since we dismiss those.
;
;-

;
; For now, put pvc labels on all the entries. If pvc runs too slowly,
; take them out.
;


;
; Macro to produce jump table
;
.macro FLD_JUMP_TABLE t
	PVC_JSR	fld_table, dest=1
	ldf	f't', (r1)				; vax F
	br	r31, trap__unaligned_fld_return

	PVC_JSR	fld_table, dest=1
	ldg	f't', (r1)				; vax G
	br	r31, trap__unaligned_fld_return

	PVC_JSR	fld_table, dest=1
	lds	f't', (r1)				; ieee S
	br	r31, trap__unaligned_fld_return

	PVC_JSR	fld_table, dest=1
	ldt	f't', (r1)				; ieee T
	br	r31, trap__unaligned_fld_return
.endm	FLD_JUMP_TABLE
	
	.align quad
trap__fld_table:
	t=0
	.repeat 32
	FLD_JUMP_TABLE	\t
	t=t+1
	.endr

;+
; Unaligned load to r31/f31. Dismiss. Although we saved r2 and r3,
; we never touched them, so we don't need to restore.
; We also need to clear the trap catcher.
;
; We don't need a MB to hold up loads that may affect MM_STAT.
; To get here, we have taken conditional branches that on the
; mfpr MM_STAT being issued.
;
; Current state:
;	p23		next pc
;-
trap__unaligned_dismiss:
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p23)				; return

;+
; Unaligned store.
;
; Current state:
;	p5		opcode
;	p7		register number
;	p20		va
;
;	r1		available
;	r2		available
;	r3		available
;	r8		available
;	r9		available
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;	PT__TRAP	set up for trap
;	PT__FAULT_PC	next pc
;
; Do a MB to ensure we have all the mfpr instructions issued.
;-

ist_table_offset = <<trap__ist_table - trap__pal_base>>

	ALIGN_CACHE_BLOCK
trap__unaligned_store:
	mb
	hw_mfpr	r2, EV6__PAL_BASE		; (4,0L) need pal base
	and	p5, #8, p4			; check for float
	beq	p4, trap__unaligned_float_store	; go off for float
;
; Save p_temp to r9 before we turn off shadow mode.
; We need to turn off shadow mode to get the register value.
;
	bis	p_temp, r31, r9			; r9 gets p_temp value
	hw_mfpr	r1, EV6__I_CTL			; (4,0L) get i_ctl
	ldah	r2,<<ist_table_offset>+32768>@-16(r2)
	lda	r2,<<ist_table_offset> & ^xFFFF>(r2)

	sll	p7, #3, p6			; table is 2 inst per register
	addq	r2, p6, r2			; index into table
	bic	r1, #<1@EV6__I_CTL__SDE7__S>, r1; zap sde
	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

;
; Now shadow mode is off. Get the data to store. Data will be returned
; in r3.
;
trap__unaligned_ist_jmp:
	bis	r2, #1, r2			; PALmode
	bsr	r31, .				; push prediction stack
	PVC_VIOLATE <29>
	PVC_JSR	ist_table			; off to get register data
	hw_ret	r31, (r2)			; pop prediction stack

	ALIGN_FETCH_BLOCK <^x47FF041F>

trap__unaligned_ist_return:
	
	hw_mfpr	r1, EV6__I_CTL			; (4,0L) get i_ctl back
	bis	r1, #<1@EV6__I_CTL__SDE7__S>, r1; or in sde
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now shadow mode is back on.
;
; Current state:
;	p5		opcode
;	p20		va
;
;	r1		available
;	r2		available
;	r3		data
;	r8		available
;	r9		available
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;	PT__TRAP	set up for trap
;	PT__FAULT_PC	next pc
;
; Unaligned store quad, long, or word. Branch out for quad and word.
; Once we do the virtual reference, we no longer can depend on p4-p7 because
; either/both loads could trap! Once the references are made, we can go
; back to using p4-p7.
;
; After doing the store, see if we need to report the unaligned event.
;

trap__unaligned_store_store:
	srl	p5, #5, p4			; to test for word
	blbc	p4, trap__unaligned_stqw	; branch for stqw
	blbs	p5, trap__unaligned_stq		; branch for stq, stg, stt
;
; Unaligned store long.
;
; Do first half (note that since we store before the second half, the
; order does not matter) of stl, stf, or sts
;
	ldq_u	r1, (p20)			; r1 = CBAx xxxx
	insll	r3, p20, r2			; r2 = cba0 0000
	mskll	r1, p20, r1			; r1 = 000x xxxx
	or	r2, r1, r2			; r2 = cbax xxxx
	stq_u	r2, (p20)			; store first half
;
; Do second half.
;
	ldq_u	r1, 3(p20)			; r1 = yyyy yyyD
	inslh	r3, p20, r2			; r2 = 0000 000d
	msklh	r1, p20, r1			; r1 = yyyy yyy0
	or	r2, r1, r2			; r2 = yyyy yyyd
	stq_u	r2, 3(p20)			; store second half
	br	r31, trap__unaligned_store_merge
;
; Unaligned store word
;
; Do first half (note that since we store before the second half, the
; order does not matter.
;

trap__unaligned_stqw:
	ldq_u	r1, (p20)			; r1 = yBAx xxxx
	inswl	r3, p20, r2			; r2 = 0ba0 0000
	mskwl	r1, p20, r1			; r1 = y00x xxxx
	or	r2, r1, r2			; r2 = ybax xxxx
	stq_u	r2, (p20)			; store first half
;
; Do second half.
;
	ldq_u	r1, 1(p20)			; r1 = yBAx xxxx
	inswh	r3, p20, r2			; r2 = 0000 0000
	mskwh	r1, p20, r1			; r1 = yBAx xxxx
	or	r2, r1, r2			; r2 = yBAx xxxx
	stq_u	r2, 1(p20)			; store second half
	br	r31, trap__unaligned_store_merge
;
; Unaligned store quad
;
; Do first half (note that since we store before the second half, the
; order does not matter.
;

trap__unaligned_stq:
	ldq_u	r1, (p20)			; r1 = CBAx xxxx
	insql	r3, p20, r2			; r2 = cba0 0000
	mskql	r1, p20, r1			; r1 = 000x xxxx
	or	r2, r1, r2			; r2 = cbax xxxx
	stq_u	r2, (p20)			; store first half
;
; Do second half.
;
	ldq_u	r1, 7(p20)			; r1 = yyyH GFED
	insqh	r3, p20, r2			; r2 = 000h gfed
	mskqh	r1, p20, r1			; r1 = yyy0 0000
	or	r2, r1, r2			; r2 = yyyh gfed
	stq_u	r2, 7(p20)			; store second half
	br	r31, trap__unaligned_store_merge
;
; Unaligned store merge. Check the DAT bit to see if we should report
; the trap.
;

trap__unaligned_store_merge:
	hw_ldq/p r1, PT__R1(p_temp)		; restore r1
	hw_ldq/p r2, PT__R2(p_temp)		; restore r2

	hw_ldq/p p4, PT__PCBB(p_temp)		; get PCB base
	hw_ldq/p p23, PT__FAULT_PC(p_temp)	; get back next pc

	hw_ldq/p r3, PT__R3(p_temp)		; restore r3
	hw_ldq/p r8, PT__R8(p_temp)		; restore r8
	hw_ldq/p r9, PT__R9(p_temp)		; restore r9

	hw_ldq/p p4, PCB__FEN(p4)		; read DAT/PME/FEN quadword
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	srl	p4, #PCB__DAT__S, p4		; isolate DAT bit
	blbc	p4, trap__unaligned_store_report; if clear, report the trap
	hw_ret_stall (p23)			; return (stall for pvc)
;
; Report unaligned store
;
; Current state:
;
;	p20		va
;
;	PT__FAULT_PC	next pc
;

trap__unaligned_store_report:
	lda	p5, 1(r31)			; indicate a write
	lda	p4, SCB__UNALIGN(r31)		; scb offset
	hw_stq/p p20, PT__FAULT_R4(p_temp)	; write va
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; write scb offset
	hw_stq/p p5, PT__FAULT_R5(p_temp)	; indicate write
	br	r31, trap__post_km_r45		; post
;+
; Integer store table. Branched to from unaligned store integer to
; fetch the register data.
;
; Current state:
;	r9		p_temp
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;
; Exit state:
;	r3	store data
;-

;
; For now, put pvc labels on all the entries. If pvc runs too slowly,
; take them out.
;
	.align quad
trap__ist_table:
	PVC_JSR ist_table, dest=1
	bis	r0, r31, r3
	br	r31, trap__unaligned_ist_return

	PVC_JSR ist_table, dest=1
	hw_ldq/p r3, PT__R1(r9)
	br	r31, trap__unaligned_ist_return

	PVC_JSR ist_table, dest=1
	hw_ldq/p r3, PT__R2(r9)
	br	r31, trap__unaligned_ist_return

	PVC_JSR ist_table, dest=1
	hw_ldq/p r3, PT__R3(r9)
	br	r31, trap__unaligned_ist_return

	PVC_JSR ist_table, dest=1
	bis	r4, r31, r3
	br	r31, trap__unaligned_ist_return

	PVC_JSR ist_table, dest=1
	bis	r5, r31, r3
	br	r31, trap__unaligned_ist_return

	PVC_JSR ist_table, dest=1
	bis	r6, r31, r3
	br	r31, trap__unaligned_ist_return

	PVC_JSR ist_table, dest=1
	bis	r7, r31, r3
	br	r31, trap__unaligned_ist_return

	PVC_JSR ist_table, dest=1
	hw_ldq/p r3, PT__R8(r9)
	br	r31, trap__unaligned_ist_return

	PVC_JSR ist_table, dest=1
	hw_ldq/p r3, PT__R9(r9)
	br	r31, trap__unaligned_ist_return

.macro IST_JUMP_TABLE t
	PVC_JSR ist_table, dest=1
	bis	r't', r31, r3
	br	r31, trap__unaligned_ist_return
.endm	IST_JUMP_TABLE

	t=10
	.repeat 22
	IST_JUMP_TABLE	\t
	t=t+1
	.endr

;+
; Unaligned float store.
;
; Current state:
;
;	p5		opcode
;	p7		register
;	p20		va
;
;	r1		available
;	r2		pal base
;	r3		available
;	r8		available
;	r9		available
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;	PT__TRAP	set up for trap
;	PT__FAULT_PC	next pc
;
;-

fst_table_offset = <<trap__fst_table - trap__pal_base>>

trap__unaligned_float_store:
	and	p5, #3, p5			; get ieee, quad flags
	sll	p7, #5, p7			; 4*2 instructions per register
	s8addq	p5, p7, p7 			; 2 instructions per data type

	lda	r1, -8(r30)			; allocate a quadword buffer
	bic	r1, #7, r1			; align buffer

	ldah	r2,<<fst_table_offset>+32768>@-16(r2)
	lda	r2,<<fst_table_offset> & ^xFFFF>(r2)
	addq	r2, p7, r2			; address of subroutine

	bis	r2, #1, r2			; PALmode
	bsr	r31, .				; push prediction stack
	PVC_VIOLATE <29>
	PVC_JSR	fst_table			; go off to load
	hw_ret	r31, (r2)			; pop prediction stack

trap__unaligned_fst_return:
	ldq	r3, (r1)			; fetch formatted data
	hw_ldq/p p5, PT__OPCODE(p_temp)		; get opcode back
	br	r31, trap__unaligned_store_store; merge with integer to store
;+
; Float store table. Branched to from unaligned load store to store float
; to the stack.
;
; 0*2i	vax F
; 1*2i	vax G
; 2*2i	ieee S
; 3*2i	ieee T
;
; Current state:
;	r1		address of data
;
; We don't need an entry for f31, since we dismiss those.
;
;-

;
; For now, put pvc labels on all the entries. If pvc runs too slowly,
; take them out.
;

;
; Macro to produce jump table
;
.macro FST_JUMP_TABLE t
	PVC_JSR fst_table, dest=1
	stf	f't', (r1)				; vax F
	br	r31, trap__unaligned_fst_return

	PVC_JSR fst_table, dest=1
	stg	f't', (r1)				; vax G
	br	r31, trap__unaligned_fst_return

	PVC_JSR fst_table, dest=1
	sts	f't', (r1)				; ieee S
	br	r31, trap__unaligned_fst_return

	PVC_JSR fst_table, dest=1
	stt	f't', (r1)				; ieee T
	br	r31, trap__unaligned_fst_return
.endm	FST_JUMP_TABLE
	
	.align quad
trap__fst_table:
	t=0
	.repeat 32
	FST_JUMP_TABLE	\t
	t=t+1
	.endr

;+
; Illegal unaligned opcode (_C, _L). Although we saved r2 and r3,
; we never touched them, so we don't need to restore.
;
; We had not yet set up the trap catcher, so we don't need to clear it.
;
; Current state:
;	p23	exc_addr
;
;-
trap__unaligned_illegal:
	addq	p23, #4, p23			; point to next pc
	lda	p4, SCB__ILLOP(r31)		; illegal operand
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save next pc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; save scb offset
	br	r31, trap__post_km		; post the trap
;+
; Unaligned_dfault
;
; Entry:
;	A dfault or TNV was detected during unalign processing.
;
; Function:
;	Report the error.
;
; Current state:
;	p5	mm_stat
;	p6	va
;	r25	need to restore
;	r26	need to restore
;
;	PT__R1		saved r1
;	PT__R2		saved r2
;	PT__R3		saved r3
;	PT__R8		saved r8
;	PT__R9		saved r9
;
;	PT__FAULT_PC	next pc for unalign
;	PT__FAULT_SCB	SCB offset
;	PT__TRAP	trap handler
;
; Exit state:
;	On exit to trap__post_km_r45
;	r1-r3			restored
;	r8-r9			restored
;	r25			restored
;	r26			restored
;	PT__FAULT_PC		pc of original faulting instruction
;	PT__FAULT_SCB		scb offset
;	PT__FAULT_R4		fault va
;	PT__FAULT_R5		memory management flags
;	PT__TRAP		cleared
;-

	PVC_JSR pal_mm_dispatch, dest=1
trap__unaligned_dfault:
	hw_ldq/p p23, PT__FAULT_PC(p_temp)	; get back pc to decrement it
	sll	p5, #63, p5			; r/w bit to mmf position

	hw_ldq/p r1, PT__R1(p_temp)		; restore r1
	hw_ldq/p r2, PT__R2(p_temp)		; restore r2
	hw_ldq/p r3, PT__R3(p_temp)		; restore r3
	hw_ldq/p r8, PT__R8(p_temp)		; restore r8
	hw_ldq/p r9, PT__R9(p_temp)		; restore r9

	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	hw_stq/p p5, PT__FAULT_R5(p_temp)	; save mmf
	hw_stq/p p6, PT__FAULT_R4(p_temp)	; save va

	subq	p23, #4, p23			; pc back to instruction
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save fault pc
	br	r31, trap__post_km_r45		; post

	END_HW_VECTOR

;+
; DTBM_SINGLE - offset 300
;
; Entry:
;	Vectored into via hardware trap on Dstream single
;	translation buffer miss.
;
; Function:
;	Do a virtual fetch of the PTE, and fill the DTB if the PTE is valid.
;	Can trap into double.
;
; Note:
;	We want to keep the main flow in a single cache line
;	(4 fetch blocks) to be as efficient as possible.
;
;	The update of the TB occurs on the when the write to PTE1 retires.
;
; NOTE:
;	1-to-1 mapping scheme implemented with bit in p_misc shadow register.
;
; Register use:
;	p4	shadow register reserved for itb/dtb miss
;	p5	shadow register reserved for itb/dtb miss
;	p6	shadow register reserved for itb/dtb miss
;	p7	shadow register reserved for itb/dtb miss
;	p_misc	physical mode bit in <63>
;	p23	exc_addr, pc of instruction causing DTB miss
;
; Exit state:
;	On exit to trap__invalid_dpte
;	p4	PTE
;	p5	no double miss: mm_stat 
;		double miss: scratch
;	p6	va
;	p7	no double miss: exc_sum 
;		double miss: <31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction causing DTB miss
;
;
; Note on scoreboarding and register dependencies: Having (4-7) on the
; VA/VA_FORM register reads ensures that they don't issue until all of the
; previous dtb miss's tag/pte writes have retired. The register dependency
; between the VA/VA_FORM read and the tag/pte writes ensures that none
; of the tag/pte issue until all of the previous tag/pte writes from
; a previous miss have retired.
;
; How to squeeze 5 pegs into 4 holes:
; 	Before fetching the vpte, we clear the low bit of p7.
;	If we take a double miss, we squeeze mm_stat and exc_sum into p7,
;		setting the low bit to indicate we took a double miss.
;	The invalid_pte code checks p7 and restores p5 if indicated.
;-
	START_HW_VECTOR <DTBM_SINGLE>

ASSUME P_MISC__PHYS__S eq 63

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) get exception address
	hw_mfpr	p4, EV6__VA_FORM		; (4-7,1L) get vpte address
	hw_mfpr	p5, EV6__MM_STAT		; (0L) get miss info
	hw_mfpr p7, EV6__EXC_SUM		; (0L) get exc_sum for ra

	hw_mfpr p6, EV6__VA			; (4-7,1L) get original va
	bic	p7, #1, p7			; clear double miss flag

	xor	p4, p6, p4			; interlock p4 and p6
	xor	p4, p6, p4			; restore p4

trap__dtbm_single_vpte:
	hw_ldq/v p4, (p4)			; (1L) get vpte

	blt	p_misc, trap__d1to1		; (xU) <63>=1 => 1-to-1
	blbc	p4, trap__invalid_dpte		; (xU) invalid => branch
;
; The following code is all inserted as part of 1.56
;
; For the force_path2 hack, we either need to do a virtual operation to
; clear a lock, or do mulq's that hold up loads until the hw_ret
; mispredicts.
;
; 1.71(2) Change ldbu r31 to ldbu p7
; Bug we have to protect against: A ldx_l dtb misses, the replayed ldx_l
; actually issues BEFORE the ldbu, the block is taken by another cpu, the ldbu
; actually issues AFTER the replayed ldx_l, inadvertenly pulls
; back in the data block and thus causes an incorrectly succceeded stx_c.
; We are vulnerable to this case because the ldbu to r31 can issue AFTER the
; replayed ldx_l. Changing the register to non-r31 forces the loads
; to go in order.
;
; What does the ldbu to a non-r31 register do? If the ldbu has a non-r31
; destination, it hits a litmus test that forces two loads to the same address
; to go in order, so the ldbu has to issue before the replayed ldx_l.
;
; Why can the ldbu to r31 cause a problem? The ldbu r31 is a prefetch.
; Even though the ldbu has the same address as the ldx_l, if the destination
; is r31, it does not hit the litmus test and is not forced to issue in order.
;
; Now, why would the the ldbu r31 not issue before the replayed ldx_l?
; There is a st-wait table indexed by 10 bits of the pc that tries to keep
; store-load traps at a minimum by remembering that a load trap occurred.
; The next time the load shows up (or any load with the same index bits), it
; does not issue until all the older unissued stores are issued. Thus the
; ldbu could be held up while the ldx_l issues, thus making it issue AFTER
; the replayed ldx_l.
;

.if eq force_path2				; 1.56
	and	p4, #^x80, p7			; isolate mb bit
	xor	p7, #^x80, p7			; flip mb bit
.iff						; 1.56
	srl	p4, #62, p5			; get i/o space bit
	and	p5, #1, p5			; 1.63 isolate i/o space bit
	and	p4, #^xe0, p7			; 1.63 isolate mb, GH bits
	xor	p7, #^x80, p7			; 1.63 flip mb bit
	bis	p7, p5, p7			; 1.63 merge bits
.endc						; 1.56

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p6, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p6, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	p4, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	p4, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

						; 1.63 deleted 4 lines
ASSUME <tb_mb_en + pte_eco> ne 2

.if ne pte_eco
	bne	p7, trap__dtbm_single_gh_mb_io	; 1.56 branch for mb

  .if ne force_path2				; 1.56
	xor	p6, #^x08, p6			; 1.73 change addr slightly
trap__dtbm_single_hack1:
	ldbu	p7, (p6)			; 1.71(2) do a virtual operation
  .iff						; 1.73
	PVC_VIOLATE <44>			; 1.73
  .endc						; 1.56 force_path2
	hw_ret (p23)				; return

  .if ne force_path2				; 1.56
	CONT_HW_VECTOR
  .endc						; 1.56

trap__dtbm_single_gh_mb_io:
  .if ne force_path2
	and	p7, #^x81, p5			; check io,mb bits
	bne	p5, trap__dtbm_single_mb_io	; continue for GH case
	EV6_MTPR p6, <EV6__VA ! ^x22>		; 1.70 hold up loads

	ALIGN_FETCH_BLOCK <^x47FF041F>
	xor	p6, #^x08, p6			; 1.73 change addr slightly
trap__dtbm_single_hack2:
	ldbu	p7, (p6)			; 1.71(2)
	hw_ret	(p23)				; return

trap__dtbm_single_mb_io:
	blbs	p7, trap__dtbm_single_io	; continue for mb case
	mb
	EV6_MTPR p6, <EV6__VA ! ^x22>		; 1.70 hold up loads

	ALIGN_FETCH_BLOCK <^x47FF041F>
	xor	p6, #^x08, p6			; 1.73 change addr slightly
trap__dtbm_single_hack3:
	ldbu	p7, (p6)			; 1.71(2)
	hw_ret	(p23)				; return

trap__dtbm_single_io:
  .endc						; 1.56 force_path2
	mb
	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p23)				; return

.iff						; 1.56 no need to be clever

  .if ne force_path2				; 1.56
	mb
	ALIGN_FETCH_BLOCK <^x47FF041F>
	mulq	p6, #1, p6			; hold up loads
	mulq	p6, #1, p6			; hold up loads
	hw_mtpr	p6, <EV6__VA ! ^x22>		; hold up loads
  .iff						; 1.73
	PVC_VIOLATE <44>			; 1.73
  .endc						; 1.56

	PVC_VIOLATE <43>			; 1.71
	hw_ret(p23)				; return

.endc						; 1.56 pte_eco

.if eq force_path2				; 1.56
	CONT_HW_VECTOR
.endc						; 1.56


;+
; trap__invalid_dpte
;
; Entry:
;	Branched to on invalid Dstream PTE fetched by DTB miss routine.
;
; Function:
;	Prepare to take a Translation Not Valid or Access Violation
;	exception. Take a different flow for pal mode.
;
; Current state:
;	p4	PTE
;	p5	no double miss: mm_stat 
;		double miss: scratch
;	p6	va
;	p7	no double miss: exc_sum 
;		double miss: <31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction taking DTB miss
;
; Register use:
;	p20	available once we know we didn't trap from pal mode
;
; Exit state:
;	On exit to trap__tnv_in_pal
;	p4	PTE with <17:16>=^b11 (level 3)
;	p5	mm_stat
;	p6	va
;	p7	exc_sum
;	p23	pc of instruction taking DTB miss
;
;	r25	saved and available
;	r26	saved and available
;
;	On exit to trap__post_km_r45
;	PT__FAULT_PC		pc of instruction causing TB miss
;	PT__FAULT_SCB		scb offset
;	PT__FAULT_R4		fault va
;	PT__FAULT_R5		memory management flags
;-
trap__invalid_dpte:
	blbs	p7, trap__invalid_dpte_get	; did we double miss?
	br	r31, trap__invalid_dpte_have	; we didn't

trap__invalid_dpte_get:				; we double missed
	srl	p7, #16, p5			; recover mm_stat

trap__invalid_dpte_have:
	blbs	p23, trap__level3_tnv_in_pal	; level 3 TNV in pal mode
	srl	p_misc, #P_MISC__CM__S, p20	; get current mode
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store fault pc
	hw_stq/p p6, PT__FAULT_R4(p_temp)	; store fault va, free up p6

	and	p20, #P_MISC__CM__M, p20	; clean mode
	addq	p20, #PTE__KRE__S, p20		; current mode re bit position

	blbc	p5, trap__invalid_dpte_check	; load => check x31

	srl	p5, #EV6__MM_STAT__OPCODE__S, p7	; get opcode
	and	p7, #EV6__MM_STAT__OPCODE__M, p7	; clean it
	cmpeq	p7, #^x18, p7				; ECB/WH64?
	bne	p7, trap__invalid_dpte_dismiss		; dismiss if so

	br	r31, trap__invalid_dpte_cont	; store
;
; Check for load x31
;
trap__invalid_dpte_check:
	srl	p7, #EV6__EXC_SUM__REG__S, p7	; get ra field
	and	p7, #EV6__EXC_SUM__REG__M, p7	; check for load x31
	cmpeq	p7, #^x1F, p7			; compare against x31
	bne	p7, trap__invalid_dpte_dismiss	; branch => dismiss
;
; Post the trap.
;
trap__invalid_dpte_cont:
	and	p5, #1, p5			; get mm_stat<0>
	sll	p5, #63, p7			; read/write flag for mmf
	s4addq	p5, r31, p6			; 0=>read, 4=>write
	hw_stq/p p7, PT__FAULT_R5(p_temp)	; store mmf
	bis	r31, #1, p5			; get a 1
	addq	p6, p20, p6			; mode/rw bit position
	sll	p5, p6, p6			; create mask

	lda	p20, SCB__TNV(r31)		; assume TNV
	and	p6, p4, p6			; get enable bit
	cmoveq	p6, #SCB__ACV, p20		; take ACV over TNV
	hw_stq/p p20, PT__FAULT_SCB(p_temp)	; store scb offset
	br	r31, trap__post_km_r45		; take the trap
;
; Dismiss the load to x31. We don't need a MB to hold up loads that
; may affect MM_STAT. To get here, we have taken a conditional branch
; that depended on the mfpr MM_STAT being issued.
;
trap__invalid_dpte_dismiss:
	addq	p23, #4, p23			; dismiss, so increment pc
	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p23)				; return
;
; We have an invalid level 3 PTE with the exception from pal mode
; Current state:
;	p4	PTE
;	p5	mm_stat
;	p6	va
;	p7	exc_sum
;	p23	pc of instruction causing DTB miss
; Get some scratch space, set the level 3 marker in the PTE and
; branch to common tnv_in_pal code.
;
trap__level3_tnv_in_pal:
	hw_stq/p r25, PT__R25(p_temp)		; get some scratch space
	hw_stq/p r26, PT__R26(p_temp)		; get some scratch space
	ldah	r25, <<3@PTE__SOFT__S>+32768>@-16(r31)
	bis	p4, r25, p4			; set level 3 marker
	br	r31, trap__tnv_in_pal		; branch to common code
;+
; trap__tnv_in_pal
;
; Entry:
;	Branched to on TNV in pal mode, either from invalid dpte
;	or from the double flow.
;
; Function:
;	Prepare to dispatch.
;
; Current state:
;	p4	PTE with correct level 3 marker
;		<17:16>=^b11	=> level 3 (from invalid dpte)
;		<17:16>=^b00	=> level 1 or level 2 (from double)
;	p5	mm_stat
;	p6	va of DTB miss
;	p7	available
;	p23	pc of PALcode instruction causing DTB miss
;			could be from call_pal
;			could be stack access (KSP not valid!)
;	r25	saved and available
;	r26	saved and available
;
; Exit state:
;	On exit to trap__pal_mm_dispatch
;	r25		saved and available
;	r26		pc of PALcode instruction with <1:0>=0
;
;	PT__FAULT_SCB	scb offset indicating TNV or ACV
;
;-
	ALIGN_CACHE_BLOCK
trap__tnv_in_pal:
	and	p5, #1, r26			; mm_stat<0>
	s4addq	r26, r31, r26			; 0=>read, 4=>write
	srl	p_misc, #P_MISC__CM__S, r25	; get current mode
	and	r25, #P_MISC__CM__M, r25	; clean current mode
	addq	r25, #PTE__KRE__S, r25		; read enable bit position
	addq	r26, r25, r26			; r/w enable bit position

	srl	p4, #PTE__SOFT__S, r25		; get level 3 marker
	cmovlbc	r25, #PTE__KRE__S, r26		; if level 1/2, just check kre

	bis	r31, #1, r25			; get a 1
	sll	r25, r26, r26			; mask for enable check
	and	r26, p4, r26			; do the check
	lda	p7, SCB__TNV(r31)		; assume TNV
	cmoveq	r26, #SCB__ACV, p7		; take ACV over TNV
	hw_stq/p p7, PT__FAULT_SCB(p_temp)	; store scb
	bic	p23, #3, r26			; clean pc
	br	r31, trap__pal_mm_dispatch	; dispatch

;+
; Do a 1-1 mapping
; Current state:
;	p6	va
;	p23	exc_addr

; We don't need a MB to hold up loads that may affect VA, etc.
; The writes to DTB_TAGx and DTB_PTE0 hold up loads.
;
; We are adding a scoreboard interlock for to avoid speculative reads
; during powerup. This way, we don't return until we have committed th
; DTB write.
;-
trap__d1to1:
	lda	p5, ^xFF01(r31)			; all r/w enable
	zapnot	p5, #3, p5			; clean mask up
	srl	p6, #13, p4			; shift out the byte offset
.if ne	nonzero_console_base ! tsunami_platform
	br	r31, trap__d1to1_system
trap__d1to1_return:
.endc

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
.endc						; 1.70
	sll	p4, #EV6__DTB_PTE0__PFN__S, p4	; get pfn into position
	bis	p4, p5, p4			; produce the pte

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p6, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p6, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	p4, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	p4, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

	EV6_MTPR r31, <EV6__MM_STAT ! ^xF0>, prealign=0
						; 1.70 (4-7,0L) stall outside IQ

    .if eq force_path				; 1.44
	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p23)				; do the ret
    .iff
	ALIGN_FETCH_BLOCK <^x47FF041F>		; align
	PVC_VIOLATE <1007>
	PVC_VIOLATE <1020>			; stop permutation
	hw_jmp	(p23)				; return with jmp
	br	r31, .-4			; stop predictor
    .endc					; 1.44

	END_HW_VECTOR

;+
; DFAULT - offset 380
;
; Entry:
;	Vectored into via hardware trap on a Dstream fault or virtual
;	address sign check error.
;
; Function:
;	Prepare to take an Access Violation, FOR, or FOW.
;	For FOx, need to to a DTB_ISx.
;
; Note: VA and MM_STAT are not written on ld_vpte fault. Also, on
; dfault not in pal, we can only get one of FOR or FOW (only
; hw_ld/w can get both), so the order of checking does not matter.
;
; Register use:
;	p4	scratch
;	p5	mm_stat. For ld_vpte fault, should be unchanged from dtb miss
;	p6	exc_addr
;	p7	exc_sum
;	p20	available once we determine we weren't in pal mode
;
; Exit state:
;	On exit to trap__dc_tag_perr
;	p5	mm_stat
;	p6	pc of faulting instruction
;	p7	exc_sum
;
;	On exit to dfault_in_pal
;	p5	mm_stat
;	p6	pc of faulting instruction
;	p7	exc_sum
;
;	On exit to trap__post_km_r45
;	PT__FAULT_PC		pc of instruction causing fault
;	PT__FAULT_SCB		scb offset
;	PT__FAULT_R4		fault va
;	PT__FAULT_R5		memory management flags
;-
	START_HW_VECTOR <DFAULT>

	hw_mfpr	p6, EV6__EXC_ADDR		; (0L) save exception address
	hw_mfpr p7, EV6__EXC_SUM		; (0L) get exc_sum
	hw_mfpr	p5, EV6__MM_STAT		; (0L) get mm_stat

	srl	p5, #EV6__MM_STAT__DC_TAG_PERR__S, p4
	blbs	p4, trap__dc_tag_perr		; dc_tag_perr => another flow
	blbs	p6, trap__dfault_in_pal		; pal mode => another flow
;
; There must be a scoreboard bit -> register dependency chain to prevent 
; hw_mtpr dtb_isx from issuing while ANY of scoreboard bits <7:4> are set.
; We read EV6__VA anyway (which waits on 4-7), so we are all set.
;
	hw_mfpr	p20, EV6__VA			; (4-7,1L) get faulting va

	CONT_HW_VECTOR <DFAULT>

	blbc	p5, trap__dfault_check		; load => check x31

	srl	p5, #EV6__MM_STAT__OPCODE__S, p7	; get opcode
	and	p7, #EV6__MM_STAT__OPCODE__M, p7	; clean it
	cmpeq	p7, #^x18, p7				; ECB/WH64?
	bne	p7, trap__dfault_dismiss		; dismiss if so

	br	r31, trap__dfault_no_dismiss	; store => no dismiss

trap__dfault_check:
	srl	p7, #EV6__EXC_SUM__REG__S, p7	; get ra
	and	p7, #EV6__EXC_SUM__REG__M, p7	; clean ra
	cmpeq	p7, #^x1F, p7			; compare against x31
	bne	p7, trap__dfault_dismiss	; branch => x31
;
; Take the trap
;
trap__dfault_no_dismiss:			; test for ACV
	and	p5, #<1@EV6__MM_STAT__ACV__S>, p4
	lda	p7, SCB__ACV(r31)		; assume ACV
	bne	p4, trap__dfault_scb		; if ACV, go on to write scb

	and	p5, #<1@EV6__MM_STAT__FOR__S>, p4
	lda	p7, SCB__FOW(r31)		; now assume FOW
	cmovne	p4, #SCB__FOR, p7		; switch to FOR if indicated
;
; Zap TB entry for FOx.
; Note that the read of VA into p20 ensures that scoreboard
; bits 4-7 are clear, a requirement for writing to DTB_ISx.
;
dfault_tbis_offset = <trap__dfault_scb - trap__dfault_tbis>

	br	p4, trap__dfault_tbis		; zap TB entry for FOx
trap__dfault_tbis:
	addq	p4, #<dfault_tbis_offset+1>, p4	; set up to jump past in palmode
	EV6_MTPR p20, EV6__DTB_IS0, postalign=0	; 1.70 (6,0L)
	EV6_MTPR p20, EV6__DTB_IS1, prealign=0	; 1.70 (7,1L)
	bsr	r31, .				; push prediction stack
	PVC_JSR dfault_tbis			; synch up
	hw_ret_stall (p4)			; pop prediction stack
	PVC_JSR dfault_tbis, dest=1

trap__dfault_scb:
	hw_stq/p p7, PT__FAULT_SCB(p_temp)	; store scb offset
	hw_stq/p p20, PT__FAULT_R4(p_temp)	; store fault va
	sll	p5, #63, p5			; set up mmf
	hw_stq/p p5, PT__FAULT_R5(p_temp)	;
	hw_stq/p p6, PT__FAULT_PC(p_temp)	; store fault pc
	br	r31, trap__post_km_r45		; now post
;+
; Dismiss load x31
;
; We don't need a MB to hold up loads that may affect MM_STAT.
; To get here, we have taken conditional branches that on the
; mfpr MM_STAT being issued.

;-
trap__dfault_dismiss:
	addq	p6, #4, p6			; dismiss
	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p6)
;+
; trap__dfault_in_pal
;
; Entry:
; 	Dstream fault trap has been taken, exc_addr points to pal code.
;
; Function:
;	Vector to specific handler for current pal routine.
;	For FOx, do DTB_ISx.
;
; Note: Does it matter what order we check mm_stat? A hw_ld/w
; can set both FOR and FOW, and the SRM indicates in the va->pa
; algorithm that FOW should be checked first. So we will take FOW
; over FOR.
;
; Current state:
;	p5 	mm_stat
;	p6	pc of instruction causing Dstream fault. Can be from
;			call_pal PALcode
;			kernel stack processing (KSP not valid case)
;	p7	available
;	p20	reserved
;	p23	if vpte fetch dfault => exc_addr of instruction causing miss
;
;	VA	faulting VA
;
; Exit state:
;	p4		fake PTE with <17:16> = ^b01 to indicate dfault
;	p6		fault va
;	p23		fault pc
;	r25		available
;	r26		cleaned exc_addr
;
;	PT__R25		saved r25
;	PT__R26		saved r26
;
;	PT__VPTE_PC	original pc for vpte fault
;
;	PT__FAULT_SCB	scb offset
;-
trap__dfault_in_pal:
	hw_stq/p r25, PT__R25(p_temp)		; get scratch space
	hw_stq/p r26, PT__R26(p_temp)		; get scratch space

	hw_stq/p p23, PT__VPTE_PC(p_temp) 	; original pc for vpte fault

	bis	p6, r31, p23			; put fault pc in p23
	bic	p6, #3, r26			; clean pc for dispatching

.if ne pte_eco					; 1.56
  .if ne force_path2				; 1.56 test for ldbu
	hw_mfpr	r25, EV6__PAL_BASE		; (4,0L) get pal base
	subq	r25, r26, r26			; pal base - cleaned pc

	lda	p7, <trap__dtbm_single_hack1 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__stc_hack

	lda	p7, <trap__dtbm_single_hack2 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__stc_hack

	lda	p7, <trap__dtbm_single_hack3 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__stc_hack
  .endc						; 1.56 force_path2
.endc						; 1.56 pte_eco

	bic	p6, #3, r26			; 1.56 clean pc for dispatching

;
; There must be a scoreboard bit -> register dependency chain to prevent 
; hw_mtpr DTB_ISx from issuing while ANY of scoreboard bits <7:4> are set.
; We read EV6__VA anyway (which waits on 4-7), so we are all set.
;
	hw_mfpr	p6, EV6__VA			; (4-7,1L) get va

	and	p5, #<1@EV6__MM_STAT__ACV__S>, r25
	lda	p7, SCB__ACV(r31)		; assume ACV
	bne	r25, trap__dfault_in_pal_scb	; if ACV, go on to write scb

	and	p5, #<1@EV6__MM_STAT__FOW__S>, r25
	lda	p7, SCB__FOR(r31)		; now assume FOR
	cmovne	r25, #SCB__FOW, p7		; switch to FOW if indicated
;
; Clear TB for FOx.
; Note that the read of VA into p6 ensures that scoreboard
; bits 4-7 are clear, a requirement for writing to DTB_ISx.
;
pal_tbis_offset = <trap__dfault_in_pal_scb - trap__dfault_in_pal_tbis>

	br	r25, trap__dfault_in_pal_tbis
trap__dfault_in_pal_tbis:
	addq	r25, #<pal_tbis_offset+1>, r25	; set up to jump past in palmode
	EV6_MTPR p6, EV6__DTB_IS0, postalign=0	; 1.70 (6,0L) invalidate TB
	EV6_MTPR p6, EV6__DTB_IS1, prealign=0	; 1.70 (7,1L)       for FOx
	bsr	r31, .				; push prediction stack
	PVC_JSR dfault_in_pal_tbis		; synch up
	hw_ret_stall (r25)			; pop prediction stack
	PVC_JSR dfault_in_pal_tbis, dest=1

trap__dfault_in_pal_scb:
	hw_stq/p p7, PT__FAULT_SCB(p_temp)	; store scb offset
	lda	p4, <<1@PTE__SOFT__S>+32768>@-16(r31) ; PTE<17:16> =^b01
	br	r31, trap__pal_mm_dispatch	; do the dispatch

.if ne force_path2				; 1.56
;+
; trap__stc_hack
;
; Current state:
;	p6		pal pc
;	PT__VPTE_PC	original pc - needs to be restore to p23
;	r25		needs to be restored
;	r26		needs to be restored
;
; The real stx_c will fault as well. Recover back to
; dtbm miss flow after the load.
;-
trap__stc_hack:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26
	hw_ldq/p p23, PT__VPTE_PC(p_temp) 	; original pc for vpte fault

	addq	p6, #4, p6			; return past load
	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p6)				; return
.endc						; 1.56


;+
; dc_tag_perr
;
; Dcache tag parity error occured during the initial tag probe of a load
; or store instruction. This error created a synchronous fault to the dfault
; PALcode entry point, and is correctable. The virtual address associated
; with the error is available in the VA register. The cbox triplicate tag
; provides the correct victim address.
;
; However, the address in the VA may or may not translate to the actual
; pa for that dcache location, and miss and ACV exceptions are lower
; priority. So we have to clear the cache with a constructed pa that is
; sure to cause the evict.
;
; We need to turn off dctag_par_en before we do the hw_ld, in order
; to avoid an exception caused by the tag lookup.
;
; If we are in PALmode, we do an error halt. Shadow registers have already
; been clobbered.
;
; Current state:
;	p5		mm_stat
;	p6		exc_addr
;	p7		exc_sum
;-
NO_DCTAG_PAR_EN = -
		<<dcache_set_en@EV6__DC_CTL__SET_EN__S> ! -
		 <0@EV6__DC_CTL__DCTAG_PAR_EN__S> ! -
		 <0@EV6__DC_CTL__DCDAT_ERR_EN__S>>

	ALIGN_FETCH_BLOCK

trap__dc_tag_perr:
	hw_mfpr	p4, EV6__VA			; (4-7,1L) get va
	bis	p6, r31, p23			; move exc_addr to p23
	bis	r31, #NO_DCTAG_PAR_EN, p20	; disable dctag_par_en
	EV6_MTPR p20, EV6__DC_CTL		; 1.70 (6,0L)

	EV6_MTPR p20, EV6__DC_CTL, prealign=0	; 1.70 (6,0L) force retire
	zap	p4, #^xF8, p4			; use va<23:0> as a pa

	mb

	ldah	p6, 1(r31)			; use <16> as a toggle bit
	xor	p4, p6, p6			; xor to get toggle
	hw_ldq/p p7, (p6)			; force evict from one set

	ldah	p6, 2(r31)			; use <17> as a toggle bit
	xor	p4, p6, p6			; xor to get toggle
	hw_ldq/p p7, (p6)			; force evict from other

	mb					; force completion

	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_ldq/p p20, PT__IMPURE(p_temp)	; impure base
	hw_ldq/p p20, CNS__DC_CTL(p20)		; get old DC_CTL value
	EV6_MTPR p20, EV6__DC_CTL		; 1.70 (6,0L) restore DC_CTL
	bis	r31, r31, r31

	hw_mtpr	p20, EV6__DC_CTL		; (6,0L) force retire
	br	r31, sys__mchk_dc_tag_perr	; post retryable mchk

	END_HW_VECTOR

;+
; OPCDEC - offset 400
;
; Entry:
;	Vectored into via hardware trap on an illegal opcode or function field
;	fault.
;
; Function:
;	Prepare to take an opcdec exception via trap__post_km.
;	
; Register use:
;	p20		scratch
;	p23		exc_addr
;
; Exit state:
;	On exit to trap__post_km
;	PT__FAULT_PC		pc of instruction
;	PT__FAULT_SCB		scb offset
;-
	START_HW_VECTOR <OPCDEC>

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) save exc_addr
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

.if ne ev6_p1
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	br	r31, trap__emul_merge		; let floating emulator handle
trap__opcdec_merge:				; return for real opcdec
.endc

	lda	p20, SCB__OPCDEC(r31)		; scb offset
	hw_stq/p p20, PT__FAULT_SCB(p_temp)	; store scb
	addq	p23, #4, p20			; pc <- pc+4 for opcdec

	blbs	p23, trap__pal_exc_bugcheck	; mchk if opcdec within pal

	hw_stq/p p20, PT__FAULT_PC(p_temp)	; store pc
	br	r31, trap__post_km		; post the opcdec exception

	END_HW_VECTOR

;+
; IACV - offset 480
;
; Entry:
;	Vectored into via hardware trap on a Istream access violation or
;	virtual address sign check/overflow/underflow error.
;
; Function:
;	For native mode, take an exception with the exception pc as follows:
;	IF (bad_iva, i.e, jsr)
;	THEN exception pc = va<63:0>
;	ELSE	IF (pc_ovfl & va_48)
;		THEN	IF (exc_addr<47>)
;			THEN exception pc = exc_addr<47:0> with <63:48>=0
;			ELSE exception pc = exc_addr<47:0> with <63:48>=1
;		ELSE	exception pc = exc_addr<63:0>
;
;	For PALmode, enter console with exception pc as follows:
;	Has to be bad_iva, i.e. jsr with exception pc= va<63:1>!1
;
; Register use:
;	p4	temporary
;	p5	temporary
;	p6	exc_addr
;	p7	exc_sum
;
; Exit state:
;	On exit to trap__post_km_r45
;	PT__FAULT_PC		pc of instruction causing fault
;	PT__FAULT_SCB		scb offset
;	PT__FAULT_R4		fault va
;	PT__FAULT_R5		memory management flags
;-
	START_HW_VECTOR <IACV>

	hw_mfpr	p6, EV6__EXC_ADDR		; (0L) save exception address
	hw_mfpr	p7, EV6__EXC_SUM		; (0L) check bad_iva and pc_ovfl
	bis	r31, #1, p4			; mmf flag = ifetch
	lda	p5, SCB__ACV(r31)		; scb offset = access violation
	hw_stq/p p4, PT__FAULT_R5(p_temp)	; store mmf flags
	hw_stq/p p5, PT__FAULT_SCB(p_temp)	; store scb offset

	blbs	p6, trap__iacv_pal		; set=>bad iva in pal mode!
	srl	p7, #EV6__EXC_SUM__BAD_IVA__S, p4
	blbs	p4, trap__iacv_bad_iva		; set=>bad_iva
	srl	p7, #EV6__EXC_SUM__PC_OVFL__S, p4
	blbs	p4, trap__iacv_pc_ovfl		; set=>pc_ovfl
	
trap__iacv_post:				; fall through for normal acv
	hw_stq/p p6, PT__FAULT_R4(p_temp)	; store fault va
	hw_stq/p p6, PT__FAULT_PC(p_temp)	; store fault pc
	br	r31, trap__post_km_r45		; post the exception
;
; We had a pc overflow/underflow. If 43 bit mode, we can use exc_addr. If
; 48 bit mode, we need to look at bit 47 and do some work to produce the
; correct exception pc.
;
trap__iacv_pc_ovfl:
	hw_mfpr	p4, EV6__I_CTL			; (4,0L) look at va_48
	srl	p4, #EV6__I_CTL__VA_48__S, p4
	blbs	p4, trap__iacv_va_48		; set=>pc_ovfl & va_48
	br	r31, trap__iacv_post		; post as is from exc_addr

trap__iacv_va_48:				; pc_ovfl & va_48
	srl	p6, #47, p4			; check exc_addr<47>
	blbc	p4, trap__iacv_under		; branch for underflow
	zap	p6, #^xC0, p6			; overflow=> <63:48>=0
	br	r31, trap__iacv_post		; post

trap__iacv_under:				; underflow=> <63:48>=1
	subq	r31, #1, p4			; generate all 1s
	zapnot	p4, #^xC0, p4			; clean lower bytes
	or	p6, p4, p6			; or 1s into <63:48>
	br	r31, trap__iacv_post		; post
;
; JSR memory format instruction generated a bad va.
; Restrictions list says <63:1> are valid. Should we zap <0>?? Yes, for now.
;	
trap__iacv_bad_iva:
	hw_mfpr p7, EV6__VA			; (4-7,1L) get address from va
	bic	p7, #1, p6			; zap bit <0> and store in p6
	br	r31, trap__iacv_post		; post

	CONT_HW_VECTOR				; continue in free space
;
; JSR memory format instruction generated a bad va while in pal mode.
; We don't even bother checking the bad_iva bit, because that's the only
; way we can get here in pal mode.
;
; Set PT__HALT_CODE to HALT__SW_HALT. (Is there a better one ??)
; Branch to update PCB and enter console.
;
; For now, we stick a MB in to ensure that the mfpr VA is issued, though
; it's unlikely we will have any virtual loads after this.
;

trap__iacv_pal:
	hw_mfpr	p7, EV6__VA			; (4-7,1L) get address from va
	bis	p7, #1, p7			; or in PALmode bit
	lda	p20, HALT__SW_HALT(r31)		; is there a better code ??
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; store code (??)
	bis	p7, r31, p23			; fault pc to p23

	mb					; make sure we issue mfpr VA

	br	r31, trap__update_pcb_and_halt

	END_HW_VECTOR


;+
; MCHK - offset 500
;
; Entry:
;	Vectored into via hardware trap on a machine check.
;-
	START_HW_VECTOR <MCHK>

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) exception addr
	bis	r31, r31, r31			; 1.44 cut down on double mchks
	bis	r31, r31, r31
	bis	r31, r31, r31

	beq	r31, trap__mchk_mb
trap__mchk_mb:
	mb
	mb					; 1.44 end of new part
	br	r31, sys__mchk			; finish in system part
;
; Bugcheck from pal mode.
; Current state:
;	p23		exc_addr
;
trap__pal_os_bugcheck:
	lda	p7, MCHK__OS_BUGCHECK(r31)	; mchk code
	br	r31, trap__pal_bugcheck

trap__pal_exc_bugcheck:
	lda	p7, MCHK__BUGCHECK(r31)		; mchk code

trap__pal_bugcheck:
	sll	p7, #P_MISC__MCHK_CODE__S, p7	; move mchk code into position
	extbl	p_misc, #2, p5			; get mces
	zap	p_misc, #^x78, p_misc		; clear mchk_code & SCBv

	bis	p5, #<1@MCES__MCHK__S>, p6	; set MCES<MCHK>
	sll	p6, #P_MISC__MCES__MCHK__S, p6	; shift into position

	lda	p4, SCB__PROCMCHK(r31)		; SCB vector
	sll	p4, #P_MISC__SCBV__S, p4	; move SCBv into position

	bis	p6, p4, p6			; or mces and scbv
	bis	p6, p7, p_misc			; or mchk code

	blbs	p5, sys__double_machine_check	; halt on double
;
; Now compute where the frame is.
;
	CONT_HW_VECTOR <MCHK>				; 1.44 move to here

	hw_ldq/p p4, PT__WHAMI(p_temp)			; get whami

	lda	p5, PAL__LOGOUT_SPECIFIC_SIZE(r31)	; short&long size
	mulq	p4, p5, p5				; * whami

.if ne nonzero_console_base
	get_base_vms p6
	GET_32CONS	p6, PAL__LOGOUT_BASE, p6	; logout base
.iff
	GET_32CONS	p6, PAL__LOGOUT_BASE, r31	; logout base
.endc
	addq	p5, p6, p5				; (size*whami) + base
	lda	p5, MCHK__BASE(p5)			; start of mchk area

	hw_stq/p p23, MCHK__EXC_ADDR(p5)		; store exc_addr
	hw_stq/p p23, PT__FAULT_PC(p_temp)		; save fault pc

	hw_mfpr	p4, EV6__ISUM				; 1.44 get isum here
	hw_stq/p p4, MCHK__ISUM(p5)			; 1.44 save isum
;
; To be neat, write 0 to the cbox logout quadwords and mm_stat logout
; quadword. Log ic_stat and dc_stat but don't clear them.
;
	hw_stq/p r31, MCHK__DC1_SYNDROME(p5)		; 1.58 store 0
	hw_stq/p r31, MCHK__DC0_SYNDROME(p5)		; 1.58 store 0
	hw_stq/p r31, MCHK__C_STAT(p5)			; 1.58 store 0
	hw_stq/p r31, MCHK__C_STS(p5)			; 1.58 store 0
	hw_stq/p r31, MCHK__C_ADDR(p5)			; 1.58 store 0

	hw_stq/p r31, MCHK__MM_STAT(p5)			; 1.58 store 0

	hw_mfpr p4, EV6__I_STAT				; (4,0L) get i_stat
	hw_ldq/p p6, PT__I_STAT_MASK(p_temp)		; 1.58 get mask
	and	p4, p6, p4				; 1.58 clean

	hw_mfpr	p6, EV6__DC_STAT			; (6,0L) get dc_stat
	hw_stq/p p4, MCHK__I_STAT(p5)			; 1.58 store i_stat
	hw_stq/p p6, MCHK__DC_STAT(p5)			; 1.58 store dc_stat

	bis	r31, r31, p20				; no retry
	br	r31, sys__mchk_header			; merge

	END_HW_VECTOR

;+
; ITB_MISS - offset 580
;
; Entry:
;	Vectored into via hardware trap on Istream translation buffer miss.
;
; Function:
;	Do a virtual fetch of the PTE, and fill the ITB if the PTE is both
;	valid and is not FOE. If invalid or FOE, exit to trap__invalid_ipte
;	or trap__foe. The virtual fetch can trap into double.
;
; Note:
;	The ITB_PTE register has the PFN starting at bit 13, so the
;	fetched pte must be manipulated before being written.
;	Also, we want to keep the main flow in a single cache line
;	(4 fetch blocks) to be as efficient as possible.
;
; NOTE: 
;	1-to-1 mapping scheme implemented with bit in p_misc shadow
; 	register. And we can probably stick it in bit 63 and just branch
;	on negative. Right now we shift and test. Critical path has the
;	cycle count.
;
; Register use:
;	p4	shadow register reserved for itb/dtb miss
;	p5	shadow register reserved for double miss
;	p6	shadow register reserved for itb/dtb miss
;	p7	shadow register reserved for dtb miss
;	p_misc	physical mode bit
;	p23	exc_addr of instruction causing ITB miss
;
; Exit state:
;	On exit to trap__invalid_ipte or trap__foe
;	p4	PTE
;	p23	pc of instruction causing ITB miss
;-
	START_HW_VECTOR <ITB_MISS>

ASSUME P_MISC__PHYS__S eq 63

	hw_mfpr	p4, EV6__IVA_FORM		; (0L,1) get vpte address
	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) get exception address
	bis	p23, r31, p6			; 1.75 p6<-va for double
	bis	r31, r31, r31			; (xU) fill out fetch block

trap__itb_miss_vpte:
	hw_ldq/v p4, (p4)			; (xL,4) get vpte
	lda	p6, ^x0FFF(r31)			; 1.75 create mask for prot
	and	p4, p6, p5			; (xL,7) get prot bits
	blt	p_misc, trap__i1to1		; (xU) 1-to-1 => branch

	srl	p4, #PTE__PFN__S, p6		; (xU,7) shift PFN to <0>
	sll	p6, #EV6__ITB_PTE__PFN__S, p6	; (xU,8) shift PFN into place
	and	p4, #<1@PTE__FOE__S>, p7	; (xL) get FOE bit
	blbc	p4, trap__invalid_ipte		; (xU) invalid => branch

	bne	p7, trap__foe			; (xU) FOE => branch

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
.endc						; 1.70
	srl	p4, #7, p7			; check for mb bit
	bis	p5, p6, p6			; (xL,9) PTE in ITB format

.if eq force_path2				; 1.47 force_path2 = 0

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p23, EV6__ITB_TAG		; (6,0L) write tag
	hw_mtpr	p6, <EV6__ITB_PTE ! ^x40>	; (0&4&6,0L) write PTE

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70, 1.73

ASSUME <tb_mb_en + pte_eco> ne 2

  .if ne pte_eco
	blbc    p7, trap__itb_miss_mb		; branch for mb
	PVC_VIOLATE <44>			; 1.73
	hw_ret_stall (p23)			; return

trap__itb_miss_mb:
	mb
	PVC_VIOLATE <44>			; 1.73
	hw_ret_stall (p23)			; return
  .iff
	PVC_VIOLATE <44>			; 1.73
	hw_ret_stall (p23)			; return
  .endc						; pte_eco

.iff						; 1.47 force_path2 = 1
;
; We need to avoid the situation where a ldx_l has acquired a lock,
; another processor has taken it, and a bad path before a stx_c has
; a load which pulls the data back in and makes it look like the
; lock has succeeded.
;
; Hold up pte write, which holds up loads. Since hw_ret_stall is in
; same fetch block (it would normally scoreboard stall off bit 0),
; it fires off right away and will mispredict before any loads
; can happen.
;
	mb					; allow hw_ret to fire
	mulq	p6, #1, p6			; hold up loads

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	mulq	p6, #1, p6			; hold up loads
	hw_mtpr	p23, EV6__ITB_TAG		; (6,0L) write tag
	hw_mtpr	p6, <EV6__ITB_PTE ! ^x40>	; (0&4&6,0L) write PTE
	PVC_VIOLATE <43>			; 1.71
	hw_ret_stall (p23)			; mis-predict before any loads

.endc						; 1.47 force_path2

	CONT_HW_VECTOR				; 1.44 continue in free space

;+
; Do a 1-1 mapping
; Current state
;	p23	exc_addr
;-
trap__i1to1:
	srl	p23, #13, p6			; shift out the byte offset
.if ne	nonzero_console_base
	br	r31, trap__i1to1_system
trap__i1to1_return:
.endc

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
.endc
	lda	p5, ^x0F01(r31)			; all read enable
	sll	p6, #EV6__ITB_PTE__PFN__S, p6	; get pfn into position
	bis	p6, p5, p6			; produce the pte

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p23, EV6__ITB_TAG		; (6,0L) write tag
	hw_mtpr	p6, <EV6__ITB_PTE ! ^x40>	; (0&4&6,0L) write PTE

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70, 1.73

    .if eq force_path				; 1.44
	hw_ret_stall (p23)			; return
    .iff
	ALIGN_FETCH_BLOCK <^x47FF041F>
	PVC_VIOLATE <1007>
	PVC_VIOLATE <1020>			; stop permutation
	hw_jmp_stall (p23)			; return with jmp
	br	r31, .-4			; stop predictor
    .endc					; 1.44


;+
; trap__foe - Fault On Execute Istream PTE fetched by ITB miss routine.
;
; Entry:
;	On branch from trap__itb_miss flow.
;
; Function:
;	At this point, we know FOE is set, and TNV isn't.
;	But we haven't checked for ACV. Assume FOE and merge
;	with invalid Istream PTE code.
;
; Current state:
;	p4	PTE
;	p23	faulting va/pc
;
; Exit state:
;	On exit to trap__invalid_ipte_merge
;	p4	PTE
;	p20	scb vector for FOE
;	p23	pc/va of instruction causing ITB miss
;-
trap__foe:
	lda	p20, SCB__FOE(r31)		; assume FOE (^xC0)
	br	r31, trap__invalid_ipte_merge	; Merge with invalid flow


;+
; trap__invalid_ipte - Invalid Istream PTE fetched by ITB miss routine.
;
; Entry:
;	On branch from trap__itb_miss flow.
;
; Function:
;	At this point, we know we know we have TNV, but we haven't checked
;	for ACV, which takes precedence. We also have a merge from the
;	trap__foe flow, which was entered on FOE and no TNV.
;	Exit to trap__post_km_r45 to post the exception.
;
; Current state:
;	p4		PTE
;	p23		faulting va/pc
;
; When merging from trap_foe:
;	p20		scb vector for FOE
;
; Exit state:
;	p20		scb vector for ACV, TNV, or FOE
;
;	PT__FAULT_PC	fault pc
;	PT__FAULT_SCB	scb offset
;	PT__FAULT_R4	fault va
;	PT__FAULT_R5	mmf flags
;-
trap__invalid_ipte:
	lda	p20, SCB__TNV(r31)		; assume TNV (^x90)
trap__invalid_ipte_merge:			; merge from FOE flow
	srl	p_misc, #P_MISC__CM__S, p6	; get current mode to <1:0>

	hw_stq/p p23, PT__FAULT_R4(p_temp)	; save away fault va
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save away fault pc

	and	p6, #P_MISC__CM__M, p6	; clean current mode
	bis	r31, #1, p5			; get a 1
	addq	p6, #PTE__KRE__S, p6		; position of mode read enable
	hw_stq/p p5, PT__FAULT_R5(p_temp)	; store mmf with ifetch flag
	sll	p5, p6, p6			; shift 1 to mode read enable
	and	p4, p6, p6			; get mode read enable
	cmoveq	p6, #SCB__ACV, p20		; take ACV (^x80) over others
	hw_stq/p p20, PT__FAULT_SCB(p_temp)	;
	br	r31, trap__post_km_r45		; post the trap

	END_HW_VECTOR

;+
; ARITH - offset 600
;
; Entry:
;	Vectored into via hardware trap on an arithmetic exception
;	synchronous trap.
;
; Function:
;	Prepare to take a arithmetic exception via trap__post_km_r45.
;	For floating exceptions, we may just update the FPCR (see below)
;	and return to the user.
;
; Register use:
;	p4		scratch
;	p5		scratch
;	p6		scratch
;	p7		scratch
;	p20		scratch
;	p23		exc_addr
;
; Exit state:
;	On exit to trap__post_km_r45
;	PT__FAULT_PC		pc of instruction after that which
;				triggered the trap
;	PT__FAULT_SCB		scb offset
;	PT__FAULT_R5		exc_sum
;	PT__FAULT_R4		exc_mask
;
; Notes on FPCR register:
;	The FPCR is both a status and control register. The status reflect
;	problems encountered with the instruction. The 'disable' control bits
;	and instruction modifiers have an effect on trapping to the os.
;
;	If a trap disable bit is set and an instruction with the /S
;	qualifier set generates the associated trapping result, EV6
;	produces the IEEE non-trapping result and suppresses the trap.
;
; 	On EV6, PALcode assistance is required to assure correction operation
;	of the status bits. When the status *changes* from what is in the FPCR,
;	an arithmetic exception is taken, even if there is to be no trap. The
;	status changes are marked in EXC_SUM SET_condition bits. The PALcode
;	uses these bits to update the FPCR. The EXC_SUM condition bits are
;	examined to determine whether a trap should be delivered.
;
;		trap bit	set_x bit	actions
;		--------	---------	-------
;		0		1		no trap, set FPCR status bit
;		1		0		trap, don't set FPCR bit
;		1		1		trap, set FPCR status bit
;
; Notes on EXC_SUM register:
;		int - bit 7	iov - bit 6
;		-----------	-----------
;		0		1		fbox int overflow
;		1		1		ebox int overflow
;
;-

	START_HW_VECTOR <ARITH>

	hw_mfpr	p7, EV6__EXC_SUM			; (0L)
	hw_mfpr	p23, EV6__EXC_ADDR			; (0L) exception addr

.if ne ev6_p1
trap__arith_merge:					; merge for fp emulation
.endc

	lda	p6, SCB__ARITH(r31)			; scb offset
	and	p7, #<1@EV6__EXC_SUM__INT__S>, p4	; check for int ovr
	srl	p7, #EV6__EXC_SUM__REG__S, p5		; shift register field
	and	p5, #EV6__EXC_SUM__REG__M, p5		; isolate register

	blbs	p23, trap__pal_exc_bugcheck		; bugcheck on pal

	zap	p7, #^xC0, p7				; clean <63:48>
	addq	p23, #4, p23				; adjust to nextpc
	hw_stq/p p23, PT__FAULT_PC(p_temp)		; save next pc
	hw_stq/p p6, PT__FAULT_SCB(p_temp)		; save scb

	bne	p4, trap__arith_int			; branch on int case
	addq	p5, #32, p5				; set up for fx mask

trap__arith_int:
	srl	p7, #EV6__EXC_SUM__SET_INV__S, p4	; check for FPCR update
	and	p7, #^x7F, p7				; isolate <6:0>
	beq	p4, trap__arith_check_post		; branch for no update

	CONT_HW_VECTOR <ARITH>

;
; Update FPCR. Then check to see if we post.
;

.if eq ev6_p1

	ftoit	f0, p6				; save f0 (thanks to ftoit!)
	mf_fpcr	f0				; get current FPCR
	sll	p4, #EV6__FPCR__INV__S, p4	; shift status into place
	ftoit	f0, p20				; convert FPCR to integer
	bis	p20, p4, p20			; or in new status bits
	itoft	p20, f0				; turn back into float

arith_fpcr_offset = <trap__arith_finish - trap__arith_fpcr>

	br	p4, trap__arith_fpcr
trap__arith_fpcr:
	addq	p4, #<arith_fpcr_offset+1>, p4	; set up to jump past in palmode
	mt_fpcr	f0				; write FPCR
	bsr	r31, .				; push prediction stack
	PVC_JSR arith_fpcr			; synch up
	hw_ret_stall (p4)			; pop prediction stack
	PVC_JSR arith_fpcr, dest=1

trap__arith_finish:
	itoft	p6, f0				; restore f0

.endc

trap__arith_check_post:
	bic	p7, #1, p6			; check <6:1>
	bne	p6, trap__arith_post		; post if indicated

	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p23)				; back to user

trap__arith_post:
	lda	p20, 1(r31)			; get a 1 for mask
	sll	p20, p5, p20			; turn reg number into mask
	hw_stq/p p7, PT__FAULT_R5(p_temp)	; save exc_sum
	hw_stq/p p20, PT__FAULT_R4(p_temp)	; store mask
	br	r31, trap__post_km_r45		; post the exception

	END_HW_VECTOR


;+
; INTERRUPT - offset 680
;
; Entry:
;	Vectored into via hardware trap on interrupt.
;
; Function:
;
; Note: If a new interrupt (hardware, serial line, crd or perf counter)
;	occurs simultaneously with an mfpr isum, the isum read will return 0's.
;	The interrupt would not be lost however. After a passive release in
;	the PALcode, we would see the interrupt again. One way to
;	minimize the number of passive releases is to read isum twice
;	and OR the results.
;-

ipl_offset = <IPL_TABLE - trap__pal_base> 

ASSUME EV6__ISUM__ASTK__S eq 3
ASSUME EV6__ISUM__ASTE__S eq 4
ASSUME EV6__ISUM__ASTS__S eq 9
ASSUME EV6__ISUM__ASTU__S eq 10

	START_HW_VECTOR <INTERRUPT>

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) get exc_addr
	hw_mfpr	p4, EV6__ISUM			; (0L) get interrupt summary
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mfpr	p5, EV6__ISUM			; (0L) read again
	bis	p4, p5, p4			; OR the results

.if ne check_ebox_iprs				; 1.60
	rpcc	p6, (p4)			; 1.60 depend on p4
	addq	p4, p6, p7			; 1.60 depend on p6
	addq	p7, #1, p7			; 1.61 depend on p7
	addq	p7, #1, p7			; 1.61 depend on p7
	rpcc	p20, (p7)			; 1.60 depend on p7
  .if ne egore					; 1.60
	bis	r31, #1, p6			; 1.60 isp has no cc
	bis	r31, #2, p20			; 1.60 isp has no cc
  .endc						; 1.60
	xor	p6, p20, p6			; 1.60 see if cc incremented
	beq	p6, trap__interrupt_crash1	; 1.60 branch if not
	br	p6, trap__interrupt_okay1	; 1.60 otherwise okay

trap__interrupt_crash1:
	lda	p20, ^x0C(r31)			; 1.60 crash code
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; 1.60
	br	r31, trap__halt_after_fix	; 1.61

trap__interrupt_okay1:				; 1.60
	lda	p20, ^xAA(r31)			; 1.60 get a constant
	insbl	p20, #0, p20			; 1.60 swapped on big-end
	cmpeq	p20, #^xAA, p7			; 1.60 compare
	beq	p7, trap__interrupt_crash2	; 1.60
	br	p7, trap__interrupt_okay2	; 1.60

trap__interrupt_crash2:
	lda	p20, ^x0B(r31)			; 1.60 crash code
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; 1.60
	br	r31, trap__halt_after_fix	; 1.61

trap__interrupt_okay2:				; 1.60

.endc						; 1.60 check_ebox_iprs

	CONT_HW_VECTOR <INTERRUPT>		; 1.60

	srl	p4, #EV6__ISUM__CR__S, p5	; check for cre (31)
	srl	p4, #EV6__ISUM__PC__S, p6	; check for perf ctr (29)
	and	p6, #EV6__ISUM__PC__M, p6	; get just perf ctr bits

	blbs	p5, sys__crd			; branch if cre
	bne	p6, sys__interrupt_pc		; branch if perf ctr

.if ne spinlock_hack				; 1.41
						; 1.60 delete cont_hw_vector
	hw_ldq/p p7, PT__PCTR_PEND(p_temp)	; check pending
	beq	p7, call_pal__interrupt_spin0	; branch for none
	srl	p_misc, #P_MISC__IPL__S, p5	; get current ipl
	and	p5, #P_MISC__IPL__M, p5		; clean
	lda	p7, IPL__PERFMON(r31)		; ipl for perf counter
	subq	p7, p5, p5			; perfmon - current
	ble	p5, call_pal__interrupt_spin0	; branch if can't take now
;
; We can take the perfmon interrupt now because IPL is below the
; real PERFMON value.
;
	hw_ldq/p p6, PT__PCTR_R4(p_temp)	; get FAULT_R4 value
	hw_stq/p r31, PT__PCTR_PEND(p_temp)	; clear pending
	br	r31, sys__interrupt_pc_take_int	; now take interrupt

call_pal__interrupt_spin0:			; now return

.endc						; 1.41

	srl	p4, #EV6__ISUM__EI__S, p7	; check for external int (20-23)
	and	p7, #EV6__ISUM__EI__M, p7	; get just ei bits
	bne	p7, sys__interrupt_ei		; branch if so

	lda	p5, ^x7FFE(r31)			; check for sw (2-15)
	sll	p5, #EV6__ISUM__SI__S, p5
	and	p4, p5, p5
	bne	p5, trap__interrupt_sw		; branch if so

	srl	p4, #EV6__ISUM__ASTK__S, p6	; check for AST (2)
	and	p6, #^xC3, p6
	bne	p6, trap__interrupt_ast		; branch if so

	srl	p4, #EV6__ISUM__SI__S, p7	; check for sw (1)
	blbc	p7, trap__interrupt_sl		; check for sl
	br	r31, trap__interrupt_sw		; handle sw1

						; 1.60 delete cont_hw_vector

;+
; AST interrupt
;
; Current state:
;	p4	isum
;-
trap__interrupt_ast:
	srl	p4, #EV6__ISUM__ASTK__S, p5	; ASTK in low bit
	srl	p4, #EV6__ISUM__ASTE__S, p6	; ASTE in low bit
	bis	r31, r31, p7			; get a 0

	blbs	p5, trap__interrupt_ack_ast	; branch on ASTK
	srl	p4, #EV6__ISUM__ASTS__S, p5	; ASTS in low bit
	addq	p7, #1, p7			; bump counter

	blbs	p6, trap__interrupt_ack_ast	; branch on ASTE
	srl	p4, #EV6__ISUM__ASTU__S, p6	; ASTU in low bit
	addq	p7, #1, p7			; bump counter

	blbs	p5, trap__interrupt_ack_ast	; branch on ASTS
	addq	p7, #1, p7			; bump counter
	blbc	p6, trap__interrupt_dismiss	; branch on dismiss
;
; Acknowledge AST.
;
; In IER, turn off SIEN<2:1> and ASTEN.
;
; Current state:
;	p7	ast mode (0-3)
;

ASSUME EV6__IER__ASTEN__S eq 13
ASSUME EV6__IER__SIEN__S eq 14 

trap__interrupt_ack_ast:
	hw_mfpr	p4, EV6__IER			; (4,0L) get current IER
	bis	r31, #^x7, p5			; get ready to bic enables
	sll	p5, #EV6__IER__ASTEN__S, p5	; shift into position
	bic	p4, p5, p4			; clear SIEN<2:1>, ASTEN
	EV6_MTPR p4, EV6__IER			; 1.70 (4,0L) write IER

	sll	p7, #4, p4			; shift to create scb addend
	lda	p5, SCB__KAST(p4)		; get correct ast scb offset
	hw_stq/p p5, PT__FAULT_SCB(p_temp)	; store it for post
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store pc

	hw_mfpr	p4, EV6__ASTRR			; (4,0L) get ASTRR
	bis	r31, #1, p5			; get ready to ack the ast
	addq	p7, #EV6__ASTRR__ASTRR__S, p6	; ast level + beginning position
	sll	p5, p6, p5			; shift into position
	bic	p4, p5, p4			; clear the ast request
	EV6_MTPR p4, EV6__ASTRR			; 1.70 (4,0L) write ASTRR

	and	p_misc, #<1@P_MISC__IP__S>, p20	; get old ip (ev4/ev5 did)
	lda	p20, <2@PS__IPL__S>(p20)	; cm=0, ip=old, ipl=2, sw=0
	hw_stq/p p20, PT__NEW_PS(p_temp)	; save new ps
	br	r31, trap__post_km_ps		; post

;+
; Software interrupt.
;
; Need to find out which one.
;
; Current state:
;	p4	isum
;-
trap__interrupt_sw:
	bis	r31, #15, p7				; start with sw15
	sll	p4, #<63-<EV6__ISUM__SI__S+14>>, p4	; shift si up to <63:49>

trap__interrupt_sw_find:
	beq	p7, trap__interrupt_dismiss	; can't find it now
	blt	p4, trap__interrupt_sw_found	; got it
	subq	p7, #1, p7			; decrement sw int number
	sll	p4, #1, p4			; get ready to test next
	br	r31, trap__interrupt_sw_find
;
; Ack the software interrupt.
;
; Current state:
;
;	p7	sw int number (=ipl)
;
trap__interrupt_sw_found:
	sll	p7, #4, p4			; shift to create scb addend
	lda	p5, SCB__SW0(p4)		; get correct sw scb offset
	hw_stq/p p5, PT__FAULT_SCB(p_temp)	; store it for post
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store pc

	hw_mfpr	p4, EV6__SIRR			; (4,0L) get current SIRR
	bis	r31, #1, p5			; get ready to ack the sw int
	addq	p7, #<EV6__SIRR__SIR__S-1>, p6	; int + beginning position
	sll	p5, p6, p5			; shift into position
	bic	p4, p5, p4			; clear the request
	EV6_MTPR p4, EV6__SIRR			; 1.70 (4,0L) ack the sw int

	bis	r31, r31, r31			; 1.41
	bis	r31, r31, r31			; 1.41
	bis	r31, r31, r31			; 1.41

	hw_mfpr	p4, EV6__PAL_BASE		; (4,0L) get pal base
	s8addq	p7, p4, p4			; pal base + index
	lda	p4, ipl_offset(p4)		; pal base + table base + index
	hw_ldq/p p4, (p4)			; get new ier
	EV6_MTPR p4, EV6__IER			; 1.70 (4,0L) write new ier

	sll	p7, #PS__IPL__S, p20		; move new ipl into position
	bis	p20, #<1@PS__IP__S>, p20	; or in IP bit
	hw_stq/p p20, PT__NEW_PS(p_temp)	; save new ps
	br	r31, trap__post_km_ps		; post


;+
; Check for serial line interrupt.
;-
trap__interrupt_sl:
	srl	p4, #EV6__ISUM__SL__S, p7	; check for sl
	beq	p7, trap__interrupt_dismiss	; dismiss if not
	br	r31, sys__interrupt_sl		; handle sl

;+
; Interrupts not being handled yet. ???
;-
trap__interrupt_passive:

;+
; Dismiss interrupt.
;-
trap__interrupt_dismiss:

.if eq force_path2				; 1.47 force_path2 = 0

	PVC_VIOLATE <44>			; 1.73
	hw_ret_stall (p23)		; return to user

.iff						; 1.47 force_path2 = 1
;
; We need to avoid the situation where a ldx_l has acquired a lock,
; another processor has taken it, and a bad path before a stx_c has
; a load which pulls the data back in and makes it look like the
; lock has succeeded. Hold up loads by writing to MM_STAT with
; scoreboard bit 2 (and 6 -- 6 is required or we hang).
;
	mb					; make sure hw_ret goes

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
	bis r0, r0, r0				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
.endc						; 1.70

	ALIGN_FETCH_BLOCK <^x47FF041F>
	mulq	p6, #1, p6			; hold up loads
	mulq	p6, #1, p6			; hols up loads
	hw_mtpr p6, <EV6__MM_STAT ! ^x44>	; hold up loads
	PVC_VIOLATE <43>			; 1.71
	hw_ret_stall (p23)			; return

.endc						; 1.47 force_path2


;+
; trap__post_km_ps
;
; Post interrupt with IPL changes already made.
;
; ?? Do we want to make common the code to update IER ??
;
; Build a stack frame on the kernel mode stack called from any mode.
; A new PS is being provided, with IPL changes already made.
; Note this routine is the same as the standard post except that
; the new PS is being passed in.
;
; Current state:
;	PT__NEW_PS	new ps with ip bit (old ip bit for AST case)
;	PT__FAULT_PC	pc
;	PT__FAULT_SCB	scb offset
;	PT__FAULT_R4	new r4
;	PT__FAULT_R5	new r5
;
;	EV6__IER	updated
;-
ps_cm_offset = <trap__post_km_ps_sp - trap__post_km_ps_cm>

trap__post_km_ps:
	EV6_MTPR r31, <EV6__MM_STAT ! ^xF0>	; 1.70, 1.39 settle the tb

	and	p_misc, #<3@P_MISC__CM__S>, p4	; get current mode <4:3>
	hw_ldq/p p20, PT__PCBB(p_temp)		; get pcbb
	beq	p4, trap__post_km_ps_cont	; skip switch if already kernel

	br	p6, trap__post_km_ps_cm		; change mode to kernel
trap__post_km_ps_cm:
	addq	p6, #<ps_cm_offset+1>, p6	; set up to jump past in palmode
	EV6_MTPR r31, EV6__PS			; 1.70 (4,0L) switch to kernel
	bsr	r31, .				; push prediction stack
	PVC_JSR post_km_ps			; synch up
	hw_ret_stall (p6)			; pop prediction stack
	PVC_JSR post_km_ps, dest=1

trap__post_km_ps_sp:
	addq	p20, p4, p20			; point to current mode SP
	hw_stq/p r30, PCB__STACKS(p20)		; save old SP
	hw_ldq/p r30, PT__KSP(p_temp)		; get new SP
;
; Write to stack. First store can miss/fault. Others are in the same
; page, so we're all set after that.
;
trap__post_km_ps_cont:
	and	r30, #63, p20			; isolate SP unaligned bits
	bic	r30, #63, r30			; round down stack
trap__post_km_ps_store:				; first store can miss/fault
	stq	r2, <<FRM__R2-64>&^xFFFF>(r30)	; write R2 to the stack
	stq	r3, <<FRM__R3-64>&^xFFFF>(r30)	; write R3 to the stack
;
; Hack alert. We don't have visibility to r4-r7 because they are in our
; shadow range. So we need to peek under the covers.
;
; Current state:
;	r2	available
;	r3	available
;
; Get some scratch space. Then save p_temp in r3, so we can get to
; the pal temps when we turn off shadow mode. Turn off shadow mode. Do the
; stores. Then turn shadow mode back on and restore the scratch space.
;-
	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_stq/p r1, PT__R1(p_temp)		; save r1
	bis	p_temp, r31, r3			; save p_temp
	hw_mfpr	r1, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r1, #<1@EV6__I_CTL__SDE7__S>, r1; zap sde

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
	bis	r0, r0, r0			; 1.70
.endc						; 1.70

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70, 1.73

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	stq	r4, <<FRM__R4-64>&^xFFFF>(r30)	; write R4 to the stack
	stq	r5, <<FRM__R5-64>&^xFFFF>(r30)	; write R5 to the stack
	stq	r6, <<FRM__R6-64>&^xFFFF>(r30)	; write R6 to the stack
	stq	r7, <<FRM__R7-64>&^xFFFF>(r30)	; write R7 to the stack

	hw_ldq/p r4, PT__FAULT_R4(r3)		; get r4
	hw_ldq/p r5, PT__FAULT_R5(r3)		; get r5
	bis	r1, #<1@EV6__I_CTL__SDE7__S>, r1; or in sde
	EV6_MTPR r1, EV6__I_CTL			; 1.70 (4,0L) write i_ctl

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_ldq/p r1, PT__R1(p_temp)		; restore r1
;
; Now back to business.
;
	hw_ldq/p p4, PT__SCBB(p_temp)		; get SCBB
	hw_ldq/p p5, PT__FAULT_SCB(p_temp)	; get SCB offset

	zap	p_misc, #^xFC, p7		; get old PS
	sll	p20, #PS__SP_ALIGN__S, p20	; get alignment bits in place
	bis	p20, p7, p20			; old PS with stack align value

	addq	p4, p5, p4			; get address of vector
	hw_ldq/p r2, 0(p4)			; get SCBV
	hw_ldq/p r3, 8(p4)			; get SCBP

	; form new PS (only difference from normal post routine)
	hw_ldq/p p4, PT__NEW_PS(p_temp)		; get new ps
	zap	p_misc, #3, p_misc		; zap old ps
	bis	p_misc, p4, p_misc		; or in new ps

	subq	r30, #64, r30			; decrement stack pointer
	hw_ldq/p p6, PT__FAULT_PC(p_temp)	; get fault pc
	bic	r2, #3, r2			; clean new pc
	stq	p20, FRM__PS(r30)		; write old PS to the stack
	stq	p6, FRM__PC(r30)		; write fault PC to the stack

.if ne tl6_ibox_timeout
	srl	p_misc, #61, p20		; check fault reset flag
	blbc	p20, 100$			; post if clear
	lda	p20, 1(r31)			; get a 1
	sll	p20, #61, p20			; shift to flag position
	bic	p_misc, p20, p_misc		; clear fault reset flag
	or	r2, r31, p23			; save os dispatch address
	lda	p20, halt__fault_reset(r31)	; go off to console
	br	r31, sys__enter_console
100$:
.endc

	hw_ret_stall	(r2)			; stall for possible ibox
						; changes
	END_HW_VECTOR

;+
; MT_FPCR - offset 700
;
; Entry:
;	Vectored into via hardware trap on issue of a MT_FPCR instruction.
;	This is a sychronous trap.
;
; Function:
;	The MT_FPCR instruction is issued from the floating point queue.
;	This instruction is implemented as an explicit IPR write: the value
;	is written into the "first" latch, and when the instruction retires,
;	the value is written into the "second" latch. There is no IPR
;	scoreboarding in the floating point queue. PALcode assistance is
;	required to ensure that subsequent readers of the FPCR get the
;	update value.
;
;	The PALcode can simply return using a HW_RET_STALL. This sequence
;	ensures that the MT_FPCR instruction will be correctly ordered with
;	respect to subsequent readers of the FPCR.
;
; Note: In pass1, we will never execute this code.
;
; Register use:
;	p23		exc_addr
;
;-
	START_HW_VECTOR <MT_FPCR>

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) save exc_addr
	NOP					; keep 1st fetch block free
	NOP					; 	of pvc violations
	NOP

	addq	p23, #4, p23			; adjust pc
	hw_ret_stall (p23)			; return to user

	END_HW_VECTOR

;+
; Wakeup
;
; Entry:
;	Vectored into via hardware trap on wakeup from sleep mode.
;	It is assumed that the SROM code uses ^x780 in its own
;	code for reset, and jumps to location 0 in the PALcode.
;-
EV6__WAKEUP_ENTRY = ^x780

	START_HW_VECTOR <WAKEUP>

.if ne <srom\1> ! mchk_on_wakeup
;
; Initialize 80 retirator "done" status bits and mapper.
; They must be initialized in the manner and order below.
; NO SOURCES other than x31 may be used until
; "mapper source enables" are turned on with a MTPR ITB_IA.
;
; On pass1, we maps only integer registers. On later passes,
; we map integer and floating registers.
;

	INITIALIZE_RETIRATOR_AND_MAPPER

;
; Now enable and map the shadow registers. This might be in
; SROM. It might be in PALcode. But it must be in this exact order.
;

	MAP_SHADOW_REGISTERS

.if ne mchk_on_wakeup
	GET_32CONS r16, SYS__WRITE_MANY_L, r31
	GET_32CONS r17, SYS__WRITE_MANY_H, r31
	sll	r17, #32, r17
	or	r16, r17, r17
	PVC_JSR wakeup_write_many, bsr=1
	bsr	p23, pal__write_many
	or	r1, r31, p23			; move exc_addr
	or	r31, #1, r16			; set fault_reset flag
	hw_mfpr	r26, EV6__PAL_BASE		; (4,0L) get pal base
	br	r31, sys__reset
.iff
	or	r31, r31, r16			; clear fault_reset flag
	br	r31, trap__powerup
.endc

.endc ; srom

	PVC_VIOLATE <1003>

	END_HW_VECTOR <^xC00 - TRAP__WAKEUP>

;+
; Routines entered from one or more of the exception entry points
;-
	GOTO_FREE_CODE

;+
; trap__pal_mm_dispatch
;
; Entry:
;	Branched to from dfault in pal or tnv_in_pal.
;
; Function:
;	Vector to specific handler.
;
; Current state:
;	p4	pte with <17:16>
;		^b00 => double
;		^b11 => invalid dpte
;		^b01 => dfault 
;	p5	mm_stat
;	p6	va
;	p7	available
;	p23	fault pc (can be from PALcode)
;	r25	saved and available
;	r26	cleaned pc (from p6 for dfault, from p23 for tnv in pal)
;
;	PT__FAULT_PC	fault pc for faults, next pc for unalign
;	PT__FAULT_SCB	SCB offset
;
;	PT__CH_MODE	old mode for chmx
;	PT__CH_SP	old sp for chmx
;
;	PT__VPTE_PC	for xldvpte_dfault, pc of instruction causing tb miss
;
;	PT__DTB_ALT_MODE
;			alt mode used in probe instruction
;
;	PT__R25		saved r25
;	PT__R26		saved r26
;
; For now, do a MB do ensure that the mfpr's for mm_stat and va have
; been issued before any loads appearing after. Later, we can analyze
; each flow and remove the mb if it is not necessary.
;-

trap__pal_mm_dispatch:
	mb

	hw_mfpr	r25, EV6__PAL_BASE		; (4,0L) get pal base
	subq	r25, r26, r26			; pal base - offset

.if eq ev6_p1

	lda	p7, <trap__post_km_r45_store - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__ksp_invalid

	lda	p7, <trap__post_km_ps_store - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__ksp_invalid

	lda	p7, <call_pal__post_xm_store - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_chmx

	lda	p7, <call_pal__rei_ldq - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_rei_ldq

	lda	p7, <call_pal__rei_ldq_from_nonkern - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_rei_ldq_from_nonkern

	lda	p7, <call_pal__prober_ldl1 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_prober

	lda	p7, <call_pal__prober_ldl2 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_prober

	lda	p7, <call_pal__probew_ldl1 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_probew

	lda	p7, <call_pal__probew_ldl2 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_probew

	lda	p7, <trap__itb_miss_vpte - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__ldvpte_dfault

	lda	p7, <trap__dtbm_single_vpte - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__ldvpte_dfault

					; 1.44 test for cmov check
	lda	p7, <sys__mchk_istream_check_cmov - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__fault_in_pal_istream_mchk

.iff

	lda	p7, <trap__post_km_r45_store - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__ksp_invalid

	lda	p7, <trap__post_km_ps_store - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__ksp_invalid

	lda	p7, <call_pal__post_xm_store - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_chmx

	lda	p7, <call_pal__rei_ldq - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_rei_ldq

	lda	p7, <call_pal__rei_ldq_from_nonkern - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_rei_ldq_from_nonkern

	lda	p7, <call_pal__prober_ldl1 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_prober

	lda	p7, <call_pal__prober_ldl2 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_prober

	lda	p7, <call_pal__probew_ldl1 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_probew

	lda	p7, <call_pal__probew_ldl2 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__mm_in_probew

	lda	p7, <trap__itb_miss_vpte - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__ldvpte_dfault

	lda	p7, <trap__dtbm_single_vpte - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__ldvpte_dfault

.endc

	hw_ldq/p p7, PT__TRAP(p_temp)		; check for special handler
	beq	p7, trap__no_pal_handler

	zap	p7, #^xF0, p7			; clear out upper 32 bits
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26
	addq	p7, r25, p7			; compute address
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25

	bis	p7, #1, p7			; PALmode
	bsr	r31, .				; push prediction stack
	PVC_VIOLATE <29>
	PVC_JSR pal_mm_dispatch			; jump to special handler
	hw_ret	r31, (p7)			; pop prediction stack


;+
; trap__fault_in_pal_istream_mchk
;
; Current state:
;	p23		pal pc of exception in pal mode
;
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__FAULT_PC	exc_addr
;	PT__R1		saved mchk frame addr
;-
trap__fault_in_pal_istream_mchk:			; 1.44
	hw_ldq/p r25, PT__R25(p_temp)			; restore r25
	hw_ldq/p r26, PT__R26(p_temp)			; restore r26

	br	r31, sys__mchk_istream_cmov_fault	; error on cmov
;+
; trap__no_pal_handler
;
; Current state:
;	p23		pal pc of exception in pal mode
;
;	r25		needs to be restored
;	r26		needs to be restored
;
; Exit state:
;	p23		fault pc (pal pc of exception in pal mode)
;	p20		HALT__SW_HALT
;-
trap__no_pal_handler:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	lda	p20, HALT__SW_HALT(r31)		; halt code
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; store it (??)

	br	r31, trap__update_pcb_and_halt

;+
; trap__ksp_invalid
;
; Entry:
;	Branched to on KSP not valid.
;
; Function:
;	Restore r25 and r26.
;	Recover original pc.
;	Set PT__HALT_CODE to HALT__KSP_INVAL.
;	Clean out CM and SW in p_misc.
;	Branch to update PCB and enter console.
;
; Current state:
;	EV6__PS		kernel
;	old SP		saved in PCB
;	new SP		kernel mode SP rounded down
;
;	p_misc		needs to get cm and sw cleared
;
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__FAULT_SCB	scb offset for kernel stack problem
;	PT__FAULT_PC	pc of instruction causing original exception
;	PT__FAULT_R4	fault va
;	PT__FAULT_R5	memory management flags
;-

trap__ksp_invalid:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	hw_ldq/p p23, PT__FAULT_PC(p_temp)	; get back original pc

trap__ksp_invalid_chmx:
trap__ksp_invalid_rei:
	lda	p20, HALT__KSP_INVAL(r31)	; ksp invalid
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; store code (??)

	; clear CM/SW
	bic	p_misc, #<<3@P_MISC__CM__S>!<3@P_MISC__SW__S>>, p_misc

	br	r31, trap__update_pcb_and_halt

;+
; trap__mm_in_chmx
;
; Current state:
;	p5		mm_stat
;	p6		va
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__FAULT_SCB	scb offset
;	PT__FAULT_PC	nextpc -- needs to be adjusted
;	PT__CH_MODE	old mode -- needs to be inserted in p_misc
;	PT__CH_SP	old sp -- needs to be restored
;-
trap__mm_in_chmx:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	hw_ldq/p p23, PT__FAULT_PC(p_temp)	; get back original pc
	subq	p23, #4, p23			; adjust pc

	and	p_misc, #<3@P_MISC__CM__S>, p4	; get mode
	beq	p4, trap__ksp_invalid_chmx	; branch for ksp inval

	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store adjusted pc
	sll	p5, #63, p5			; mmf r/w bit
	hw_stq/p p6, PT__FAULT_R4(p_temp)	; store	fault va
	hw_stq/p p5, PT__FAULT_R5(p_temp)	; store mmf
;
; Clear new mode from p_misc, and insert old mode.
; Get old stack pointer back.
;
	hw_ldq/p p4, PT__CH_MODE(p_temp)	; get old mode back
	hw_ldq/p r30, PT__CH_SP(p_temp)		; get old stack pointer back
	bic	p_misc, #<3@P_MISC__CM__S>, p_misc
	bis	p_misc, p4, p_misc		; or in old mode
	br	r31, trap__post_km_r45		; post

;+
; trap__mm_in_probew
; trap__mm_in_prober
;
; Current state:
;	p4		pte with <17:16>
;			^b00 => double
;			^b11 => invalid dpte
;			^b01 => dfault 
;	p6		va
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__FAULT_SCB	scb offset
;	PT__CALL_PAL_PC	user pc
;
; We have arrived from one of the following three flows:
;	(1) Dtb miss with invalid level 3 pte. Either TNV or ACV.
;		But we need to recheck which using alt mode.
;	(2) Double miss with invalid level 1/2 pte. Either TNV or ACV.
;	(3) Dfault due to FOR, FOW, or ACV.
;
; We do the following checks and actions:
;
;	(1) Check for FOR, FOW. If so, dismiss error and continue the probe.
;	(2) Check for dtb miss flow. If so, recheck for TNV or ACV.
;		If TNV, dismiss the error and continue the probe.
;		If ACV, terminate probe, and return failure to the user.
;	(3) If neither of the above, we have a Level 1/2 TNV/ACV or level 3 ACV.
;		If TNV, take a trap.
;		If ACV, terminate the probe, and return failure to the user.
;
; Exit state:
;	On exit to trap__post_km_r45
;	r25			restored
;	r26			restored
;	PT__FAULT_PC		pc of probe instruction
;	PT__FAULT_SCB		scb offset
;	PT__FAULT_R4		fault va
;	PT__FAULT_R5		memory management flags
;-
trap__mm_in_probew:
	bis	r31, #1, p5			; mark write
	br	r31, trap__mm_in_probe

trap__mm_in_prober:
	bis	r31, r31, p5			; mark read

trap__mm_in_probe:
;
; Check for FOR and FOW. Dismiss if so.
;
	hw_ldq/p p7, PT__FAULT_SCB(p_temp)	; need to look at scb offset
	cmpeq	p7, #SCB__FOR, r25		; check for FOR
	bne	r25, trap__mm_in_probe_dismiss
	cmpeq	p7, #SCB__FOW, r25		; check for FOW
	bne	r25, trap__mm_in_probe_dismiss
;
; Check for dtb miss flow. If so, go off to recheck ACV vs TNV with alt mode.
;
	srl	p4, #<PTE__SOFT__S+1>, r25	; check for level 3 problem
	blbs	r25, trap__mm_in_probe_recheck	; if so, need to do a re-check
;
; Either came from double miss TNV/ACV on level 1/2 or dfault ACV on level 3.
; Check scb offset for TNV or ACV.
; 	If TNV, take a trap.
;	If ACV, terminate the probe and return failure to the user.
;
	cmpeq	p7, #SCB__ACV, r25		; check for ACV
	bne	r25, trap__mm_in_probe_fail
;
; Got a TNV on level 1/2. Take a trap.
;
	sll	p5, #63, p5			; set up mmf
	hw_ldq/p p7, PT__CALL_PAL_PC(p_temp)	; get user pc + 4

	hw_stq/p p6, PT__FAULT_R4(p_temp)	; store va
	hw_stq/p p5, PT__FAULT_R5(p_temp)	; store mmf

	subq	p7, #4, p7			; point to probe instruction
	hw_stq/p p7, PT__FAULT_PC(p_temp)	; store fault pc

	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26
	br	r31, trap__post_km_r45		;
;
; Came from dtb miss flow. Recheck for ACV vs TNV.
;	If TNV, dismiss the error and continue the probe.
;	If ACV, terminate probe, and return failure to the user.
;
trap__mm_in_probe_recheck:
	hw_ldq/p p7, PT__DTB_ALT_MODE(p_temp)		; get alt mode
	bis	r31, #1, r26				; get a 1
	s4addq	p5, r31, r25				; read = 0, write = 4
	addq 	r25, #PTE__KRE__S, r25			; r/w + KRE position
	srl	p7, #EV6__DTB_ALT_MODE__MODE__S, p7	; mode in <1:0>
	addq	p7, r25, p7				; r/w + KRE + mode
	sll	r26, p7, r26				; shift into place

	and	p4, r26, p4			; test access
	beq	p4, trap__mm_in_probe_fail	; no access, so return failure
;
; Dismiss and go on with PALcode probe flow.
;
trap__mm_in_probe_dismiss:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	addq	p23, #4, p23			; bump pc
	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p23)				; return to PALcode probe flow
;
; Probe failed. Return r0 = 0.
;
trap__mm_in_probe_fail:
	bis	r31, r31, r0			; flag failure
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get user pc + 4

	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	PVC_VIOLATE <44>			; 1.72
	hw_ret	(p23)				; return to user

;+
; trap__mm_in_rei...
;
; Entry:
;	Branched to from memory mananagment problem on the stack.
;
; Current state:
;	p5	mm_stat
;	p6	va
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__FAULT_SCB	scb offset
;	PT__FAULT_PC	nextpc -- needs to be adjusted
;
; If rei from kernel, take a kernel stack not valid
; If rei from non-kernel, post a trap.
;-

;
; From kernel mode. Kernel stack not valid.
;
trap__mm_in_rei_ldq:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	hw_ldq/p p23, PT__FAULT_PC(p_temp)	; get back original pc
	subq	p23, #4, p23			; adjust pc

	br	r31, trap__ksp_invalid_rei	; branch to ksp inval
;
; From non-kernel mode. Take a trap.
;
trap__mm_in_rei_ldq_from_nonkern:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	hw_ldq/p p23, PT__FAULT_PC(p_temp)	; get back original pc
	subq	p23, #4, p23			; adjust pc

	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store adjusted pc
	sll	p5, #63, p5			; mmf r/w bit
	hw_stq/p p6, PT__FAULT_R4(p_temp)	; store	fault va
	hw_stq/p p5, PT__FAULT_R5(p_temp)	; store mmf

	br	r31, trap__post_km_r45		; post

;+
; trap__mm_in_float_emul_instr
;
; Entry:
;	Branched to on a problem with fetching the instruction stream
;	during floating point emulation in pass1 chips.
;
;	Got here on PTE not valid. Just take a TNV.
;+
.if ne ev6_p1

trap__mm_in_float_emul_instr:

	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
;
; Restore GPRs.
;
	bis	p4, r31, r1			; impure base into r1
	PVC_JSR	emul_restore, bsr=1
	bsr	r3, trap__emul_restore		; restore all but r0-r3
;
; Now restore the rest of the GPRs.
;
	hw_ldq/p r0, CNS__R0_EMUL(p4)		; restore r0
	hw_ldq/p r1, CNS__R1_EMUL(p4)		; restore r1
	hw_ldq/p r2, CNS__R2_EMUL(p4)		; restore r2
	hw_ldq/p r3, CNS__R3_EMUL(p4)		; restore r3
;
; Now post a TNV.
;
	hw_ldq/p p23, CNS__FP_PC(p4)		; get back original pc
	lda	p20, SCB__TNV(r31)		; get TNV vector
	hw_stq/p p20, PT__FAULT_SCB(p_temp)	; save away vector
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save away fault pc
	hw_stq/p p23, PT__FAULT_R4(p_temp)	; save away fault va
	bis	r31, #1, p5			; get a 1
	hw_stq/p p5, PT__FAULT_R5(p_temp)	; store mmf with ifetch flag
	br	r31, trap__post_km_r45		; post the trap
.endc


;+
; trap__ldvpte_dfault
;
; Entry:
;	Branched to from dfault of ld_vpte during tb miss processing.
;	For itb miss, the originator is user code. For dtb miss, the
;	originator can be user code, PALcode (including stack processing).
;
;	This is a halt condition. This means that we had a valid level 2
;	PTE with KRE/KWE=0 in the TB, pulled in by the double miss
;	flow. The SRM states that protection is ignored on level 1
;	and level 2 valid PTE's, implying that valid level 1 and
;	level 2 PTE's must have KRE/WRE set so that TB accesses work.
;
; Function:
;	Restore r25 and r26.
;	Set PT__HALT_CODE to HALT__PTBR_INVAL.
;	Recover original pc.
;	Branch to update PCB and enter console.
;
; Current state:
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__VPTE_PC	pc of instruction causing tb miss
;-
trap__ldvpte_dfault:
	hw_ldq/p r25, PT__R25(p_temp)			; restore r25
	hw_ldq/p r26, PT__R26(p_temp)			; restore r26

	lda	p20, HALT__PTBR_INVAL(r31)		; ptbr invalid
	hw_stq/p p20, PT__HALT_CODE(p_temp)		; store code (??)

	hw_ldq/p p23, PT__VPTE_PC(p_temp)		; get back original pc
	br	r31, trap__update_pcb_and_halt


;+
; trap__post_km_r45
;
; Entry:
;	Enter to post a trap for memory management problem.
;
; Function:
;	Build a stack frame on kernel stack called from any mode.
;
; Input:
;	PT__FAULT_PC	fault pc
;	PT__FAULT_SCB	scb offset
;	PT__FAULT_R4	fault va
;	PT__FAULT_R5	mmf flags
;
; Note: Left out bic'ing fault_pc<1:0>. Don't think pc is ever pal mode.
;
; For now, do a MB do ensure that the mfpr's for mm_stat and va have
; been issued before any loads appearing after. Later, we can analyze
; each flow and remove the mb if it is not necessary.
;
; This is also the entry for trap__post_km. Since r4 and r5 are unpredictable
; on entry to the os, we can write them from PT__FAULT_R4 and PT__FAULT_R5.
;-
r45_cm_offset = <trap__post_km_r45_sp - trap__post_km_r45_cm>

trap__post_km_r45:
trap__post_km:					; 1.39 settle the tb
	mb					; protect mm_stat and va
	EV6_MTPR r31, <EV6__MM_STAT ! ^xF0>	; 1.70, 1.55 settle the tb

	and	p_misc, #<3@P_MISC__CM__S>, p4	; get current mode <4:3>
	hw_ldq/p p20, PT__PCBB(p_temp)		; get pcbb
	beq	p4, trap__post_km_r45_cont	; skip switch if already kernel

	br	p6, trap__post_km_r45_cm	; change mode to kernel
trap__post_km_r45_cm:
	addq	p6, #<r45_cm_offset+1>, p6	; set up to jump past in palmode
	EV6_MTPR r31, EV6__PS			; 1.70 (4,0L) switch to kernel
	bsr	r31, .				; push prediction stack
	PVC_JSR	post_km_r45			; synch up
	hw_ret_stall (p6)			; pop prediction stack
	PVC_JSR	post_km_r45, dest=1

trap__post_km_r45_sp:
	addq	p20, p4, p20			; point to current mode SP
	hw_stq/p r30, PCB__STACKS(p20)		; save old SP
	hw_ldq/p r30, PT__KSP(p_temp)		; get new SP
;
; Write to stack. First store can miss/fault. Others are in the same
; page, so we're all set after that.
;

trap__post_km_r45_cont:
	and	r30, #63, p20			; isolate SP unaligned bits
	bic	r30, #63, r30			; round down stack
trap__post_km_r45_store:			; first store can miss/fault
	stq	r2, <<FRM__R2-64>&^xFFFF>(r30)	; write R2 to the stack
	stq	r3, <<FRM__R3-64>&^xFFFF>(r30)	; write R3 to the stack
;
; Hack alert. We don't have visibility to r4-r7 because they are in our
; shadow range. So we need to peek under the covers.
;
; Current state:
;	r2	available
;	r3	available
;
; Get some scratch space. Then save p_temp in r3, so we can get to
; the pal temps when we turn off shadow mode. Turn off shadow mode. Do the
; stores. Then turn shadow mode back on and restore the scratch space.
;-
	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_stq/p r1, PT__R1(p_temp)		; save r1
	bis	p_temp, r31, r3			; save p_temp
	hw_mfpr	r1, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r1, #<1@EV6__I_CTL__SDE7__S>, r1; zap sde

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
	bis	r0, r0, r0			; 1.70
.endc						; 1.70

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70. 1.73

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	stq	r4, <<FRM__R4-64>&^xFFFF>(r30)	; write R4 to the stack
	stq	r5, <<FRM__R5-64>&^xFFFF>(r30)	; write R5 to the stack
	stq	r6, <<FRM__R6-64>&^xFFFF>(r30)	; write R6 to the stack
	stq	r7, <<FRM__R7-64>&^xFFFF>(r30)	; write R7 to the stack

	hw_ldq/p r4, PT__FAULT_R4(r3)		; get fault va
	hw_ldq/p r5, PT__FAULT_R5(r3)		; get mmf
	bis	r1, #<1@EV6__I_CTL__SDE7__S>, r1; or in sde
	EV6_MTPR r1, EV6__I_CTL			; 1.70 (4,0L) write i_ctl

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_ldq/p r1, PT__R1(p_temp)		; restore r1
;
; Now back to business.
;
	hw_ldq/p p4, PT__SCBB(p_temp)		; get SCBB
	hw_ldq/p p5, PT__FAULT_SCB(p_temp)	; get SCB offset

	zap	p_misc, #^xFC, p7		; get old PS
	sll	p20, #PS__SP_ALIGN__S, p20	; get alignment bits in place
	bis	p20, p7, p20			; old PS with stack align value

	addq	p4, p5, p4			; get address of vector
	hw_ldq/p r2, 0(p4)			; get SCBV
	hw_ldq/p r3, 8(p4)			; get SCBP

	; clear CM/SW
	bic	p_misc, #<<3@P_MISC__CM__S>!<3@P_MISC__SW__S>>, p_misc

	subq	r30, #64, r30			; decrement stack pointer
	hw_ldq/p p6, PT__FAULT_PC(p_temp)	; get fault pc
	bic	r2, #3, r2			; clean new pc
	stq	p20, FRM__PS(r30)		; write old PS to the stack
	stq	p6, FRM__PC(r30)		; write fault PC to the stack

	hw_ret_stall (r2)			; go to os (stall for pvc)

;+						; 1.61
; trap__halt_after_fix
;
; Entry:
;	On va_ctl or cc_ctl corruption
;
; Function:
;	Restore va_ctl and cc_ctl and then halt
;
; Current state:
;	p20	halt code
;	p23	offending pc
;-
	ALIGN_CACHE_BLOCK
trap__halt_after_fix:				; 1.61 block begin
	hw_ldq/p p4, PT__VPTB(p_temp)		; get vptb base
	EV6_MTPR p4, EV6__VA_CTL		; 1.71 write it, clearing <29:0>
	bis	r31, r31, r31
	bis	r31, r31, r31

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70, 1.73

	hw_mtpr p4, EV6__VA_CTL			; write it, clearing <29:0>
	bis	r31, #1, p5			; get a 1
	sll	p5, #32, p5			; get into position
	bis	r31, r31, r31

	ASSUME_FETCH_BLOCK
	hw_mtpr	p5, EV6__CC_CTL			; enable
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
	
	ASSUME_FETCH_BLOCK
	hw_mtpr	p5, EV6__CC_CTL			; enable
	br	r31, trap__update_pcb_and_halt	; now halt
						; 1.61 block end
;+
; trap__update_pcb_and_halt
;
; Entry:
;	Branched to on halt conditions.
;
; Function:
;	Update PCB and branch to sys__enter_console.
;
; Current state:
;	p20		halt code
;	p23		offending pc
;
;	PT__HALT_CODE	halt code
;-
	ALIGN_CACHE_BLOCK
trap__update_pcb_and_halt:
	hw_ldq/p p4, PT__PCBB(p_temp)		; get pcbb
	and	p_misc, #<3@P_MISC__CM__S>, p5	; current mode in p5<4:3>

	bne	p5, trap__update_pcb		; branch if not kernel
	hw_stq/p r30, PT__KSP(p_temp)		; save ksp if we are kernel

trap__update_pcb:
	addq	p4, p5, p5			; get addr of current mode sp
	hw_stq/p r30, PCB__STACKS(p5)		; update current mode sp

;
; Currently the process context ipr format is:
;	<12:9>	astrr
;	<8:5>	aster
;
	hw_mfpr p5, EV6__PROCESS_CONTEXT	; (4,0L) get astrr and aster
	srl	p5, #EV6__ASTER__ASTER__S, p5	; shift into PCB location
	and	p5, #^xFF, p5			; clean
	hw_stq/p p5, PCB__AST(p4)		; store into PCB

	rpcc	p5				; get cycle counter
	srl	p5, #32, p6			; shift offset in cc<63:32>
	addl	p5, p6, p6			; cc<31:0> + cc<63:32>
	hw_stl/p p6, PCB__CPC(p4)		; store cc

	br	r31, sys__enter_console

;+
; pal__save_state
;
; The shadow registers, except of course p_misc and p_temp, are pretty much
; scratch, so we don't worry about stepping on them.
;
; Current state:
;	p7		return address
;	p20		halt code
;	p23		exc_addr
;
;	PT__HALT_CODE	halt code (do we need it there ??)
;-

	ALIGN_FETCH_BLOCK

pal__save_state:
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	hw_stq/p r31, CNS__FLAG(p4)		; clear dump flag
	hw_stq/p p20, CNS__HALT(p4)		; halt code
	bis	r31, r31, r31
;
; 1.64 store process_context before enabling fpe to save
; floating point registers
;
	STORE_REG <PROCESS_CONTEXT>, srn=p5, irn=p4, ipr=1	; 1.64 (4,0L)
	bis	r31, r31, r31					; 1.64
	bis	r31, r31, r31					; 1.64

	STORE_REG 0, irn=p4			; save r0
	STORE_REG 1, irn=p4			; save r1
	STORE_REG 2, irn=p4			; save r2
	hw_mfpr	r0, EV6__I_CTL			; (4,0L) get i_ctl

	bis	p4, r31, r1			; base of impure area
	bic	r0, #<1@EV6__I_CTL__SDE7__S>, r0; zap sde
	STORE_REG 3, irn=p4			; save r3
	STORE_REG 8, irn=r1			; save gpr
	
	EV6_MTPR r0, EV6__I_CTL			; 1.70 (4,0L) write i_ctl
	STORE_REG 9, irn=r1			; save gpr
	STORE_REG 10, irn=r1			; save gpr
	STORE_REG 11, irn=r1			; save gpr

	EV6_MTPR r0, EV6__I_CTL			; 1.70 (4,0L) stall outside IQ
	STORE_REG 12, irn=r1			; save gpr
	STORE_REG 13, irn=r1			; save gpr
	STORE_REG 14, irn=r1			; save gpr

	STORE_REG 15, irn=r1			; buffer block 1 -- save gpr
	STORE_REG 16, irn=r1			; save gpr
	STORE_REG 17, irn=r1			; save gpr
	STORE_REG 18, irn=r1			; save gpr

	STORE_REG 19, irn=r1			; buffer block 2 --save gpr
	STORE_REG 24, irn=r1			; save gpr
	STORE_REG 25, irn=r1			; save gpr
	STORE_REG 26, irn=r1			; save gpr

	STORE_REG 27, irn=r1			; buffer block 3 --save gpr
	STORE_REG 28, irn=r1			; save gpr
	STORE_REG 29, irn=r1			; save gpr
	STORE_REG 30, irn=r1			; save gpr

	STORE_REG 4, irn=r1			; now store the un-shadowed gprs
	STORE_REG 5, irn=r1			; save gpr
	STORE_REG 6, irn=r1			; save gpr
	STORE_REG 7, irn=r1			; save gpr

	STORE_REG 20, irn=r1			; save gpr
	STORE_REG 21, irn=r1			; save gpr
	STORE_REG 22, irn=r1			; save gpr
	STORE_REG 23, irn=r1			; save gpr
;
; Now turn shadow registers back on.
;
.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
	bis	r0, r0, r0			; 1.70
.endc						; 1.70

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70. 1.73

	bis	r0, #<1@EV6__I_CTL__SDE7__S>, r0; or in sde
	hw_mtpr	r0, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r0, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now save pal shadows. Registers p_temp and p_misc are really the
; only important ones.
;
; Current state:
;	p4	base of impure area
;
pal__save_shadow:
	hw_stq/p p4, CNS__P4(p4)
	hw_stq/p p5, CNS__P5(p4)
	hw_stq/p p6, CNS__P6(p4)
	hw_stq/p p7, CNS__P7(p4)
	hw_stq/p p20, CNS__P20(p4)
	hw_stq/p p_temp, CNS__P_TEMP(p4)
	hw_stq/p p_misc, CNS__P_MISC(p4)
	hw_stq/p p23, CNS__P23(p4)

.if eq ev6_p1					; no float in pass1

;
; Now save the floating point registers and FPCR.
; First make sure FEN is on.
;
	GET_16CONS	r0, <1@EV6__PROCESS_CONTEXT__FPE__S>, r31
	EV6_MTPR r0, EV6__FPE			; 1.70 (4,0L) write new fpe

	ALIGN_FETCH_BLOCK <^x47FF041F>		; align with nops

	hw_mtpr	r0, EV6__FPE			; (4,0L) force retire
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	t = 0
	.repeat 32
	STORE_REG \t, srn=p5, irn=p4, fpu=1
	t=t+1
	.endr

	mf_fpcr	f0				; get current FPCR
	ftoit	f0, p5				; convert to integer
	hw_stq/p p5, CNS__FPCR(p4)		; save

.endc

;
; Now save the important PALtemps
;
	STORE_REG <IMPURE>, srn=p5, irn=p4, pal=1
	STORE_REG <WHAMI>, srn=p5, irn=p4, pal=1
	STORE_REG <SCC>, srn=p5, irn=p4, pal=1
	STORE_REG <PRBR>, srn=p5, irn=p4, pal=1
	STORE_REG <PTBR>, srn=p5, irn=p4, pal=1
.if ne separate_page_tables				; 1.62
	STORE_REG <VIRBND>, srn=p5, irn=p4, pal=1	; 1.62
	STORE_REG <SYSPTBR>, srn=p5, irn=p4, pal=1	; 1.62
.endc							; 1.62
	STORE_REG <TRAP>, srn=p5, irn=p4, pal=1
	STORE_REG <HALT_CODE>, srn=p5, irn=p4, pal=1	; Do I need this?
	STORE_REG <KSP>, srn=p5, irn=p4, pal=1
	STORE_REG <SCBB>, srn=p5, irn=p4, pal=1
	STORE_REG <PCBB>, srn=p5, irn=p4, pal=1
	STORE_REG <VPTB>, srn=p5, irn=p4, pal=1
	STORE_REG <M_CTL>, srn=p5, irn=p4, pal=1
;
; Now save the IPRs that are restorable with some informational sandwiched in.
;
	hw_ldq/p p5, PT__VA_CTL(p_temp)			; control part
	hw_ldq/p p6, PT__VPTB(p_temp)			; vtpb part
	bis	p5, p6, p5				; combine
	hw_stq/p p5, CNS__VA_CTL(p4)

	hw_stq/p p23, CNS__EXC_ADDR(p4)

	STORE_REG <IER_CM>, srn=p5, irn=p4, ipr=1		; (4,0L)
	STORE_REG <I_STAT>, srn=p5, irn=p4, ipr=1		; (0L) info only
	STORE_REG <SIRR>, srn=p5, irn=p4, ipr=1			; (4,0L)
	STORE_REG <MM_STAT>, srn=p5, irn=p4, ipr=1		; (0L) info only
	STORE_REG <PAL_BASE>, srn=p5, irn=p4, ipr=1		; (4,0L)
	STORE_REG <DTB_ALT_MODE>, srn=p5, irn=p4, pal=1		; (6,0L)
;	STORE_REG <I_CTL>, srn=p5, irn=p4, ipr=1		; (4,0L)
; isp bug workaround
;
	hw_mfpr	p5, EV6__I_CTL
	GET_32CONS p6, <^x804000>, r31
	bic	p5, p6, p5			; clear unpredicatables
	hw_stq/p p5, CNS__I_CTL(p4)
; End of hack

	NOP
	NOP
	NOP					; don't save PCTR_CTL
	NOP
	NOP
;
; 1.64 delete store_reg of process_context
;
; Now save the rest of the informational IPRs. For now, we don't bother
; with the CBOX chain. ??
;
	STORE_REG <DC_STAT>, srn=p5, irn=p4, ipr=1	; (6,0L)
	STORE_REG <VA>, srn=p5, irn=p4, ipr=1		; (4-7,1L)
	STORE_REG <ISUM>, srn=p5, irn=p4, ipr=1		; (0L)
	PVC_VIOLATE <12>
	STORE_REG <EXC_SUM>, srn=p5, irn=p4, ipr=1	; (0L)
;
; For system partners that want unix PALcode to jump directly to
; the VMS PALcode enter_console, restore the pal base.
;
.if ne nonzero_console_base
	get_base_vms r0
	GET_32ADDR r0, <PAL__PAL_BASE>, r0
.iff
	GET_32ADDR r0, <PAL__PAL_BASE>, r31
.endc
	EV6_MTPR r0, EV6__PAL_BASE		; 1.70 (4,0L) write pal base
;
; For some reason, previous implementations have computed the size of the
; impure area taken up by a mchkflag quadword, the pal_temps, shadows, and
; iprs. That size is written to the mchkflag quadword. I will do that
; for now in case the console wants that information. Everything is quad,
; so I'm not bothering with rounding.
;
	GET_16CONS	r0, <CNS__SIZE>, r31
	GET_16CONS	r1, <CNS__MCHKFLAG>, r31
	subq	r0, r1, r0			; size - mchkflag location
	hw_stq/p r0, CNS__MCHKFLAG(p4)		; save the computation
;
; Now write the dump flag. 
;
	bis	r31, #1, r0
	hw_stq/p r0, CNS__FLAG(p4)		; set dump area flag
;
; Now return to caller
;
	bis	p7, #1, p7			; return in pal mode
	PVC_JSR save_state, bsr=1, dest=1
	hw_ret_stall (p7)			; stall for pal base change

;+
; pal__restore_state
;
; The shadow registers, except of course p_misc and p_temp, are pretty much
; scratch, so we don't worry about stepping on them.
;
; Current state:
;	Shadow mode on
;
;	p7		return address
;
;	p_temp		valid
;	PT__IMPURE	valid
;	PT__WHAMI	valid
;
; The assumptions here are that shadow mode is on, that p_temp is valid,
; and that PT__IMPURE and PT__WHAMI have not been written over.
; If these assumptions are not valid (i.e, if we are switching between other
; than our VMS and UNIX PALcodes, the exit_console platform-dependent code
; will have to figure out how to set these up.
;
; Exit state:
;	p23		exc_addr
;-

pal__restore_state:
	hw_ldq/p	r1, PT__IMPURE(p_temp)	; get base of impure area

.if eq ev6_p1					; no float in pass1

;
; Restore floating point registers. First make sure FEN is on.
;
	GET_16CONS	r0, <1@EV6__PROCESS_CONTEXT__FPE__S>, r31
	EV6_MTPR r0, EV6__FPE			; 1.70 (4,0L) write new fpe

	ALIGN_FETCH_BLOCK <^x47FF041F>		; align with nops

	hw_mtpr	r0, EV6__FPE			; (4,0L) force retire
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r0, r0, r0			; for pvc #7
	bis	r0, r0, r0
	bis	r0, r0, r0

	hw_ldq/p r0, CNS__FPCR(r1)		; get FPCR
	itoft	r0, f0				; convert to float

restore_fpcr_offset = < pal__restore_fpcr_done - pal__restore_fpcr>

	br	r2, pal__restore_fpcr
pal__restore_fpcr:
	addq	r2, #<restore_fpcr_offset+1>, r2	; return in palmode
	mt_fpcr	f0					; restore FPCR
	bsr	r31, .				; push prediction stack
	PVC_JSR restore_fpcr
	hw_ret_stall (r2)			; pop prediction stack
	PVC_JSR restore_fpcr, dest=1
pal__restore_fpcr_done:

	t=0
	.repeat 31
	RESTORE_REG \t, srn=r0, irn=r1, fpu=1
	t=t+1
	.endr

.endc

;
; Now restore the important PALtemps.
; We assume the following did not change or were restored by exit_console
;	p_temp
;	PT__IMPURE
;	PT__WHAMI
;
	RESTORE_REG <SCC>, srn=r0, irn=r1, pal=1
	RESTORE_REG <PRBR>, srn=r0, irn=r1, pal=1
	RESTORE_REG <PTBR>, srn=r0, irn=r1, pal=1
.if ne separate_page_tables					; 1.62
	RESTORE_REG <VIRBND>, srn=r0, irn=r1, pal=1		; 1.62
	RESTORE_REG <SYSPTBR>, srn=r0, irn=r1, pal=1		; 1.62
.endc								; 1.62
	RESTORE_REG <KSP>, srn=r0, irn=r1, pal=1
	RESTORE_REG <SCBB>, srn=r0, irn=r1, pal=1
	RESTORE_REG <PCBB>, srn=r0, irn=r1, pal=1
	RESTORE_REG <VPTB>, srn=r0, irn=r1, pal=1
;
; 1.58 In spinlock hack, we need to look at SPE bits only.
;
.if ne spinlock_hack				; 1.42
	hw_ldq/p r0, CNS__M_CTL(r1)		; get m_ctl
	and r0, #<7@EV6__M_CTL__SPE__S>, r0	; 1.58 get spe bits
	hw_ldq/p r2, PT__PCTR_VMS(p_temp)	; get vms flag
	beq	r0, pal__restore_state_spin0	; branch if not going to unix
	beq	r2, pal__restore_state_spin0	; branch if not the first time

	hw_stq/p r31, PT__PCTR_VMS(p_temp)	; clear 'vms'
	hw_stq/p r31, PT__PCTR_SAVE(p_temp)	; clear pctr save location
	hw_stq/p r31, PT__PCTR_FLAG(p_temp)	; clear flag
	hw_stq/p r31, PT__PCTR_R4(p_temp)	; clear r4 location
	hw_stq/p r31, PT__PCTR_PEND(p_temp)	; clear pending

	hw_ldq/p r0, CNS__I_CTL(r1)		; get i_ctl
	bis	r31, #1, r2			; get a 1
	sll	r2, #EV6__I_CTL__PCT0_EN__S, r2	; shift into place
	bic	r0, r2, r0			; zap pc0
	bis	r31, #1, r2			; get a 1
	sll	r2, #EV6__I_CTL__SPCE__S, r2	; shift into place
	bic	r0, r2, r0			; zap spce
	hw_stq/p r0, CNS__I_CTL(r1)		; write back for restore

pal__restore_state_spin0:

.endc						; 1.42

;
; 1.58 If the SMC bits in CNS__M_CTL are zero, set to default of ^x10.
; Restores will be from the modified CNS__M_CTL.
;
	hw_ldq/p r0, CNS__M_CTL(r1)				; 1.58 get m_ctl
	and	r0, #<3@EV6__M_CTL__SPEC_ST_CONS__S>, r2	; 1.58 get SMC
	bne	r2,  pal__restore_state_smc_ok			; 1.58
	bis	r0, #<2@EV6__M_CTL__SPEC_ST_CONS__S>, r0	; 1.58 OR SMC
	hw_stq/p r0, CNS__M_CTL(r1)				; 1.58 write
pal__restore_state_smc_ok:					; 1.58

.if ne kseg_hack				; kseg hack
	hw_stq/p r31, PT__M_CTL(p_temp)
.iff
	RESTORE_REG <M_CTL>, srn=r0, irn=r1, pal=1
.endc

;
; Now restore the IPRs.
;
; Don't do I_CTL until we are ready to turn shadow registers off.
;
; Don't restore DC_CTL. The code has to assume this register has not
; changed. It wouldn't do to fool with the set enables.
;
; Restore the VPTB portion of VA_CTL from CNS__VPTB, the generic spot
; the firmware wants to use when it's modifying VPTB. Also restore the
; control part to PT__VA_CTL.
;
	hw_ldq/p r0, CNS__VA_CTL(r1)			; get va_ctl
	hw_ldq/p r2, CNS__VPTB(r1)			; get vptb
	sll	r0, #<64-EV6__VA_CTL__RSV1__S>, r0	; clean control part
	srl	r0, #<64-EV6__VA_CTL__RSV1__S>, r0
	hw_stq/p r0, PT__VA_CTL(p_temp)			; store control part

.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
	bis	r0, r0, r0			; 1.71
.endc						; 1.70

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.71, 1.73

	srl	r2, #<EV6__VA_CTL__VPTB__S>, r2		; clean vptb part
	sll	r2, #<EV6__VA_CTL__VPTB__S>, r2
	bis	r0, r2, r0				; combine

	hw_mtpr	r0, EV6__VA_CTL				; (5,1L) write va_ctl
	bis	r31, r31, r31				; 1.66
	bis	r31, r31, r31				; 1.66
	bis	r31, r31, r31				; 1.66
	hw_mtpr	r0, EV6__VA_CTL				; 1.66 (5,1L) retire

	ALIGN_FETCH_BLOCK <^x47FF041F>			; 1.66

	RESTORE_REG <IER_CM>, srn=r0, irn=r1, ipr=1		; (4,0L)
	RESTORE_REG <DTB_ALT_MODE>, srn=r0, irn=r1, ipr=1	; (6,0L)
	RESTORE_REG <SIRR>, srn=r0, irn=r1, ipr=1		; (4,0L)

.if ne kseg_hack					; kseg hack
	hw_stq/p r31, CNS__M_CTL(r1)
	hw_stq/p r31, PT__M_CTL(p_temp)	
	hw_mtpr	r31, EV6__M_CTL
.iff
	RESTORE_REG <M_CTL>, srn=r0, irn=r1, ipr=1		; (6,0L)
.endc

	RESTORE_REG <PAL_BASE>, srn=r0, irn=r1, ipr=1		; (4,0L)
	NOP
	NOP
	NOP				; don't restore PCTR_CTL
	NOP
	NOP
	RESTORE_REG <PROCESS_CONTEXT>, srn=r0, irn=r1, ipr=1	; (4,0L)

.if ne ev6_p1
	srl	r0, #EV6__FPE__FPE__S, r2		; shift into position
	and	r2, #1, r2				; clean
	hw_stq/p r2, CNS__FPE_STATE(r1)			; restore 'fpe' state
.endc							; 	from pctx

;
; Current state:
;	r0		process context
;
; Grab ASN and write it to DTB_ASNx
;
	srl	r0, #EV6__ASN__ASN__S, r0	; shift down
	and	r0, #EV6__ASN__ASN__M, r0	; clean it
	sll	r0, #EV6__DTB_ASN0__ASN__S, r0 	; ASN into mbox spot
;
; There must be a scoreboard bit -> register dependency chain to prevent
; hw_mtpr DTB_ASx from issuing while ANY of scoreboard bits <7:4> are set.
;
	hw_mfpr	r2, <EV6__PAL_BASE ! ^xF0>	; (4-7,0L)
	xor	r2, r2, r2			; zap r2
	bis	r2, r0, r0			; force register dependency
	NOP					; force fetch block

	EV6_MTPR r0, EV6__DTB_ASN0, postalign=0	; 1.70 (4,0L)
	EV6_MTPR r0, EV6__DTB_ASN1, prealign=0	; 1.70 (7,1L)
;
; Now restore shadows as needed.
; Analysis:
;	p4		not necessary
;	p5		not necessary
;	p6		not necessary
;	p7		return address. Do not touch.
;	p20		not necessary
;	p_temp		valid. Do not touch.
;	p_misc		restore
;	p23		restore from CNS__EXC_ADDR (or CNS__P23)
;
	hw_ldq/p p_misc, CNS__P_MISC(r1)
	hw_ldq/p p23, CNS__EXC_ADDR(r1)
;
; Now restore integer registers. Restore I_CTL with
; sde clear. After restoring integer registers, rewrite I_CTL
; with sde set. Assumption: PALcode is running with sde set!!
;
; We need to write I_CTL twice in case we are toggling the VA_48 bit
; in order to stall the pipe during the change.
;
; Restore the VPTB portion of the I_CTL from CNS__VPTB, the generic spot
; the firmware wants to use when it's modifying VPTB.
;
	hw_ldq/p r0, CNS__I_CTL(r1)			; get i_ctl
	hw_ldq/p r2, CNS__VPTB(r1)			; get vptb
	sll	r0, #<64-EV6__I_CTL__CHIP_ID__S>, r0	; clean control part
	srl	r0, #<64-EV6__I_CTL__CHIP_ID__S>, r0
	srl	r2, #<EV6__I_CTL__VPTB__S>, r2		; clean vptb part
	sll	r2, #<EV6__I_CTL__VPTB__S>, r2

	ALIGN_FETCH_BLOCK <^x47FF041F>

	bis	r0, r2, r0			; combine
	bic	r0, #<1@EV6__I_CTL__SDE7__S>, r0; zap sde
	RESTORE_REG 8, irn=r1
	RESTORE_REG 9, irn=r1

	EV6_MTPR r0, EV6__I_CTL			; 1.70 (4,0L) write i_ctl
	RESTORE_REG 10, irn=r1
	RESTORE_REG 11, irn=r1
	RESTORE_REG 12, irn=r1

	EV6_MTPR r0, EV6__I_CTL			; 1.70 (4,0L) stall outside IQ
	RESTORE_REG 13, irn=r1
	RESTORE_REG 14, irn=r1
	RESTORE_REG 15, irn=r1

	RESTORE_REG 16, irn=r1			; buffer block 1
	RESTORE_REG 17, irn=r1
	RESTORE_REG 18, irn=r1
	RESTORE_REG 19, irn=r1

	RESTORE_REG 24, irn=r1			; buffer block 2
	RESTORE_REG 25, irn=r1
	RESTORE_REG 26, irn=r1
	RESTORE_REG 27, irn=r1

	RESTORE_REG 28, irn=r1			; buffer block 3
	RESTORE_REG 29, irn=r1
	RESTORE_REG 30, irn=r1
	bis	r31, r31, r31

	RESTORE_REG 4, irn=r1
	RESTORE_REG 5, irn=r1
	RESTORE_REG 6, irn=r1
	RESTORE_REG 7, irn=r1

	RESTORE_REG 20, irn=r1
	RESTORE_REG 21, irn=r1
	RESTORE_REG 22, irn=r1
	RESTORE_REG 23, irn=r1
;
; Now turn shadow registers back on.
;
.if ne applu_fix				; 1.70
	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70
	bis	r0, r0, r0			; 1.70
.endc						; 1.70

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.70, 1.73

	bis	r0, #<1@EV6__I_CTL__SDE7__S>, r0; or in sde
	hw_mtpr	r0, EV6__I_CTL			; (4,0L) write i_ctl
	bis 	r31, r31, r31
	bis 	r31, r31, r31

	hw_mtpr	r0, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now restore r0-r3.
;
	bis	r1, r31, p4			; base of impure area

	RESTORE_REG 0, irn=p4			; restore r0
	RESTORE_REG 1, irn=p4			; restore r1
	RESTORE_REG 2, irn=p4			; restore r2
	RESTORE_REG 3, irn=p4			; restore r3
;
; Clear dump flag and return.
;
	hw_stq/p r31, CNS__FLAG(p4)		; clear dump flag

	bis	p7, #1, p7			; return in pal mode
	PVC_JSR restore_state, bsr=1, dest=1
	hw_ret_stall (p7)			; add stall for pvc

	END_FREE_CODE

	.=^xC00

INITIAL_PCBB:
	.repeat	16
	    .quad 0
	.endr

	ASSUME <.> le <^x0C800> 
