;+
; ev6_vms_callpal.mar
;-

;+
; Last Edit:	28-May-1999
;-

;
; Populate the CALL_PAL space with OPCDEC
;
; HACK ALERT!!!!!!!!!! ??? Bug in HAL!!!!!!! ???
; If this code contains a branch, HAL will later re-compute the branch and
; overwrite the code that was supposed to overwrite this code.
;
; So do the branch to trap__post_km_offset with a jmp.
;
; We should probably also add check for pal mode. (??)
;
post_km_offset = <<trap__post_km - trap__pal_base>>

	. = ^x2000
CALL_PAL__START:
    .if eq add_extra_ret			; 1.50 add_extra_ret
	.repeat 128
	hw_mfpr	p6, EV6__PAL_BASE		; (4,0L) need pal base
	ldah	p6,<<post_km_offset>+32768>@-16(p6)
	lda	p6,<<post_km_offset> & ^xFFFF>(p6)

	lda	p4, SCB__OPCDEC(r31)		; scb
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb
	bis	p6, #1, p6			; pal mode
	bsr	r31, .				; push prediction stack
	PVC_VIOLATE <1007>
	hw_ret	(p6)				; pop prediction stack
						; post
	.align 6
	.endr
    .iff					; 1.50 add_extra_ret
;
; On p2.3, call_pal does extra push on prediction stack. To
; realign, just skip bsr push.
;
	.repeat 128
	hw_mfpr	p6, EV6__PAL_BASE		; (4,0L) need pal base
	ldah	p6,<<post_km_offset>+32768>@-16(p6)
	lda	p6,<<post_km_offset> & ^xFFFF>(p6)

	lda	p4, SCB__OPCDEC(r31)		; scb
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb
	bis	p6, #1, p6			; pal mode
	PVC_VIOLATE <1007>
	hw_ret	(p6)				; pop prediction stack
						; post
	.align 6
	.endr
    .endc					; 1.50 add_extra_ret
	GOTO_FREE_CODE

;+
; CALL_PAL__HALT -- PALcode for halt instruction
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;
; Function:
;	Set PT__HALT_CODE to HALT__SW_HALT. Decrement pc in p23,
;	and branch off to save PCB and enter console.
;
; Exit state:
;	PT__HALT_CODE	HALT__SW_HALT
;	p23		pc of halt instruction
;-
	START_CALL_PAL <HALT>

	subq	p23, #4, p23			; adjust pc (??)
	lda	p20, HALT__SW_HALT(r31)		; halt code
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; store (??)
	br	r31, trap__update_pcb_and_halt

	END_CALL_PAL

;+
; CALL_PAL__CFLUSH
;
; Entry:
;	r16	page frame number (PFN) of page to be flushed
;	p23	pc of instruction following call_pal instruction
;
; Function:
;	Flush all dstream caches of 1 entire page.
;
; Note on implementation:
;	Since the dcache is two-way set associative, we need to
;	do two passes with different tags.
;-
	START_CALL_PAL <CFLUSH>

	br	r31, sys__cflush		; handle in system module

	END_CALL_PAL

;+
; CALL_PAL__DRAINA
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;
; Function:
;	Stall instruction issuing until all prior instruction are
;	guaranteed to complete without incurring aborts.
;
; Note:
;	This call_pal is not all that useful, since it could incur
;	a machine_check_while_in_pal, which is not a very useful
;	state to get in to. However, as long as the machine check
;	flow does not write into p23, we could conceivably, in the
;	machine check flow, determine we were in a draina, and do a
;	recoverable machine check.
;
; Do an MB to drain all the queues, and a hw_ret_stall to synchronize
; retires.
;-
	START_CALL_PAL <DRAINA>

	mb
	NOP					; no hw_ret in 1st fetchblock
	NOP
	NOP

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__LDQP
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;	r16	quadword-aligned physical address
;
; Function:
;	Fetch and write to r0 the quadword-aligned memory operand.
;	r0 <- (r16)
;
; Exit state:
;	r0		data
; 
;-
	START_CALL_PAL <LDQP>

	hw_ldq/p r0, 0(r16)		; do the load
	NOP				; hw_ret can't be in 1st block
	NOP
	NOP

	hw_ret	(p23)			; return to user

	END_CALL_PAL

;+
; CALL_PAL__STQP
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;	r16	quadword-aligned physical address
;	r17	data to be written
;
;
; Function:
;	Write the quadword contents of r17 to the memory location
;	whose physical address is in r16.
;	(r16) <- r17
;
;-
	START_CALL_PAL <STQP>
	hw_stq/p r17, 0(r16)		; do the store
	NOP				; hw_ret can't be in 1st block
	NOP
	NOP

	hw_ret	(p23)			; return to user

	END_CALL_PAL

;+
; CALL_PAL__SWPCTX
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;	r16	physical address of new HWPCB
;
; Function:
;	Swap privileged context.
;_
	START_CALL_PAL <SWPCTX>

	and	r16, #^x7F, p4			; check for r16<6:0> NE 0
;
; Get current process context, so we can extract astrr and asten information.
;
	hw_mfpr	p20, EV6__PROCESS_CONTEXT	; (4,0L) get the context
	bne	p4, call_pal__swpctx_bad_pcb	; error if unaligned
;
; Fetch new information out of the new HWPCB
;
	hw_ldq/p p4, PCB__AST(r16)		; get new ast info
	hw_ldq/p p5, PCB__FEN(r16)		; get dat, pme, fen
	hw_ldq/p p6, PCB__ASN(r16)		; get new ASN
;
; From Ibox Process Context IPR, shift and clean the astrr, aster information
;
	ASSUME EV6__ASTER__ASTER__S eq 5
	ASSUME EV6__ASTRR__ASTRR__S eq 9

	srl	p20, #EV6__ASTER__ASTER__S, p20 ; shift down
	and	p20, #^xFF, p20			; clean

	CONT_CALL_PAL <SWPCTX>
;
; Current state:
;	p4		new ast quadword
;	p5		new dat/pme/fen quadword
;	p6		new ASN quadword
;	p20<3:0>	old asten
;	p20<7:4>	old astrr
;	p23		return pc
;
;	r16		physical address of new HWPCB
;

;
; Now write ASN to DTB_ASNx.
;
ASSUME PCB__ASN__M eq ^xFF

	and	p6, #PCB__ASN__M, p6		; clean ASN quadword
	sll	p6, #EV6__DTB_ASN0__ASN__S, p6	; ASN into mbox spot
;
; There must be a scoreboard bit -> register dependency chain to prevent
; hw_mtpr DTB_ASx from issuing while ANY of scoreboard bits <7:4> are set.
;
	hw_mfpr	p7, <EV6__PAL_BASE ! ^xF0>	; (4-7, 0L)
	xor	p7, p7, p7			; zap p7
	bis	p7, p6, p6			; force register dependency
	NOP					; force fetch block

	hw_mtpr	p6, EV6__DTB_ASN0		; (4,0L)
	hw_mtpr	p6, EV6__DTB_ASN1		; (7,1L)
;
; Create Ibox Process Context IPR, filling in ASN, ASTRR, ASTER, FPE, PPCE
;
ASSUME EV6__DTB_ASN0__ASN__S eq 56
ASSUME EV6__ASN__ASN__S eq 39
ASN_SHIFT = <EV6__DTB_ASN0__ASN__S - EV6__ASN__ASN__S>

	srl	p6, #<ASN_SHIFT>, p6		; ASN back into ibox position
	and	p4, #^xFF, p4			; clean AST quadword
	sll	p4, #EV6__ASTER__ASTER__S, p4	; shift into position
	bis	p6, p4, p6			; or in AST info

ASSUME EV6__FPE__FPE__S eq 2
FPE_SHIFT = <EV6__FPE__FPE__S - PCB__FEN__S>

	sll	p5, #FPE_SHIFT, p4		; get FEN into position
	and	p4, #<1@EV6__FPE__FPE__S>, p4	; clean it
	bis	p6, p4, p6			; or in FEN enable

.if ne ev6_p1
	hw_ldq/p p7, PT__IMPURE(p_temp)		; get impure pointer
	and	p5, #1, p4			; clean PCB FEN bit
	hw_stq/p p4, CNS__FPE_STATE(p7)		; write FPE_STATE
.endc

ASSUME EV6__PPCE__PPCE__S eq 1
PPCE_SHIFT = <PCB__PME__S - EV6__PPCE__PPCE__S>

	ALIGN_FETCH_BLOCK <^x47FF041F>

	srl	p5, #PPCE_SHIFT, p4		; get pme into position
	and	p4, #<1@EV6__PPCE__PPCE__S>, p4	; clean it
	bis	p6, p4, p6			; or in ppce bit
	hw_mtpr	p6, EV6__PROCESS_CONTEXT	; (4,0L) write it
;
; Spinlock workaround
;
; Current state:
;	p4	ppce bit in process_context position
;	p6	process_context -- LEAVE ALONE -- gets written again below
;
; PT__PCTR_FLAG	<0> real pc0
;		<1> real spce
;		<2> real ppce
;

.if ne spinlock_hack				; 1.41
	hw_ldq/p p7, PT__PCTR_FLAG(p_temp)	; get flag
	beq	p4, call_pal__swpctx_spin0	; branch for pme=0
;
; Turn on PPCE. See if already on.
;
	and	p7, #4, p4			; check previous
	bne	p4, call_pal__swpctx_spin_end	; branch if already on
;
; Going from PPCE off to PPCE on.
;
	bis	p7, #4, p5			; turn 'on'
	hw_stq/p p5, PT__PCTR_FLAG(p_temp)	; save that
;
; If spce 'off' and pc0 'off', do nothing more.
; If spce 'off' and pc0 'on', turn spce off and restore the counter.
; If spce 'on' and pc0 'off', do nothing more.
; If spce 'on' and pc0 'on', do nothing more.
;
	cmpeq	p7, #1, p4			; 001 = spce 'off', pc0 'on'
	beq	p4, call_pal__swpctx_spin_end	; branch if not that case

	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get I_CTL
	bis	r31, #1, p5			; get a 1
	sll	p5, #EV6__I_CTL__SPCE__S, p5	; get into spce position
	bic	p4, p5, p4			; zap spce
	hw_mtpr	p4, EV6__I_CTL			; (4,0L) write I_CTL

	hw_ldq/p p7, PT__PCTR_SAVE(p_temp)	; get saved PCTR
	GET_32CONS p5, <^xFFFFF>, r31		; get mask for count field
	sll	p5, #EV6__PCTR_CTL__PCTR0__S, p5; mask for pctr0
	and	p7, p5, p7			; clean pctr0

	hw_mfpr	p4, EV6__PCTR_CTL		; (4,0L) get PCTR_CTL
	bic	p4, p5, p4			; clean old pctr0
	bis	p4, p7, p4			; new pctr0
	bis	r31, r31, r31			; nop
	hw_mtpr	p4, EV6__PCTR_CTL		; (4,0L) write PCTR_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire
	bis	r31, r31, r31
	bis	r31, r31, r31
	br	r31, call_pal__swpctx_spin_end	; done
;
; Turn off PPCE. See if already off.
;
; PT__PCTR_FLAG	<0> real pc0
;		<1> real spce
;		<2> real ppce
;
; p7=flag
;
call_pal__swpctx_spin0:
	and	p7, #4, p4			; check previous
	beq	p4, call_pal__swpctx_spin_end	; branch if already off
;
; Going from PPCE on to PPCE off.
;
	bic	p7, #4, p7			; ppce is being turned 'off'
	hw_stq/p p7, PT__PCTR_FLAG(p_temp)	; save that
;
; If spce 'off' and pc0 'off', do nothing more.
; If spce 'off' and pc0 'on', save the counter, and turn spce back on.
; If spce 'on' and pc0 'off', do nothing.
; If spce 'on' and pc0 'on', do nothing.
;
	cmpeq	p7, #1, p4			; 001 = spce 'off, pc0 'on'
	beq	p4, call_pal__swpctx_spin_end	; br if not that case

	hw_mfpr p5, EV6__PCTR_CTL		; (4,0L) get PCTR
	hw_stq/p p5, PT__PCTR_SAVE(p_temp)	; save it away
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get I_CTL
	bis	r31, #1, p5			; get a 1
	sll	p5, #EV6__I_CTL__SPCE__S, p5	; get into spce position
	bis	p4, p5, p4			; or in spce

	hw_mtpr	p4, EV6__I_CTL			; (4,0L) write I_CTL
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

call_pal__swpctx_spin_end:

.endc						; 1.41

;
; Current state
;	p4, p5, p7	available
;	p6		process context
;	p20<3:0>	old asten
;	p20<7:4>	old astrr
;	p23		return pc
;
;	r16		physical address of new HWPCB
;


;
; Get CPC (charged process cycles). Get new PTBR.
;
	hw_mtpr	p6, EV6__PROCESS_CONTEXT; (4,0L) force retire
	hw_ldq/p p7, PCB__CPC(r16)	; get new CPC
	hw_ldq/p p6, PCB__PTBR(r16)	; get new PTBR
;
; Get current PCC, and separate out offset and counter.
;
	rpcc	p4			; tmp1 <- offset<63:32>|counter<31:0>
	zap	p4, #^xF0, p5		; tmp2 <- ZEXT(tmp1<31:0>) 
	srl	p4, #32, p4		; tmp3 <- ZEXT(tmp1<63:32>)
	addq	p4, p5, p4		; CPC for current process
;
; Compute new offset by computing CPC - current counter. Write it.
;
	zap	p7, #^xF0, p7		; tmp4 <- ZEXT(PCB_PCC<31:0>)
	subq	p7, p5, p7		; tmp4 <- tmp4 - tmp2
	sll	p7, #32, p7		; move into position
	hw_mtpr	p7, EV6__CC		; (5,1L) write it

	bis	r31, r31, r31		; 1.53
	bis	r31, r31, r31		; 1.53
	bis	r31, r31, r31		; 1.53
	hw_mtpr	p7, EV6__CC		; 1.53 (5,1L) force the retire
;
; Write out new PTBR
;
	sll	p6, #page_offset_size_bits, p6	; convert PFN
	hw_stq/p p6, PT__PTBR(p_temp)		; store new PTBR
;
; Get address of current (soon to be old) HWPCB.
; Store CPC, ast info, and current ksp.
;
	hw_ldq/p p5, PT__PCBB(p_temp)	; get current PCBB
	hw_ldq/p p6, PCB__KSP(r16)	; get new ksp

	hw_stl/p p4, PCB__CPC(p5)	; save CPC
	hw_stq/p p20, PCB__AST(p5)	; save ast information
	hw_stq/p r30, PCB__KSP(p5)	; save current KSP

	bis	 p6, r31, r30		; new sp
	hw_stq/p r16, PT__PCBB(p_temp)	; new PCBB

	hw_ret_stall (p23)		; return with stall
;
; HWPCB not aligned. 
;
; Current state:
;	p23	return pc
;
; Take an illegal operand exception via trap__post_km
;
; Exit state:
;	PT__FAULT_PC	fault pc
;	PT__FAULT_SCB	scb offset
;
call_pal__swpctx_bad_pcb:
	lda	p4, SCB__ILLOP(r31)		; illegal operand
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store fault pc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb offset
	br	r31, trap__post_km

	END_CALL_PAL

;+
; CALL_PAL__MFPR_ASN
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;
; Function:
;	Write into r0 the Address Space Number (ASN).
;
; Exit state:
;	r0	ZEXT(ASN)
;_
	START_CALL_PAL <MFPR_ASN>

ASSUME EV6__ASN__ASN__M eq ^xFF

	hw_mfpr	r0, EV6__PROCESS_CONTEXT	; (4,0L)
	srl	r0, #EV6__ASN__ASN__S, r0	; shift down
	and	r0, #EV6__ASN__ASN__M, r0	; clean it
	NOP					; put hw_ret in 2nd fetch block

	hw_ret	(p23)

	END_CALL_PAL

;+
; CALL_PAL__MTPR_ASTEN
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16<7:4>	U/S/E/K mode on bits
;	r16<3:0>	U/S/E/K mode clear bits
;
; Function:
;	Return the current asten state, and set a new state based on the
;	contents of r16.
;
;	ASTEN<3:0> <-	((ASTEN<3:0> AND r16<3:0>) OR r16<7:4>)
;
; Exit state:
;	r0		ZEXT(old ASTEN<3:0>)
;-
	START_CALL_PAL <MTPR_ASTEN>

	hw_mfpr	r0, EV6__PROCESS_CONTEXT	; (4,0L) get asten bits

	srl	r16, #4, p4			; shift 'on' bits to <3:0>
	and	p4, #^xF, p4			; clean
	srl	r0, #EV6__ASTER__ASTER__S, r0	; get asten bits
	and	r0, #^xF,r0			; clean

	and	r0, r16, p7			; and with 'clear' bits
	bis	p7, p4, p7			; or with 'on' bits

	sll	p7, #EV6__ASTER__ASTER__S, p7	; shift into position
	hw_mtpr	p7, EV6__ASTER			; (4,0L) just write aster

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__MTPR_ASTSR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16<7:4>	U/S/E/K mode on bits
;	r16<3:0>	U/S/E/K mode clear bits
;
; Function:
;	Return the current astsr state, and set a new state based on the
;	contents of r16.
;
;	ASTSR<3:0> <-	((ASTSR<3:0> AND r16<3:0>) OR r16<7:4>)
;
; Exit state:
;	r0		ZEXT(ASTSR<3:0>)
;-
	START_CALL_PAL <MTPR_ASTSR>

	hw_mfpr	r0, EV6__PROCESS_CONTEXT	; (4,0L) get astsr bits

	srl	r16, #4, p4			; shift 'on' bits to <3:0>
	and	p4, #^xF, p4			; clean
	srl	r0, #EV6__ASTRR__ASTRR__S, r0	; get astsr bits
	and	r0, #^xF,r0			; clean

	and	r0, r16, p7			; and with 'clear' bits
	bis	p7, p4, p7			; or with 'on' bits

	sll	p7, #EV6__ASTRR__ASTRR__S, p7	; shift into position
	hw_mtpr	p7, EV6__ASTRR			; (4,0L) just write astrr

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__CSERVE
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Do an implementation specific console service.
;
; Exit state:
;	Exit to sys__cserve
;
;-
	START_CALL_PAL <CSERVE>

	br	r31, sys__cserve		; handle in system code

	END_CALL_PAL

;+
; CALL_PAL__SWPPAL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Swap control to another PALcode.
;
; Register use:
;	r16	new PALcode identifier
;			0 	=> new PALcode at address 0
;			2 	=> OSFPAL
;			>255	=> new PALcode at address in r16
;	r17	new PC
;	r18	new PCBB
;	r19	new VPTB
;
; Exit state:
;	r0	0	success (PALcode was switched)
;		1	unknown PALcode variant
;		2	know PALcode variant, but not loaded
;-
	START_CALL_PAL <SWPPAL>

	cmpule	r16, #255, r0			; see if a variant
	cmoveq	r16, r16, r0			; r16 = 0 is a valid address
	bis	r16, r31, p5			; will jump through p5
	blbc	r0, call_pal__swppal_addr	; try as an address

	cmpeq	r16, #2, r0			; see if our buddy OSFPAL
	blbc	r0, call_pal__swppal_fail	; nope, don't know this fellow

.iif eq <.&4>, NOP
	br	p4, call_pal__swppal_osf
	.quad	PAL__ENTER_OSF			; address must fit in a quadword
call_pal__swppal_osf:
	hw_ldq/p p4, 0(p4)			; fetch target address pointer

	CONT_CALL_PAL <SWPPAL>			; 1.59 move here to allow for
						;   nonzero_console_base macro
.if ne	nonzero_console_base
	get_base_vms p5
	or	p4, p5, p4
.endc
	hw_ldq/p p5, 0(p4)			; fetch target address

	ble	p5, call_pal__swppal_fail	; if not linked, say not loaded
	lda	p4, ^x7FFF(r31)			; checker for pal base
	and	p5, p4, p4			; get low 15 bits

	cmpeq	p4, #0, r0			; check for non-zero bits
	blbc	r0, call_pal__swppal_fail	; if not clear, say unknown
;
; Branch to here explicitly when r16 contains the address of the new PALcode
;
call_pal__swppal_doit:				; 1.59

;
; Spinlock workaround.
; Turn off SPCE. Turn off PC0. Clear all flags.
;
.if ne spinlock_hack				; 1.42

	hw_stq/p r31, PT__PCTR_SAVE(p_temp)	; clear pctr save location
	hw_stq/p r31, PT__PCTR_FLAG(p_temp)	; clear flag
	hw_stq/p r31, PT__PCTR_R4(p_temp)	; clear r4 location
	hw_stq/p r31, PT__PCTR_PEND(p_temp)	; clear pending
	hw_stq/p r31, PT__PCTR_VMS(p_temp)	; clear 'vms'

	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get i_ctl
	bis	r31, #1, p6			; 1.57 get a 1
	sll	p6, #EV6__I_CTL__PCT0_EN__S, p6	; shift into place
	bic	p4, p6, p4			; zap pc0
	bis	r31, #1, p6			; 1.57 get a 1
	sll	p6, #EV6__I_CTL__SPCE__S, p6	; shift into place
	bic	p4, p6, p4			; zap spce
	hw_mtpr	p4, EV6__I_CTL			; (4,0L) write i_ctl
.endc						; 1.42

;
; Current state:
;	p5	address
;
	bis	r31, #1, p4			; get a '1'
	sll	p4, #P_MISC__SWITCH__S,p4	; switch bit into position
	bis	p_misc, p4, p_misc		; mark 'switch'

	bis	r31, r31, r0			; status to success
	bis	p5, #1, p5			; set pal mode bit
	bsr	r31, .				; push prediction stack
	PVC_VIOLATE	<1007>			; go to it
	hw_ret	(p5)				; pop prediction stack
;
; Looks like an address, either 0 or > 255. Check low bits.
; Current state:
;	p5	address
;
call_pal__swppal_addr:
	lda	p4, ^x7FFF(r31)			; checker for pal base
	and	p5, p4, p4			; get low 15 bits
	cmpeq	p4, #0, r0			; check for non-zero bits
	blbc	r0, call_pal__swppal_fail	; if not clear, say unknown
	br	r31, call_pal__swppal_doit	; 1.59 address ok, so go on
;
; We have failed.
; Current state:
;	r0	0	unknown variant or bad address
;		1	osfpal, but not loaded
; Exit state:
;	r0	1	unknown variant or bad address
;		2	osfpal, but not loaded
;
call_pal__swppal_fail:
	
	addq	r0, #1, r0			; affect r0
	hw_ret	(p23)				; return

	END_CALL_PAL


;+
; CALL_PAL__MFPR_FEN
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0 <- ZEXT(FEN)	
;
; Exit state:
;	r0	ZEXT(FEN)
;-
	START_CALL_PAL <MFPR_FEN>

.if ne ev6_p1
	bis	r31, r31, r31				; keep pvc happy
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_ldq/p p5, PT__IMPURE(p_temp)			; get impure pointer
	hw_ldq/p r0, CNS__FPE_STATE(p5)			; get FPE state
	and	r0, #1, r0				; clean
.iff
	hw_mfpr	r0, EV6__PROCESS_CONTEXT		; (4,0L) get context
	srl	r0, #EV6__PROCESS_CONTEXT__FPE__S, r0	; shift down
	and	r0, #EV6__PROCESS_CONTEXT__FPE__M, r0	; clean
	NOP						; hw_ret in next block
.endc
	hw_ret	(p23)					; return

	END_CALL_PAL


;+
; CALL_PAL__MTPR_FEN
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16<0>		new fen bit
;
; Function:
;	FEN 	   <- R16<0>
;	(HWPCB+56) <- FEN
;-
	START_CALL_PAL <MTPR_FEN>

	hw_ldq/p p4, PT__PCBB(p_temp)		; get PCBB
	and	r16, #1, r16			; clean new fen
	sll	r16, #EV6__FPE__FPE__S, p7	; shift into position
	hw_mtpr	p7, EV6__FPE			; (4,0L) write new fpe

	hw_mtpr	p7, EV6__FPE			; (4,0L) force retire
	hw_stl/p r16, PCB__FEN(p4)		; store FEN in PCB
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	CONT_CALL_PAL <MTPR_FEN>

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

.if ne ev6_p1
	hw_ldq/p p5, PT__IMPURE(p_temp)		; get impure pointer
	hw_stq/p r16, CNS__FPE_STATE(p5)	; save 'fpe' state
.endc
	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__MTPR_IPIR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	IPIR <- r16
;	Handled in system-specific code
;-
	START_CALL_PAL <MTPR_IPIR>

	br	r31, sys__mtpr_ipir

	END_CALL_PAL

;+
; CALL_PAL__MFPR_IPL
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;
; Function:
;	r0 <- ZEXT(PS<IPL>)
;	Return current interrupt priority level in r0.
;
; Exit state:
;	r0	current IPL
;-
	START_CALL_PAL <MFPR_IPL>

	srl	p_misc, #P_MISC__IPL__S, r0		; get ipl
	and	r0, #P_MISC__IPL__M, r0			; clean it
	NOP
	NOP						; hw_ret in next block

	hw_ret	(p23)

	END_CALL_PAL

;+
; CALL_PAL__MTPR_IPL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16<4:0>	new ipl
;	r16<63:5>	should be zero. Otherwise UNDEFINED results.
;
; Function:
;	Return the current interrupt priority level in r0, and set the
;	interrupt priority level to the value in r16. The new ier bits
;	are taken from an ipl-indexed table of quadwords.
;
; Exit state:
;	r0	ZEXT(PS<IPL>)
;-
	START_CALL_PAL <MTPR_IPL>

ASSUME P_MISC__IPL__S eq 8

.if ne intercept_change_ipl
	br	r31, call_pal__mtpr_ipl_intercept
call_pal__mtpr_ipl_intercept_return:
.endc
	hw_mfpr	p4, EV6__PAL_BASE		; (4,0L) get pal base

	sll	r16, #P_MISC__IPL__S, p7	; move new ipl into position

	s8addq	r16, p4, p4			; pal base + index
	lda	p4, ipl_offset(p4)		; pal base + table base + index
	hw_ldq/p p4, (p4)			; get new ier

	zap	p_misc, #2, r16			; clear out <15:8>
	extbl	p_misc, #1, r0			; get old ipl to <0>
	or	p7, r16, p_misc			; put new ipl in p_misc

	hw_mtpr	p4, EV6__IER			; (4,0L) write new ier

.if ne spinlock_hack				; 1.41

	CONT_CALL_PAL <MTPR_IPL>

	hw_ldq/p p7, PT__PCTR_PEND(p_temp)	; check pending
	beq	p7, call_pal__mtpr_ipl_spin0	; branch for none
	lda	p7, IPL__PERFMON(r31)		; ipl for perf counter
	and	r16, #^x1F, p6			; 1.42 clean ipl
	subq	p7, p6, p7			; perfmon - current
	ble	p7, call_pal__mtpr_ipl_spin0	; branch if can't take now
;
; We can take the perfmon interrupt now because IPL is below the
; real PERFMON value.
;
	hw_ldq/p p6, PT__PCTR_R4(p_temp)	; get FAULT_R4 value
	hw_stq/p r31, PT__PCTR_PEND(p_temp)	; clear pending
	br	r31, sys__interrupt_pc_take_int	; now take interrupt

call_pal__mtpr_ipl_spin0:			; now return
	hw_ret_stall (p23)			;

.endc						; 1.41

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__MFPR_MCES
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return in r0 the MCES IPR.
;	r0 <- ZEXT(MCES)
;
; Exit state:
;	r0		MCES
;-
	START_CALL_PAL <MFPR_MCES>

ASSUME P_MISC__MCES__MCHK__S eq 16

	extbl	p_misc, #2, r0			; get mces from p_misc
	NOP					; no ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_MCES
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		mces information
;
; Function:
;	if (r16<0> eq 1) then MCES<0> <- 0	mchk
;	if (r16<1> eq 1) then MCES<1> <- 0	system correctable -- sce
;	if (r16<2> eq 1) then MCES<2> <- 0	processor correctable -- pce
;	MCES<3> <- r16<3>			set <- disable pce -- dpc
;	MCES<4> <- r16<4>			set <- disable sce -- dsc
;
;-
	START_CALL_PAL <MTPR_MCES>

ASSUME P_MISC__MCES__MCHK__S eq 16

mces_clear = <<1@MCES__MCHK__S> ! <1@MCES__SCE__S> ! <1@MCES__PCE__S>>
mces_dis = <<1@MCES__DPC__S> ! <1@MCES__DSC__S>>

ASSUME mces_clear eq <^x7>
ASSUME mces_dis le <^x18>

	extbl	p_misc, #2, p4		; get mces from p_misc

	and	r16, #mces_clear, p6	; get mchk, sce, pce
	ornot	r31, p6, p6		; flip mchk, sce, pce bits
	and	r16, #mces_dis, p7	; get disable bits

	and	p4, p6, p4		; update mchk, sce, pce
	bic	p4, #mces_dis, p4	; clear old dpc, dsc
	or	p4, p7, p4		; update dpc, dsc

	zap	p_misc, #4, p_misc		; clear out old mces
	sll	p4, #P_MISC__MCES__MCHK__S, p4	; move into position
	bis	p_misc, p4, p_misc		; or new mces in

	hw_ret	(p23)			; return to user

	END_CALL_PAL

;+
; CALL_PAL__MFPR_PCBB
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return in r0 the Privileged Contect Block Base register.
;	(The PCBB is written by SWPCTX.)
;
; Exit state:
;	r0	ZEXT(PCBB)
;-
	START_CALL_PAL <MFPR_PCBB>

	hw_ldq/p r0, PT__PCBB(p_temp)	; get pcbb
	NOP				; no ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)			; return

	END_CALL_PAL

;+
; CALL_PAL__MFPR_PRBR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return in r0 the Processor Base Register.
;
; Exit state:
;	r0	PRBR
;-
	START_CALL_PAL <MFPR_PRBR>

	hw_ldq/p r0, PT__PRBR(p_temp)	; get prbr
	NOP				; no ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)			; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_PRBR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		new PRBR value
;
; Function:
;	PRBR <- r16
;-
	START_CALL_PAL <MTPR_PRBR>

	hw_stq/p r16, PT__PRBR(p_temp)	; write prbr
	NOP				; no ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)			; return

	END_CALL_PAL

;+
; CALL_PAL__MFPR_PTBR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return in r0 the value of the Page Table Base Register.
;	(The PTBR is written during SWPCTX.)
;
; Exit state:
;	r0		PTBR
;-
	START_CALL_PAL <MFPR_PTBR>

	hw_ldq/p r0, PT__PTBR(p_temp)		; get PTBR
	srl	r0, #page_offset_size_bits, r0	; convert from PA to PFN
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MFPR_SCBB
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return in r0 to value of the System Control Block Register.
;
; Exit state:
;	r0		ZEXT(SCB)
;-
	START_CALL_PAL <MFPR_SCBB>

	hw_ldq/p r0, PT__SCBB(p_temp)		; get SCBB
	srl	r0, #page_offset_size_bits, r0	; convert from PA to PFN
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_SCBB
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		SCBB
;
; Function:
;	SCBB <- r16
;
;-
	START_CALL_PAL <MTPR_SCBB>

	zapnot	r16, #^xF, r16			; clear IGN longword
	sll	r16, #page_offset_size_bits, p6; convert from PFN to PA

	hw_stq/p p6, PT__SCBB(p_temp)		; update SCBB
	NOP					; no hw_ret in 1st fetch block

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_SIRR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16<3:0>	software interrupt request level
;
; Function:
;	Request a software interrupt for a particular Interrupt Priority
;	Level.
;
;	IF r16<3:0> NE 0 THEN SISR<r16<3:0>> <- 1
;
; Exit state:
;	r16		unpredictable
;-
	START_CALL_PAL <MTPR_SIRR>

	hw_mfpr	p7, EV6__SIRR				; (4,0L) get SIRR
	and	r16, #^xF, r16				; clean
	lda	p4, 1@<EV6__SIRR__SIR__S-1>(r31)	; get a 1
	sll	p4, r16, p4				; shift 1 into position
	bis	p7, p4, p7				; set the bit
	hw_mtpr	p7, EV6__SIRR				; (4,0L) write SIRR
	hw_ret_stall (p23)				; return with stall

	END_CALL_PAL

;+
; CALL_PAL__MFPR_SISR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0 <- ZEXT(SISR<15:0>)
;
; Note:
;	The SIRR value is being cleaned, though it's probably not
;	necessary, since the other bits are RAZ.
;
; Exit state:
;	r0		ZEXT(SISR<15:0>)
;-
	START_CALL_PAL <MFPR_SISR>

	hw_mfpr	r0, EV6__SIRR			; (4,0L) get SIRR
	lda	p7, ^x7FFF(r31)		; start to create clean mask
	lda	p7, ^x7FFF(p7)		; create ^xFFFE
	srl	r0, #<EV6__SIRR__SIR__S-1>, r0	; shift field to <15:0>
	and	r0, p7, r0			; clear RAZ bits
	hw_ret	(p23)				; return	

	END_CALL_PAL

;+
; CALL_PAL__MFPR_TBCHK
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	TBCHK is not implemented in ev6 hardware.
;	r0<63> <- 1
;
; Exit state:
;	r0<63>		1
;-
	START_CALL_PAL <MFPR_TBCHK>

	bis	r31, #1, r0			; get a 1
	sll	r0, #63, r0			; shift into position
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_ret (p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_TBIA
;
; Entry:
;	p23		pc of instruction following the call_pal instruction

; Function:
;	Invalidate all entries in the Translation Buffer.
;-
	START_CALL_PAL <MTPR_TBIA>

	hw_mtpr	r31, EV6__ITB_IA		; (4,0L)
	hw_mtpr	r31, EV6__DTB_IA		; (7,0L)
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_mtpr	r31, EV6__IC_FLUSH		; (4,0L) flush the icache
	bne	r31, .				; pvc #24 -- separate flush and stall
	hw_ret_stall (p23)			; return with stall
						; (flush icache)
	END_CALL_PAL

;+
; CALL_PAL__MTPR_TBIAP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Invalidate all TB entries with PTE<ASM> clear.
;-
	START_CALL_PAL <MTPR_TBIAP>

	hw_mtpr	r31, EV6__ITB_IAP		; (4,0L)
	hw_mtpr	r31, EV6__DTB_IAP		; (7,0L)
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_mtpr	r31, EV6__IC_FLUSH_ASM		; (4,0L)
	bne	r31, .				; pvc #24 -- separate flush and stall
	hw_ret_stall (p23)			; return with stall
						; (flush relevant icache)
	END_CALL_PAL

;+
; CALL_PAL__MTPR_TBIS
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		va
;
; Function:
;	Invalidate single DTB and ITB entry.
;
; Note:
;	Even though the SRM says r0 is unpredictable, don't touch it
;	or EGORE will fail tbis testing.
;-
	START_CALL_PAL <MTPR_TBIS>
;
; There must be a scoreboard bit -> register dependency chain to prevent
; hw_mtpr DTB_ISx from issuing while ANY of scoreboard bits <7:4> are set.
;
	hw_mfpr	p6, <EV6__PAL_BASE ! ^xF0>	; (4-7, 0L)
	xor	p6, p6, p6			; zap p6
	bis	p6, r16, r16			; force register dependency
	NOP					; force fetch block

	hw_mtpr	r16, EV6__DTB_IS0		; (6,0L)
	hw_mtpr	r16, EV6__DTB_IS1		; (7,1L)
	NOP					; force fetch block
	NOP

	hw_mtpr	r16, EV6__ITB_IS		; (4&6,0L)

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__MFPR_ESP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0 <- (IPR_PCBB + HWPCB_ESP)
;
; Exit state:
;	r0		ESP
;-
	START_CALL_PAL <MFPR_ESP>

	hw_ldq/p r0, PT__PCBB(p_temp)		; get PCBB
	hw_ldq/p r0, PCB__ESP(r0)		; get ESP
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_ESP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		new ESP
;
; Function:
;	(IPR_PCBB + HWPCB_ESP) <- r16
;-
	START_CALL_PAL <MTPR_ESP>

	hw_ldq/p p7, PT__PCBB(p_temp)		; get PCBB
	hw_stq/p r16, PCB__ESP(p7)		; store new ESP
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MFPR_SSP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0 <- (IPR_PCBB + HWPCB_SSP)
;
; Exit state:
;	r0		SSP
;-
	START_CALL_PAL <MFPR_SSP>

	hw_ldq/p r0, PT__PCBB(p_temp)		; get PCBB
	hw_ldq/p r0, PCB__SSP(r0)		; get SSP
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_SSP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		new SSP
;
; Function:
;	(IPR_PCBB + HWPCB_SSP) <- r16
;-
	START_CALL_PAL <MTPR_SSP>

	hw_ldq/p p7, PT__PCBB(p_temp)		; get PCBB
	hw_stq/p r16, PCB__SSP(p7)		; store new SSP
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MFPR_USP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0 <- (IPR_PCBB + HWPCB_USP)
;
; Exit state:
;	r0		USP
;-
	START_CALL_PAL <MFPR_USP>

	hw_ldq/p r0, PT__PCBB(p_temp)		; get PCBB
	hw_ldq/p r0, PCB__USP(r0)		; get USP
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_USP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		new USP
;
; Function:
;	(IPR_PCBB + HWPCB_USP) <- r16
;-
	START_CALL_PAL <MTPR_USP>

	hw_ldq/p p7, PT__PCBB(p_temp)		; get PCBB
	hw_stq/p r16, PCB__USP(p7)		; store new USP
	NOP					; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_TBISD
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		va
;
; Function:
;	Invalidate single DTB entry.
;-
	START_CALL_PAL <MTPR_TBISD>

;
; There must be a scoreboard bit -> register dependency chain to prevent
; hw_mtpr DTB_ISx from issuing while ANY of scoreboard bits <7:4> are set.
;
	hw_mfpr	p6, <EV6__PAL_BASE ! ^xF0>	; (4-7,0L)
	xor	p6, p6, p6			; zap p6
	bis	p6, r16, r16			; force register dependency
	NOP					; force fetch block

	hw_mtpr	r16, EV6__DTB_IS0		; (6,0L)
	hw_mtpr	r16, EV6__DTB_IS1		; (7,1L)
	NOP					; force fetch block (??)
	NOP

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__MTPR_TBISI
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		va
;
; Function:
;	Invalidate single ITB entry.
;-
	START_CALL_PAL <MTPR_TBISI>

	hw_mtpr	r16, EV6__ITB_IS		; (4&6,0L)
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__MFPR_ASTEN
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return AST enable state in r0
;
; Exit state:
;	r0		ZEXT(ASTEN<3:0>)
;-
	START_CALL_PAL <MFPR_ASTEN>

	hw_mfpr	r0, EV6__PROCESS_CONTEXT	; (4,0L) get asten bits
	srl	r0, #EV6__ASTER__ASTER__S, r0	; get asten bits
	and	r0, #^xF,r0			; clean
	NOP					; no hw_ret in 1st fetch block

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MFPR_ASTSR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return AST pending state in r0
;
; Exit state:
;	r0		ZEXT(ASTSR<3:0>)
;-
	START_CALL_PAL <MFPR_ASTSR>

	hw_mfpr	r0, EV6__PROCESS_CONTEXT	; (4,0L) get astrr bits
	srl	r0, #EV6__ASTRR__ASTRR__S, r0	; get astrr bits
	and	r0, #^xF,r0			; clean
	NOP					; no hw_ret in 1st fetch block

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MFPR_VPTB
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return in r0 the VPTB.
;
; Exit state:
;	r0		VPTB
;
; Note: PT__VPTB already as <29:0> cleared.
;-
	START_CALL_PAL <MFPR_VPTB>

	hw_ldq/p r0, PT__VPTB(p_temp)		; get VPTB
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_VPTB
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16		new VPTB
;
; Function:
;	VPTB <- r16
;
; Note: We clean <29:0> before storing.
;-
	START_CALL_PAL <MTPR_VPTB>

ASSUME EV6__I_CTL__VPTB__S eq EV6__VA_CTL__VPTB__S
;
; 1.53 Move around code to force retire of EV6__VA_CTL before the hw_ret_stall.

	hw_mfpr	p6, EV6__I_CTL			; (4,0L) get i_ctl
	hw_ldq/p p7, PT__VA_CTL(p_temp)		; get control part of va_ctl

	bis	p7, r16, p7			; new va_ctl
	hw_stq/p r16, PT__VPTB(p_temp)		; new vptb
	hw_mtpr	p7, EV6__VA_CTL			; (5,0L)

	srl	r16, #EV6__I_CTL__VPTB__S, r16	; shift new vptb to clean
	sll	r16, #EV6__I_CTL__VPTB__S, r16	; shift back into position

	sll	p6, #<64 - EV6__I_CTL__VPTB__S>, p6	; clean
	srl	p6, #<64 - EV6__I_CTL__VPTB__S>, p6	; move back
	bis	p6, r16, p6				; or new vptb
	hw_mtpr	p6, <EV6__I_CTL ! ^x20>		; 1.53 (4&5,0L) write i_ctl

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.53
	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__MTPR_PERFMON
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16(a0)		function select
;	r17(a1)		value
;
; Function:
;	Execute indicated performance monitoring function.
;
; Exit state:
;	r0(v0)		Value returned for read
;	r17(a1)		UNPREDICTABLE
;-


;-----------------------------------------------------------------------
;	Arguments and Actions
;-----------------------------------------------------------------------
;	r16(a0) = 0		Disable performance monitoring
;	r17(a1) = bitmask	r17(a1)<0>=1	disable CTR0
;				r17(a1)<1>=1	disable CTR1
;
;	Action: Clear PCTx_EN in I_CTL as directed. If CTRx is already
;		disabled, it stays disabled.
;-----------------------------------------------------------------------
;	r16(a0) = 1		Enable performance monitoring
;	r17(a1) = bitmask	r17(a1)<0>=1	enable CTR0
;				r17(a1)<1>=1	enable CTR1
;
;	Action: Set PCTx_EN in I_CTL as directed. If CTRx is already
;		enabled, it stays enabled.
;-----------------------------------------------------------------------
;	r16(a0) = 2		Mux select
;	r17(a1) = pctr_ctl	r17(a1)<3:0>	SL1 pass 2.3 (see table)
;				r17(a1)<3:2>	SL1 pass 3   (see table)
;				r17(a1)<4>	SL0 (see table below)
;	Action: Write PCTR_CTL SLx fields.
;-----------------------------------------------------------------------
;	r16(a0) = 3		Options
;	r17(a1) = options	r17(a1)<0>	logging option
;					0 => log all processes
;					1 => log only selected processes
;	Action: If r17(a1)<0> is clear, set SPCE bit in I_CTL
;-----------------------------------------------------------------------
;	r16(a0) = 5		Read counters
;
;	Action: Return in r0(v0) the counters of PCTR_CTL
;-----------------------------------------------------------------------
;	r16(a0) = 6		Write counters
;	r17(a1)			r17(a1)<0> Write counter 0
;				r17(a1)<1> Write counter 1
;				r17(a1)<25:6>  new pctr1 value
;				r17(a1)<47:28> new pctr0 value
;	Action: Selectively write counter fields in PCTR_CTL
;-----------------------------------------------------------------------
;	r16(a0) = 7		Enable and write selected counters
;	r17(a1) = bitmask	r17(a1)<0>=1	enable and clear PCTR0
;				r17(a1)<1>=1	enable and clear PCTR1
;
;	Action: Write specified counter(s) as directed
;		Set PCTx_EN in I_CTL as directed
;		User requests a clear by having 0 in the counter fields
;-----------------------------------------------------------------------
;	r16(a0) = 8		Read i_stat (ProfileMe)
;
;	Action: Return in r0(v0) the i_stat values
;-----------------------------------------------------------------------
;	r16(a0) = 9		Read pmpc (ProfileMe)
;
;	Action: Return in r0(v0) the PC of the last profiled instruction
;-----------------------------------------------------------------------
;-----------------------------------------------------------------------
; Pass 2.3 Aggregate mode
;
; SL0 mux values
;	0	PCTR0 <- cycles
;	1	PCTR0 <- retired instructions
; SL1 mux values
;	0000 	PCTR1 <- cycles
;	0001 	PCTR1 <- retired conditional branches
;	0010 	PCTR1 <- cycles (retired branch mispredicts not implemented)
;	0011 	PCTR1 <- retired dtb single misses * 2 (bats 943)
;	0100 	PCTR1 <- retired dtb double misses
;	0101 	PCTR1 <- retired itb misses
;	0110 	PCTR1 <- retired unaligned traps
;	0111 	PCTR1 <- replay traps
;-----------------------------------------------------------------------
; Pass 3 or higher Aggregate mode
;
; SL0 mux values
;	0	aggregate mode
; SL1 mux values
;	00	PCTR0 <- retired  PCTR1 <- cycles
;	01	PCTR0 <- cycles	  PCTR1 <- undefined
;	10	PCTR0 <- retired  PCTR1 <- bcache misses/long probe latency
;	11	PCTR0 <- cycles	  PCTR1 <- Mbox replay traps
;-----------------------------------------------------------------------
; Pass 3 or higher ProfileMe mode
;
; SL0 mux values
;	1	ProfileMe mode
; SL1 mux values
;	00	PCTR0 <- retired  PCTR1 <- cycles
;	01	PCTR0 <- cycles	  PCTR1 <- cycles of delayed retire ptr advance
;	10	PCTR0 <- retired  PCTR1 <- bcache misses/long probe latency
;	11	PCTR0 <- cycles	  PCTR1 <- Mbox replay traps
;-----------------------------------------------------------------------
;-----------------------------------------------------------------------
; Note that the PPCE (per process counter enable) bit in PCTX, is
; set/cleared on swap context, and does not get changed as a result of
; a perfmon call.
;-----------------------------------------------------------------------

	START_CALL_PAL <MTPR_PERFMON>

ASSUME EV6__I_CTL__PCT0_EN__S eq ^x12
ASSUME EV6__I_CTL__PCT1_EN__S eq ^x13

	cmpeq	r16, #5, r0			; check for read counters
	bne	r0, call_pal__perfmon_rd	; branch if so
	cmpeq	r16, #6, r0			; check for write counters
	bne	r0, call_pal__perfmon_wr	; branch if so
	cmpeq	r16, #8, r0			; check for read i_stat
	bne	r0, call_pal__perfmon_rd_istat	; branch if so
	cmpeq	r16, #9, r0			; check for read pmpc
	bne	r0, call_pal__perfmon_rd_pmpc	; branch if so
	cmpeq	r16, #1, r0			; check for enable
	bne	r0, call_pal__perfmon_en	; branch if so
	cmpeq	r16, #2, r0			; check for mux select
	bne	r0, call_pal__perfmon_mux	; branch if so
	cmpeq	r16, #3, r0			; check for options
	bne	r0, call_pal__perfmon_opt	; branch if so

	CONT_CALL_PAL<MTPR_PERFMON>

	cmpeq	r16, #7, r0			; check for enable and write
	bne	r0, call_pal__perfmon_en_wr	; branch if so
	bne	r16, call_pal__perfmon_unknown	; branch for unknown
	br	r31, call_pal__perfmon_dis	; branch for disable
;
; Disable
;	r17(a1) = bitmask	r17(a1)<0>=1	disable CTR0
;				r17(a1)<1>=1	disable CTR1
;
;	Action: Clear PCTx_EN in I_CTL as directed
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_dis:
	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get current control
	and	r17, #3, p5			; clean input
	sll	p5, #EV6__I_CTL__PCT0_EN__S, p5	; shift into position
	bic	p4, p5, p4			; clear bits as requested
	hw_mtpr	p4, EV6__I_CTL			; (4,0L)
;
; Make sure that the counters are not near overflow so that we
; avoid interrupts being blocked in anticipation of an overflow interrupt.
; Rather than checking the whole count, just look at the most significant bit.
; Zap low nibble of count if most significant bit of count is set and
; we were asked to disable that counter.
;
	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_mfpr	p4, EV6__PCTR_CTL			; (4,0L) get PCTR_CTL

	bis	r31, #^xF, p5				; get an ^xF
	srl	p4, #<EV6__PCTR_CTL__PCTR0__S+19>, p6 	; pctr0 msbit to lsbit
	cmovlbc	p6, r31, p5				; zap mask if lsbit=0
	and	r17, #1, p6				; check for disable
	cmoveq	p6, r31, p5				; zap mask if dis=0

	bis	r31, #^xF, p7				; get an ^xF
	srl	p4, #<EV6__PCTR_CTL__PCTR1__S+19>, p6 	; pctr1 msbit to lsbit
	cmovlbc	p6, r31, p7				; zap mask if lsbit=0
	and	r17, #2, p6				; check for disable
	cmoveq	p6, r31, p7				; zap mask if dis=0

	sll	p5, #EV6__PCTR_CTL__PCTR0__S,  p5	; shift mask into place
	sll	p7, #EV6__PCTR_CTL__PCTR1__S,  p7	; shift mask into place
	bis	p5, p7, p7				; or together
	bne	p7, call_pal__perfmon_zap		; zap if non-zero

	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>		; (4,0L) force retire

.if ne spinlock_hack					; 1.41
	br	r31, call_pal__perfmon_spin0
.endc							; 1,41

	hw_ret_stall (p23)				; return with stall

call_pal__perfmon_zap:
	bic	p4, p7, p4				; clear as needed
	hw_mtpr	p4, EV6__PCTR_CTL			; (4,0L) write PCTR_CTL
	bis	r31, r31, r31				; 1.42
	bis	r31, r31, r31				; 1.42
	bis	r31, r31, r31				; 1.42
	hw_mtpr p4, EV6__PCTR_CTL			; (4,0L) write again

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>		; (4,0L) force retire
;
; Spinlock workaround
; Perfmon disable function
;
; PT__PCTR_FLAG	<0> real pc0
;		<1> real spce
;		<2> real ppce
;
.if ne spinlock_hack				; 1.41

call_pal__perfmon_spin0:
	hw_ldq/p p7, PT__PCTR_FLAG(p_temp)	; get flag
	blbc	p7, call_pal__perfmon_spin0a	; br if already disabled
	and	r17, #1, p6			; check pc0 disable
	beq	p6, call_pal__perfmon_spin0a	; br if not disabling
;
; Going from enable to disable.
;
	bic	p7, #1, p7			; clear pc0 enable flag
	hw_stq/p p7, PT__PCTR_FLAG(p_temp)	; save it away
;
; If spce or ppce was 'on', save counter away, but then re-enable.
;
	beq	p7, call_pal__perfmon_spin0b	; br if spce/ppce both 'off'
	hw_mfpr p5, EV6__PCTR_CTL		; (4,0L) get PCTR
	hw_stq/p p5, PT__PCTR_SAVE(p_temp)	; save it away
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now re-enable pc0.
;
call_pal__perfmon_spin0b:
	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get I_CTL
	bis	r31, #1, p5			; get a 1
	sll	p5, #EV6__I_CTL__PCT0_EN__S, p5	; re-enable
	bis	r31, #1, p7			; get a 1
	sll	p7, #EV6__I_CTL__SPCE__S, p7	; spce bit
	bis	p5, p7, p5			; or them together
	bis	p4, p5, p4			; or them into I_CTL
	hw_mtpr	p4, EV6__I_CTL			; (4,0L) write I_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire

call_pal__perfmon_spin0a:
	hw_ret_stall (p23)			; return with stall
	
.endc						; 1.41
	hw_ret_stall (p23)				; return with stall
;
; Enable
;	r17(a1) = bitmask	r17(a1)<0>=1	enable CTR0
;				r17(a1)<1>=1	enable CTR1
;
;	Action: Set PCTx_EN in I_CTL as directed. If CTRx is already enabled,
;		it stays enabled.
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_en:

;
; Spinlock workaround
; Perfmon enable function
;
; PT__PCTR_FLAG	<0> real pc0
;		<1> real spce
;		<2> real ppce
;
.if ne spinlock_hack				; 1.41

	blbc	r17, call_pal__perfmon_spin2	; branch if not enabling pc0	
	hw_ldq/p p7, PT__PCTR_FLAG(p_temp)	; get flag
	blbs	p7, call_pal__perfmon_spin2	; branch if already enabled
	bis	p7, #1, p6			; set pc0 enable flag
	hw_stq/p p6, PT__PCTR_FLAG(p_temp)	; save it away
;
; If spce or ppce was 'on', restore the counter.
;
	beq	p7, call_pal__perfmon_spin2	; br if spce/ppce both 'off'
	hw_ldq/p p7, PT__PCTR_SAVE(p_temp)	; get saved PCTR
	GET_32CONS p5, <^xFFFFF>, r31		; get mask for count field
	sll	p5, #EV6__PCTR_CTL__PCTR0__S, p5; mask for pctr0
	and	p7, p5, p7			; clean pctr0

	hw_mfpr	p4, EV6__PCTR_CTL		; (4,0L) get PCTR_CTL
	bic	p4, p5, p4			; clean old pctr0
	bis	p4, p7, p4			; new pctr0
	bis	r31, r31, r31			; nop
	hw_mtpr	p4, EV6__PCTR_CTL		; (4,0L) write PCTR_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

call_pal__perfmon_spin2:
	
.endc						; 1.41

 	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get current control
	and	r17, #3, p5			; clean input
	sll	p5, #EV6__I_CTL__PCT0_EN__S, p5	; shift into position
	bis	p4, p5, p4			; enable bits as requested
	hw_mtpr	p4, EV6__I_CTL			; (4,0L)

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire

	hw_ret_stall (p23)			; return with stall
;
; Mux select
;	r17(a1) = pctr_ctl	r17(a1)<3:0>	SL1
;				r17(a1)<4>	SL0
;					0 => cycles
;					1 => retired instructions
;	Action: Write PCTR_CTL SLx fields.
;
ASSUME EV6__PCTR_CTL__SL1__S eq 2
ASSUME EV6__PCTR_CTL__SL0__S eq 4

	ALIGN_CACHE_BLOCK
call_pal__perfmon_mux:
	hw_mfpr	p4, EV6__PCTR_CTL		; (4,0L) get current control
	bis	r31, #^x1F, p6			; cleaning mask
	and	r17, #^x1F, p5			; clean input
	bic	p4, p6, p4			; clean mux
	bis	p4, p5, p4			; or in new mux values
	hw_mtpr	p4, EV6__PCTR_CTL		; (4,0L)

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire

	hw_ret_stall (p23)			; return with stall
;
; Options
;	r17(a1) = options	r17(a1)<0>	logging option
;					0 => log all processes
;					1 => log only selected processes
;	Action: If r17(a1)<0> is clear, set SPCE bit in I_CTL
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_opt:
	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get I_CTL
	bis	r31, #1, p5			; get a 1
	sll	p5, #EV6__I_CTL__SPCE__S, p5	; get into spce position
	bic	p4, p5, p4			; clear spce
	cmovlbs	r17, r31, p5			; if lbs, just process
	bis	p4, p5, p4			; or new spce in
	hw_mtpr	p4, EV6__I_CTL			; (4,0L) write I_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire
;
; Spinlock workaround.
; Options.
;
; PT__PCTR_FLAG	<0> real pc0
;		<1> real spce
;		<2> real ppce
;
; r17<0>=0 => all
; r17<1>=1 => selected
;
.if ne spinlock_hack				; 1.41

	hw_ldq/p p7, PT__PCTR_FLAG(p_temp)	; get flag
	blbs	r17, call_pal__perfmon_spin3a	; branch for spce turning 'off'
;
; Turn SPCE 'on'. See if it was already 'on'.
;
	and	p7, #2, p4			; check previous status
	beq	p4, call_pal__perfmon_spin3c	; if change, more stuff to do
	hw_ret_stall (p23)			; just return
;
; Going from SPCE 'off' to SPCE 'on'.
; 
;
call_pal__perfmon_spin3c:
	bis	p7, #2, p6			; turn 'on'
	hw_stq/p p6, PT__PCTR_FLAG(p_temp)	; save that
;
; If ppce 'off' and pc0 'off', do nothing more.
; If ppce 'off' and pc0 'on', restore the counter.
; If ppce 'on' and pc0 'off', do nothing more.
; If ppce 'on' and pc0 'on', do nothing more.
;
	cmpeq	p7, #1, p4			; 001 = ppce 'off', pc0 'on'
	beq	p4, call_pal__perfmon_spin3d	; branch if not that case

	hw_ldq/p p7, PT__PCTR_SAVE(p_temp)	; get saved PCTR
	GET_32CONS p5, <^xFFFFF>, r31		; get mask for count field
	sll	p5, #EV6__PCTR_CTL__PCTR0__S, p5; mask for pctr0
	and	p7, p5, p7			; clean pctr0

	hw_mfpr	p4, EV6__PCTR_CTL		; (4,0L) get PCTR_CTL
	bic	p4, p5, p4			; clean old pctr0
	bis	p4, p7, p4			; new pctr0
	bis	r31, r31, r31			; nop
	hw_mtpr	p4, EV6__PCTR_CTL		; (4,0L) write PCTR_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

call_pal__perfmon_spin3d:
	hw_ret_stall(p23)			; return
;
; Turn SPCE 'off'. See it it was already 'off'.
;
; p7 = flag
; PT__PCTR_FLAG	<0> real pc0
;		<1> real spce
;		<2> real ppce
;
call_pal__perfmon_spin3a:			; spce is being turned 'off'
	and	p7, #2, p4			; check previous status
	beq	p4, call_pal__perfmon_spin3g	; br if already 'off'
;
; Going from SPCE 'on' to SPCE 'off'.
;
	bic	p7, #2, p7			; turn 'off'
	hw_stq/p p7, PT__PCTR_FLAG(p_temp)	; save that
;
; If ppce 'off' and pc0 'off', turn spce back on.
; If ppce 'off' and pc0 'on', save the counter, and turn spce back on.
; If ppce 'on' and pc0 'on', do nothing more.
; If ppce 'on' and pc0 'off', turn spce back on.
;
	beq	p7, call_pal__perfmon_spin3f	; br if ppce 'off', pc0 'off'
	cmpeq	p7, #4, p4			; next case
	bne	p4, call_pal__perfmon_spin3f	; br if ppce 'on', pc0 'off'
	cmpeq	p7, #1, p4			; check next case
	bne	p4, call_pal__perfmon_spin3e	; br if ppce 'off', pc0 'on'
	hw_ret_stall (p23)			; return otherwise

call_pal__perfmon_spin3e:
	hw_mfpr p5, EV6__PCTR_CTL		; (4,0L) get PCTR
	hw_stq/p p5, PT__PCTR_SAVE(p_temp)	; save it away
	bis	r31, r31, r31
	bis	r31, r31, r31

call_pal__perfmon_spin3f:			; turn spce on
	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get I_CTL
	bis	r31, #1, p5			; get a 1
	sll	p5, #EV6__I_CTL__SPCE__S, p5	; get into spce position
	bis	p4, p5, p4			; or new spce in
	hw_mtpr	p4, EV6__I_CTL			; (4,0L) write I_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire
	hw_ret_stall (p23)			; return with stall
;
; Going from SPCE 'off' to SPCE 'off'.
;
; If ppce 'off' and pc0 'off', turn spce back on.
; If ppce 'off' and pc0 'on', turn spce back on.
; If ppce 'on' and pc0 'on', do nothing more.
; If ppce 'on' and pc0 'off', turn spce back on.
;
; p7 = flag
;
call_pal__perfmon_spin3g:
	cmpeq	p7, #5, p4			; check ppce 'on', pc0 'on'
	beq	p4, call_pal__perfmon_spin3f	; if not, turn spce back on
	hw_ret_stall (p23)			; otherwise, all done

.endc						; 1.41

	hw_ret_stall (p23)			; return with stall
;
; Read counters
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_rd:
	hw_mfpr	r0, EV6__PCTR_CTL		; (4,0L) get PCTR_CTL
;
; Spinlock workaround
; Perfmon read function
;
; PT__PCTR_FLAG	<0> real pc0
;		<1> real spce
;		<2> real ppce
;
; If (ppce 'on' OR spce 'on') AND pc0 'on' get pctr0 from pctr_ctl,
;	otherwise from save location.
;

.if ne spinlock_hack				; 1.41

	hw_ldq/p p7, PT__PCTR_FLAG(p_temp)	; get flag
	cmpeq	p7, #3, p4			; check spce 'on', pc0 'on'
	bne	p4, call_pal__perfmon_spin5	; if so, all done
	cmpeq	p7, #5, p4			; check ppce 'on', pc0 'on'
	bne	p4, call_pal__perfmon_spin5	; if so, all done
	cmpeq	p7, #7, p4			; check all 'on'
	bne	p4, call_pal__perfmon_spin5	; if so, all done

	hw_ldq/p p7, PT__PCTR_SAVE(p_temp)	; get saved PCTR
	GET_32CONS p5, <^xFFFFF>, r31		; get mask for count field
	sll	p5, #EV6__PCTR_CTL__PCTR0__S, p5; mask for pctr0
	and	p7, p5, p7			; clean pctr0

	hw_mfpr	p4, EV6__PCTR_CTL		; (4,0L) get PCTR_CTL
	bic	p4, p5, p4			; clean old pctr0
	bis	p4, p7, r0			; new pctr0

call_pal__perfmon_spin5:
	hw_ret	(p23)				; return
                                                                            
.endc						; 1.41

	hw_ret	(p23)				; just return, no changes
;
; Write counters selectively
;	r17(a1)			r17(a1)<0> Write counter 0
;				r17(a1)<1> Write counter 1
;				r17(a1)<25:6>  new ctr1 value
;				r17(a1)<47:28> new ctr0 value
;	Action: Selectively write counter fields in PCTR_CTL
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_wr:
	hw_mfpr	p4, EV6__PCTR_CTL		; (4,0L) get PCTR_CTL
	GET_32CONS p5, <^xFFFFF>, r31		; get mask for count field

	blbc	r17, call_pal__perfmon_wr_1	; if clear, don't write 0
	sll	p5, #EV6__PCTR_CTL__PCTR0__S, p6; mask for pctr0
	and	r17, p6, p7			; new pctr0
	bic	p4, p6, p4			; clear old pctr0
	bis	p4, p7, p4			; or in new pctr0
;
; Spinlock workaround
; Perfmon write function
;
; Just write to save location.
;

.if ne spinlock_hack				; 1.41
	hw_stq/p p4, PT__PCTR_SAVE(p_temp)	; save away pctr0 value
.endc						; 1.41

call_pal__perfmon_wr_1:
	srl	r17, #1, p6			; now look at pctr1
	blbc	p6, call_pal__perfmon_wr_ipr	; if clear, go on to write
	sll	p5, #EV6__PCTR_CTL__PCTR1__S, p6; mask for pctr1
	and	r17, p6, p7			; new pctr1
	bic	p4, p6, p4			; clear old pctr1
	bis	p4, p7, p4			; or in new pctr1

call_pal__perfmon_wr_ipr:
;
; To make counter accurate, we no longer zap the lower nibble. But the user
; must be careful with the values! If the counter is near overflow and
; counting is disabled, interrupts can be blocked in anticipation of an
; overflow interrupt.
;
	hw_mtpr	p4, EV6__PCTR_CTL		; (4,0L) write PCTR_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire

	hw_ret_stall (p23)			; return with stall
;
; Enable and write specified counters
;	r17(a1) = bitmask	r17(a1)<0>=1	enable CTR0
;				r17(a1)<1>=1	enable CTR1
;
;	Action: Write specified counter(s)
;		Set PCTx_EN in I_CTL as directed
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_en_wr:
 	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get current control
	and	r17, #3, p5			; clean input
	sll	p5, #EV6__I_CTL__PCT0_EN__S, p5	; shift into position
	bis	p4, p5, p4			; enable bits as requested
	hw_mtpr	p4, EV6__I_CTL			; (4,0L)
;
; Spinlock workaround.
;
; PT__PCTR_FLAG	<0> real pc0
;		<1> real spce
;		<2> real ppce
;
.if ne spinlock_hack				; 1.41

	blbc	r17, call_pal__perfmon_spin4	; branch if not enabling pc0
	hw_ldq/p p7, PT__PCTR_FLAG(p_temp)	; get flag
	blbs	p7, call_pal__perfmon_spin4	; branch if already enabled
	bis	p7, #1, p7			; set pc0 enable flag
	hw_stq/p p7, PT__PCTR_FLAG(p_temp)	; save it away

call_pal__perfmon_spin4:

.endc						; 1.41

	br	r31, call_pal__perfmon_wr	; write selected counters
;
; Read I_STAT
;
EV6__I_STAT__PRFME__S = ^x1e	
EV6__I_STAT__PRFME__V = ^xb	
EV6__I_STAT__PRFME__M = ^x7FF

	ALIGN_CACHE_BLOCK

call_pal__perfmon_rd_istat:
	hw_mfpr	r0, EV6__I_STAT			; (4,0L) get I_STAT
	srl	r0, #EV6__I_STAT__PRFME__S, r0	; clean <29:0>
	lda	p4, EV6__I_STAT__PRFME__M(r31)	; mask for cleaning
	and	p4, r0, r0			; mask
	sll	r0, #EV6__I_STAT__PRFME__S,r0	; shift back
	hw_ret	(p23)				; just return, no changes
;
; Read pmpc
;
call_pal__perfmon_rd_pmpc:
	hw_mfpr	r0, EV6__PMPC			; get PMPC
	bic	r0, #2, r0			; clear RAZ bit to be neat
	hw_ret	(p23)				; just return, no changes
;
; Unknown function
;
call_pal__perfmon_unknown:

	hw_ret	(p23)				; just return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_DATFX
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16<0>		new datfx bit
;
; Function:
;	DATFX <- r16<0>
;
; Exit state:
;	r16		unpredictable
;-
	START_CALL_PAL <MTPR_DATFX>

	hw_ldq/p p6, PT__PCBB(p_temp)		; get PCBB

	bis	r31, #1, p7			; get a 1
	sll	p7, #PCB__DAT__S, p7		; 1 in DAT position
	sll	r16, #PCB__DAT__S, r16		; shift value to DAT position

	hw_ldq/p p4, PCB__FEN(p6)		; read DAT/PME/FEN quadword

	bic	p4, p7, p4			; clear old DATFX
	bis	p4, r16, p4			; update DATFX

	hw_stq/p p4, PCB__FEN(p6)		; update PCB

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__003B
;
; Entry:
;	r16		0xBAC
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return in palmode. For FP emulation.
;-
.if ne ev6_p1

PAL_FUNC__003B = ^x3B

	START_CALL_PAL <003B>

	lda	p4, ^xBAC(r31)			; test for ^xBAC
	cmpeq	p4, r16, p4
	bis	r31, r31, r31
	bis	r31, r31, r31

	beq	p4, call_pal__003B_fail		; fail if not ^xBAC
	sll	p23, #22, p23
	srl	p23, #22, p23			; clean off kseg
	bis	p23, #1, p23			; or in pal-mode bit
	hw_ret	(p23)				; return in pal-mode

call_pal__003B_fail:
	lda	p4, SCB__OPCDEC(r31)		; scb
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb
	br	r31, trap__post_km		; post

	END_CALL_PAL
.endc


;+
; CALL_PAL__MFPR_WHAMI
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0 <- WHAMI
;
; Exit state:
;	r0		WHAMI
;
;-
	START_CALL_PAL <MFPR_WHAMI>

	hw_ldq/p r0, PT__WHAMI(p_temp)		; get whami
	NOP
	NOP
	NOP					; no hw_ret in 1st fetch block

	hw_ret	(p23)				; return

	END_CALL_PAL


;+
; CALL_PAL__BPT
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The BPT instruction is provided for program debugging.
;	Initiate breakpoint exception with new_mode=kernel. Switch to
;	kernel mode, push r2..r7, the update PC, and PS on the kernel stack.
;	Dispatch to the address in the Breakpoint SCB vector.
;
; Exit state:
;	On exit to trap__post_km
;	PT__FAULT_PC		nextpc
;	PT__FAULT_SCB		SCB vector
;-
	START_CALL_PAL <BPT>

	lda	p4, SCB__BPT(r31)		; SCB vector
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; save SCB vector
	br	r31, trap__post_km		; post the trap

	END_CALL_PAL

;+
; CALL_PAL__BUGCHK
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The BUGCHK instruction is provided for error reporting.
;	Initiate bugcheck exception with new_mode=kernel. Switch to
;	kernel mode, push r2..r7, the update PC, and PS on the kernel stack.
;	Dispatch to the address in the Bugcheck SCB vector.
;
; Exit state:
;	On exit to trap__post_km
;	PT__FAULT_PC		nextpc
;	PT__FAULT_SCB		SCB vector
;-
	START_CALL_PAL <BUGCHK>

	lda	p4, SCB__BUGCHK(r31)		; SCB vector
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; save SCB vector
	br	r31, trap__post_km		; post the trap

	END_CALL_PAL

;+
; CALL_PAL__CHME
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	tmp1 <- MINU(1,PS<CM>)
;	Initiate exception with new_mode=tmp1
;
; Exit state:
;	On exit to call_pal__post_xm
;	p5		old mode
;	p6		new mode
;	p20		1 => mode hasn't changed
;	p23		nextpc
;	PT__FAULT_PC	nextpc
;	PT__FAULT_SCB	SCB vector
;	PT__CH_MODE	old mode
;	PT__CH_SP	old sp
;-
	START_CALL_PAL <CHME>

	and	p_misc, #<3@P_MISC__CM__S>, p5	; current mode in p5<4:3>
	cmplt	p5, #PS__EXEC, p20		; is current < exec?
	bis	p5, r31, p6			; assume minu = current
	cmoveq	p20, #PS__EXEC, p6		; new_mode <- minu(1,current)

	hw_stq/p p5, PT__CH_MODE(p_temp)	; stash old mode for faults
	hw_stq/p r30, PT__CH_SP(p_temp)		; stash old sp for faults (??)

	lda	p4, SCB__CHME(r31)		; SCB vector
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; save SCB vector

	cmpeq	p6, p5, p20			; any change to mode?
	br	r31, call_pal__post_xm		; now post

	END_CALL_PAL

;+
; CALL_PAL__CHMK
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Initiate CHMK exception with new_mode=kernel.
;
; Exit state:
;	On exit to trap__post_km
;	PT__FAULT_PC		nextpc
;	PT__FAULT_SCB		SCB vector
;-
	START_CALL_PAL <CHMK>

	lda	p4, SCB__CHMK(r31)		; SCB vector
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; save SCB vector
	br	r31, trap__post_km		; post the trap

	END_CALL_PAL

;+
; CALL_PAL__CHMS
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	tmp1 <- MINU(2,PS<CM>)
;	Initiate exception with new_mode=tmp1
;
; Exit state:
;	On exit to call_pal__post_xm:
;	p5		old mode
;	p6		new mode
;	p20		1 => mode hasn't changed
;	p23		nextpc
;	PT__FAULT_PC	nextpc
;	PT__FAULT_SCB	SCB vector
;	PT__CH_MODE	old mode
;	PT__CH_SP	old sp
;-
	START_CALL_PAL <CHMS>

	and	p_misc, #<3@P_MISC__CM__S>, p5	; current mode in p5<4:3>
	cmplt	p5, #PS__SUPR, p20		; is current < supr?
	bis	p5, r31, p6			; assume minu = current
	cmoveq	p20, #PS__SUPR, p6		; new_mode <- minu(2,current)

	hw_stq/p p5, PT__CH_MODE(p_temp)	; stash old mode for faults
	hw_stq/p r30, PT__CH_SP(p_temp)		; stash old sp for faults (??)

	lda	p4, SCB__CHMS(r31)		; SCB vector
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; save SCB vector

	cmpeq	p6, p5, p20			; any change to mode?
	br	r31, call_pal__post_xm		; now post

	END_CALL_PAL

;+
; CALL_PAL__CHMU
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Initiate exception with new_mode=PS<CM>. This instruction is
;	provided for VAX compatibility only.
;
; Exit state:
;	PT__FAULT_PC	nextpc
;	PT__FAULT_SCB	SCB vector
;	PT__CH_MODE	old mode
;	PT__CH_SP	old sp
;-
	START_CALL_PAL <CHMU>

	and	p_misc, #<3@P_MISC__CM__S>, p5	; current mode in p5<4:3>

	hw_stq/p p5, PT__CH_MODE(p_temp)	; stash old mode for faults
	hw_stq/p r30, PT__CH_SP(p_temp)		; stash old sp for faults (??)

	lda	p4, SCB__CHMU(r31)		; SCB vector
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; save SCB vector

	br	r31, call_pal__post_xm_cont	; now post

	END_CALL_PAL

;+
; CALL_PAL__IMB
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Make instruction stream coherent with data stream. Does not
;	guarantee other processors see a modification of the instruction
;	stream.
;-
	START_CALL_PAL <IMB>

	MB					; no hw_ret in 1st fetch block
	NOP
	NOP
	NOP

	hw_mtpr	r31, EV6__IC_FLUSH		; (4,0L) flush the icache
	bne	r31, .				; pvc #24 -- separate flush and stall
	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__INSQHIL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of header
;	r17		contains address of entry
;
; Function:
;	If the secondary interlock is clear, INSQHIL inserts the entry
;	specified in r17 into the long self-relative queue following the header
;	specified in r16.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;	r2		saved and used as scratch
;	r3		saved and used as scratch
;	r8		saved and used as scratch
;	r9		saved and used as scratch
;
; Exit state:
;	r0	-1	failed to get secondary interlock
;		 0	queue was not empty
;		 1	queue was empty
;-
	START_CALL_PAL <INSQHIL>

.iif ndf queue_retry_count, queue_retry_count = 32

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	bis	r16, r17, p4			; merge to check alignment
	and	p4, #^x7, p4			; check H and E
	bne	p4, call_pal__queue_addr_error	; branch if not quad aligned

	xor	r16, r17, p4			; check H=E
	beq	p4, call_pal__queue_addr_error	; branch if H=E

	addl	r16, r31, p4			; sext H
	xor	r16, p4, p4			; check H = sext H
	addl	r17, r31, p5			; sext E
	xor	r17, p5, p5			; check E = sext E
	bne	p4, call_pal__queue_addr_error	; branch if bad sext H
	bne	p5, call_pal__queue_addr_error	; branch if bad sext E

	CONT_CALL_PAL <INSQHIL>

	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

queue_setup_lock_offset = <<call_pal__queue_setup_fault_lock - trap__pal_base>>

	ldah	p4,<<queue_setup_lock_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_lock_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	bis	r31, r31, r2			; flag that we don't have lock
	lda	p20, queue_retry_count(r31)	; set retry count
;
; Now try to acquire the lock
;
call_pal__insqhil_lock:
	ldl_l	r0, (r16)			; try to get H, interlocked
	blbs	r0, call_pal__queue_busy	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stl_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__insqhil_retry	; retry on failure

	mb					; per SRM

	bis	r2, #1, r2			; flag that we have the lock
;
; Now check access and quad alignment.
; Current state:
;	r0	S-H
;	r2	1, lock acquired
;	r16	H
;	r17	E
;
	hw_ldl/w r31, 0(r17)				; check E access
	and	r0, #^x7, p4				; check S alignment
	addq	r16, r0, r9				; get S address
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment
	hw_ldl/w r31, 0(r9)				; check S access

;
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
queue_fault_s_offset = <<call_pal__queue_fault_s - trap__pal_base>>

	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
;
; Current state:
;	r0	S-H
;	r2	1 => lock acquired
;	r9	S
;	r16	H
;	r17	E
;
	subl	r16, r17, r3			; H-E
	stl	r3, 4(r17)			; (E+4)	<- H-E
	addl	r3, r0, r3			; S-E
	stl	r3, (r17)			; (E)	<- S-E
	subl	r31, r3, r3			; E-S
	stl	r3, 4(r9)			; (S+4)	<- E-S

	mb

	subl	r17, r16, r3			; E-H
	stl	r3, (r16)			; (H)	<- E-H, clear secondary

	cmpeq	r0, r31, r0			; if S-H=0 => queue was empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

;+
; Check retry count and try again if not zero.
;-
call_pal__insqhil_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__insqhil_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__INSQTIL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of header
;	r17		contains address of entry
;
; Function:
;	If the secondary interlock is clear, INSQTIL inserts the entry
;	specified in r17 into the quad long-relative queue preceding the header
;	specified in r16.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;	r2		saved and used as scratch
;	r3		saved and used as scratch
;	r8		saved and used as scratch
;	r9		saved and used as scratch
;
; Exit state:
;	r0	-1	failed to get secondary interlock
;		 0	queue was not empty
;		 1	queue was empty
;-
	START_CALL_PAL <INSQTIL>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	bis	r16, r17, p4			; merge to check alignment
	and	p4, #^x7, p4			; check H and E
	bne	p4, call_pal__queue_addr_error	; branch if not quad-aligned

	xor	r16, r17, p4			; check H=E
	beq	p4, call_pal__queue_addr_error	; branch if H=E

	addl	r16, r31, p4			; sext H
	xor	r16, p4, p4			; check H = sext H
	addl	r17, r31, p5			; sext E
	xor	r17, p5, p5			; check E = sext E
	bne	p4, call_pal__queue_addr_error	; branch if bad sext H
	bne	p5, call_pal__queue_addr_error	; branch if bad sext E

	CONT_CALL_PAL <INSQTIL>
	
	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

	ldah	p4,<<queue_setup_lock_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_lock_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	bis	r31, r31, r2			; flag that we don't have lock
	lda	p20, queue_retry_count(r31)	; set retry count
;
; Note: We do a ldq_l to get the backward link as well as the lock.
;
call_pal__insqtil_lock:
	ldq_l	r0, (r16)			; try to get H, interlocked
	blbs	r0, call_pal__queue_busy	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stl_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__insqtil_retry	; retry on failure

	mb					; per SRM

	bis	r2, #1, r2			; flag that we have the lock
;
; Current state:
;	r0<31:0> 	S-H
;	r0<63:32>	P-H
;	r2		1, lock acquired
;	r16		H
;	r17		E
;
	sra	r0, #32, r9				; P-H
	bis	r0, r9, p4				; combine for S/P check
	and	p4, #^x7, p4				; check S/P alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment

	hw_ldl/w r31, 0(r17)				; check E access
	addl	r16, r9, r9				; P
	beq	r0, call_pal__insqtil_was_empty		; branch on queue empty
	hw_ldl/w r31, 0(r9)				; check P access
;
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
;
; Current state:
;	r0<31:0> 	S-H
;	r0<63:32>	P-H
;	r2		1, lock acquired
;	r9		P
;	r16		H
;	r17		E
;
	subl	r16, r17, r3			; H-E
	stl	r3, (r17)			; (E)	<- H-E
	subl	r9, r17, r3			; P-E
	stl	r3, 4(r17)			; (E+4)	<- P-E
	subl	r31, r3, r3			; E-P
	stl	r3, (r9)			; (P)	<- E-P
	subl	r17, r16, r3			; E-H
	stl	r3, 4(r16)			; (H+4)	<- E-H

	mb

	stl	r0, (r16)			; (H)	<- S-H, clear secondary

	bis	r31, #0, r0			; queue was not empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;
; Flow for queue was empty.
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
call_pal__insqtil_was_empty:

	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
;
; Current state:
;	r0<31:0> 	S-H = 0
;	r0<63:32>	P-H = 0
;	r2		1, lock acquired
;	r9		P
;	r16		H
;	r17		E
;
	subl	r16, r17, r3			; H-E
	stl	r3, (r17)			; (E)	<- H-E	
	stl	r3, 4(r17)			; (E+4)	<- H-E
	subl	r17, r16, r3			; E-H
	stl	r3, 4(16)			; (H+4)	<- E-H

	mb

	stl	r3, (r16)			; (H)	<- E-H, clear secondary

	bis	r31, #1, r0			; queue was empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

;+
; Check retry count and try again if not zero.
;-
call_pal__insqtil_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__insqtil_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__INSQHIQ
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of header
;	r17		contains address of entry
;
; Function:
;	If the secondary interlock is clear, INSQHIQ inserts the entry
;	specified in r17 into the quad self-relative queue following the header
;	specified in r16.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;	r2		saved and used as scratch
;	r3		saved and used as scratch
;	r8		saved and used as scratch
;	r9		saved and used as scratch
;
; Exit state:
;	r0	-1	failed to get secondary interlock
;		 0	queue was not empty
;		 1	queue was empty
;-
	START_CALL_PAL <INSQHIQ>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	bis	r16, r17, p4			; merge to check alignment
	and	p4, #^xF, p4			; check H and E
	bne	p4, call_pal__queue_addr_error	; branch if not octa-aligned

	xor	r16, r17, p4			; check H=E
	beq	p4, call_pal__queue_addr_error	; branch if H=E
	
	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

queue_setup_lock_offset = <<call_pal__queue_setup_fault_lock - trap__pal_base>>

	ldah	p4,<<queue_setup_lock_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_lock_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	bis	r31, r31, r2			; flag that we don't have lock
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <INSQHIQ>
;
; Now try to acquire the lock
;
call_pal__insqhiq_lock:
	ldq_l	r0, (r16)			; try to get H, interlocked
	blbs	r0, call_pal__queue_busy	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stq_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__insqhiq_retry	; retry on failure

	mb					; per SRM

	bis	r2, #1, r2			; flag that we have the lock
;
; Now check access.
; Current state:
;	r0	S-H
;	r2	1, lock acquired
;	r16	H
;	r17	E
;
	hw_ldq/w r31, 0(r17)				; check E access
	and	r0, #^xF, p4				; check S alignment
	addq	r16, r0, r9				; get S address
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment
	hw_ldq/w r31, 0(r9)				; check S access
;
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
;
; Current state:
;	r0	S-H
;	r2	1 => lock acquired
;	r9	S
;	r16	H
;	r17	E
;
	subq	r16, r17, r3			; H-E
	stq	r3, 8(r17)			; (E+8)	<- H-E
	addq	r3, r0, r3			; S-E
	stq	r3, (17)			; (E)	<- S-E
	subq	r31, r3, r3			; E-S
	stq	r3, 8(r9)			; (S+8)	<- E-S

	mb

	subq	r17, r16, r3			; E-H
	stq	r3, (r16)			; (H)	<- E-H, clear secondary

	cmpeq	r0, r31, r0			; if S-H=0 => queue was empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

;+
; Check retry count and try again if not zero.
;-
call_pal__insqhiq_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__insqhiq_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

;+
; Queue busy. Enter for secondary lock set and for retry exceeded.
; Clear lock for secondary lock failure case, and return r0 = -1 to user.
; Also restore saved registers.
;-
call_pal__queue_busy:
	stl_c	r0, (r16)			; release primary lock

	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get pc back
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher

	hw_ldq/p r2, PT__R2(p_temp)		; restore registers
	hw_ldq/p r3, PT__R3(p_temp)
	hw_ldq/p r8, PT__R8(p_temp)
	hw_ldq/p r9, PT__R9(p_temp)

	subq	r31, #1, r0			; return with -1
	hw_ret	(p23)				; return

;+
; Queue addressing error. Clear the lock, restore scratch registers 
; and take an illegal operand trap.
;-
call_pal__queue_addr_error_unlock:
	stl	r0, (r16)			; release the lock

	hw_ldq/p r2, PT__R2(p_temp)		; restore registers
	hw_ldq/p r3, PT__R3(p_temp)
	hw_ldq/p r8, PT__R8(p_temp)
	hw_ldq/p r9, PT__R9(p_temp)

	br	r31, call_pal__queue_addr_error	; merge to take exception

;+
; call_pal__queue_setup_fault_lock
;
; Entered from pal_mm_dispatch.
; Fault during checking of queue operands, lock case. The memory operation
;	was either a read, read lock, store conditional, or a hw_ld with
;	write check.
; Exit to trap__post_km_r45.
; Current state:
;	p5			mm_stat
;	p6			va
;
;	r2			needs to be restored
;	r3			needs to be restored
;	r8			needs to be restored
;	r9			needs to be restored
;
;	r25			saved, needs to be restored
;	r26			saved, needs to be restored
;
;	PT__FAULT_SCB		SCB offset
;	PT__CALL_PAL_PC		pc+4 of callpal
;	PT__TRAP		trap handler
;	PT__R25			saved r25
;	PT__R26			saved r26
;
; Exit State:
;	PT__FAULT_PC		pc of callpal
;	PT__FAULT_SCB		SCB offset
;	PT__FAULT_R4		fault va
;	PT__FAULT_R5		mmf
;	PT__TRAP		cleared
;-
	PVC_JSR	pal_mm_dispatch, dest=1
call_pal__queue_setup_fault_lock:
	hw_ldq/p p4, PT__FAULT_SCB(p_temp)		; get scb offset back
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)		; get pc+4 of callpal

	hw_ldq/p r25, PT__R25(p_temp)			; restore r25
	hw_ldq/p r26, PT__R26(p_temp)			; restore r26
	hw_stq/p r31, PT__TRAP(p_temp)			; clear pt__trap

	cmpeq	p4, #SCB__FOR, p7			; p7=1 => FOR
	srl	p5, #EV6__MM_STAT__OPCODE__S, p20	; get opcode
	and	p20, #EV6__MM_STAT__OPCODE__M, p20	; clean opcode
	cmpeq	p20, #OPCODE__HW_LD, p20			; HW_LD => write

	bis	p5, p20, p5				; write, hw_ld => write
	and	p5, #1, p5				; isolate r/w bit

	bic	p5, p7, p5				; FOR => back to read
	sll	p5, #63, p5				; set up mmf
	hw_stq/p p5, PT__FAULT_R5(p_temp)		; save mmf

	subq	p23, #4, p23				; point to queue instr
	hw_stq/p p23, PT__FAULT_PC(p_temp)		; store fault pc
	hw_stq/p p6, PT__FAULT_R4(p_temp)		; store fault va

	bis	r2, r31, p4				; get lock indicator
	hw_ldq/p r2, PT__R2(p_temp)			; restore scratch
	hw_ldq/p r3, PT__R3(p_temp)			; restore scratch
	hw_ldq/p r8, PT__R8(p_temp)			; restore scratch
	hw_ldq/p r9, PT__R9(p_temp)			; restore scratch

	beq	p4, trap__post_km_r45
;
; Need to release the lock
;	r0	S-H
;	r16	H
;

queue_fault_offset = <<call_pal__queue_fault - trap__pal_base>>

	ldah	p4,<<queue_fault_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	stl	r0, (r16)			; clear secondary lock

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	br	r31, trap__post_km_r45		; now take the exception

	END_CALL_PAL

;+
; CALL_PAL__INSQTIQ
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of header
;	r17		contains address of entry
;
; Function:
;	If the secondary interlock is clear, INSQTIQ inserts the entry
;	specified in r17 into the quad self-relative queue preceding the header
;	specified in r16.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;	r2		saved and used as scratch
;	r3		saved and used as scratch
;	r8		saved and used as scratch
;	r9		saved and used as scratch
;
; Exit state:
;	r0	-1	failed to get secondary interlock
;		 0	queue was not empty
;		 1	queue was empty
;-
	START_CALL_PAL <INSQTIQ>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	bis	r16, r17, p4			; merge to check alignment
	and	p4, #^xF, p4			; check H and E
	bne	p4, call_pal__queue_addr_error	; branch if not octa-aligned

	xor	r16, r17, p4			; check H=E
	beq	p4, call_pal__queue_addr_error	; branch if H=E
	
	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

	ldah	p4,<<queue_setup_lock_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_lock_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	bis	r31, r31, r2			; flag that we don't have lock
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <INSQTIQ>

call_pal__insqtiq_lock:
	ldq_l	r0, (r16)			; try to get H, interlocked
	blbs	r0, call_pal__queue_busy	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stq_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__insqtiq_retry	; retry on failure

	mb					; per SRM

	bis	r2, #1, r2			; flag that we have the lock
;
; Current state:
;	r0	S-H
;	r2	1, lock acquired
;	r16	H
;	r17	E
;
	ldq	r9, 8(r16)				; get P-H
	bis	r0, r9, p4
	and	p4, #^xF, p4				; check S/P alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment

	hw_ldq/w r31, 0(r17)				; check E access
	addq	r16, r9, r9				; P
	beq	r0, call_pal__insqtiq_was_empty		; branch on queue empty
	hw_ldq/w r31, 0(r9)				; check P access
;
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
;
; Current state:
;	r0	S-H
;	r2	1, lock acquired
;	r9	P
;	r16	H
;	r17	E
;
	subq	r16, r17, r3			; H-E
	stq	r3, (r17)			; (E)	<- H-E
	subq	r9, r17, r3			; P-E
	stq	r3, 8(r17)			; (E+8)	<- P-E
	subq	r31, r3, r3			; E-P
	stq	r3, (r9)			; (P)	<- E-P
	subq	r17, r16, r3			; E-H
	stq	r3, 8(r16)			; (H+8)	<- E-H

	mb

	stq	r0, (r16)			; (H)	<- S-H, clear secondary

	bis	r31, #0, r0			; queue was not empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;
; Flow for queue was empty.
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
call_pal__insqtiq_was_empty:

	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
;
; Current state:
;	r0	S-H
;	r2	1, lock acquired
;	r9	P
;	r16	H
;	r17	E
;
	subq	r16, r17, r3			; H-E
	stq	r3, (r17)			; (E)	<- H-E	
	stq	r3, 8(r17)			; (E+8)	<- H-E
	subq	r17, r16, r3			; E-H
	stq	r3, 8(16)			; (H+8)	<- E-H

	mb

	stq	r3, (r16)			; (H)	<- E-H, clear secondary

	bis	r31, #1, r0			; queue was empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

;+
; Check retry count and try again if not zero.
;-
call_pal__insqtiq_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__insqtiq_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__INSQUEL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of the predecessor entry
;	r17		contains address of the new entry
;
; Function:
;	INSQUEL inserts the entry specified in r17 into the long absolute queue
;	following the entry specified by the predecessor addressed by r16.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Note:
;	Based on 3.5.1 of the SRM, we could conceivably get a 3rd level tnv
;	on a mp machine. In that case, we ignore the tnv and write the
;	otherwise good pte back into the tb and continue.
;
; Register use:
;	r2		saved and used as scratch
;	r3		saved and used as scratch
;	r8		saved and used as scratch
;	r9		saved and used as scratch
;
; Exit state:
;	r0	0	queue was not empty
;		1	queue was empty
;-
	START_CALL_PAL <INSQUEL>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	hw_stq/p r2, PT__R2(p_temp)		; get scratch registers
	hw_stq/p r3, PT__R3(p_temp)		; get scratch registers
	hw_stq/p r8, PT__R8(p_temp)		; get scratch registers
	hw_stq/p r9, PT__R9(p_temp)		; get scratch registers

queue_setup_s_offset = <<call_pal__queue_setup_fault_nolock_s - trap__pal_base>>

	ldah	p4,<<queue_setup_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	bis	r16, r31, r8			; r8 <- r16 for merge

	br	r31, call_pal__insqueld_merge_from_insquel

	END_CALL_PAL

;+
; CALL_PAL__INSQUEQ
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of the predecessor entry
;	r17		contains address of the new entry
;
; Function:
;	INSQUEQ inserts the entry specified in r17 into the quad absolute queue
;	following the entry specified by the predecessor addressed by r16.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed. Header and entries must be
;	OCTAWORD aligned. If not aligned, take an illegal operand exception.
;	Register r0 is UNPREDICTABLE if an exception occurs.
;
; Note:
;	Based on 3.5.1 of the SRM, we could conceivably get a 3rd level tnv
;	on a mp machine. In that case, we ignore the tnv and write the
;	otherwise good pte back into the tb and continue.
;
; Exit state:
;	r0	0	queue was not empty
;		1	queue was empty
;-
	START_CALL_PAL <INSQUEQ>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

queue_setup_offset = <<call_pal__queue_setup_fault_nolock - trap__pal_base>>

	ldah	p4,<<queue_setup_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	bis	r16, r31, r0			; r0 <- r16 for merge

	br	r31, call_pal__insqueqd_merge_from_insqueq

	END_CALL_PAL

;+
; CALL_PAL__INSQUELD
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of the address of the predecessor entry
;	r17		contains address of the new entry
;
; Function:
;	INSQUELD inserts the entry specified in r17 into the long absolute queue
;	following the entry specified by the predecessor. Register r16
;	contains the 32 bit address of the 32 bit address of the
;	predecssor.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Note:
;	Based on 3.5.1 of the SRM, we could conceivably get a 3rd level tnv
;	on a mp machine. In that case, we ignore the tnv and write the
;	otherwise good pte back into the tb and continue.
;
; Exit state:
;	r0	0	queue was not empty
;		1	queue was empty
;-
	START_CALL_PAL <INSQUELD>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	hw_stq/p r2, PT__R2(p_temp)		; get scratch registers
	hw_stq/p r3, PT__R3(p_temp)		; get scratch registers
	hw_stq/p r8, PT__R8(p_temp)		; get scratch registers
	hw_stq/p r9, PT__R9(p_temp)		; get scratch registers

	ldah	p4,<<queue_setup_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	ldq_u	r2, (r16)			; first half
	ldq_u	r3, 3(r16)			; second half
	extll	r2, r16, r2			; extract low
	extlh	r3, r16, r3			; extract high
	or	r2, r3, r2			; combine
	addl	r2, r31, r8			; sign extend to get addr of P

	CONT_CALL_PAL <INSQUELD>
;
; Probe the addresses. We do non-trapping hw_ld, so probe both ends.
; After probe of address of P, read P to get address of S.
;
call_pal__insqueld_merge_from_insquel:		; merge from insquel
	hw_ldl/w r31, 0(r8)			; probe first part of P
	hw_ldl/w r31, 3(r8)			; probe second part of P

	ldq_u	r2, (r8)			; first half
	ldq_u	r3, 3(8)			; second half
	extll	r2, r8, r2			; extract low
	extlh	r3, r8, r3			; extract high
	or	r2, r3, r2			; combine
	addl	r2, r31, r9			; sign extend to get S

	hw_ldl/w r31, 0(r17)			; probe first part of E flink
	hw_ldl/w r31, 7(r17)			; probe second part of E blink

	hw_ldl/w r31, 4(r9)			; probe first part of S blink
	hw_ldl/w r31, 7(r9)			; probe second part of S blink

;
; Probes have completed. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
;
; One shot at optimization -- If all 3 aligned, do as aligned.
; Current state:
;	r8	P
;	r17	E
;	r9	S
;
	bis	r8, r9, r2			; combine r8 and r9
	bis	r2, r17, r2			; combine r17
	and	r2, #^x3, r3			; check long alignment
	bne	r3, call_pal__insqueld_una	; do as unaligned

	stl	r9, (r17)			; (E) 	<- S
	stl	r8, 4(r17)			; (E+4)	<- P
	stl	r17, 4(r9)			; (S+4)	<- E
	stl	r17, (r8)			; (P)	<- E
	br	r31, call_pal__insqueld_done
;
; Do all as unaligned.
; Use macro with arguments reg, disp, addr, scr1, scr2, scr3
;
call_pal__insqueld_una:

	QSTORE_UNALIGNED_LONG r9, 0, r17, r2, r3, p20 	; (E) 	<- S

	QSTORE_UNALIGNED_LONG r8, 4, r17, r2, r3, p20	; (E+4) <- P

	QSTORE_UNALIGNED_LONG r17, 4, r9, r2, r3, p20	; (S+4)	<- E

	QSTORE_UNALIGNED_LONG r17, 0, r8, r2, r3, p20	; (P)	<- E
;
; Done. Clear catcher, write r0, restore scratch registers, and return.
;	r8	P
;	r17	E
;	r9	S
;
call_pal__insqueld_done:
	cmpeq	r9, r8, r0			; if S=P => queue was empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

;+
; call_pal__queue_setup_fault_nolock_s
;
; Entered from trap__pal_mm_dispatch.
; Fault during checking of queue operands, nolock case, scratch registers,
; The memory operation was either a read or a hw_ld with write check.
; The longword operation required scratch registers.
; There are scratch registers r2, r3, r8, r9 that need to be restored.
; Restore and merge with quadword case.
;
; Current state:
;	r2			needs to be restored
;	r3			needs to be restored
;	r8			needs to be restored
;	r9			needs to be restored
;-
	PVC_JSR	pal_mm_dispatch, dest=1
call_pal__queue_setup_fault_nolock_s:
	hw_ldq/p r2, PT__R2(p_temp)			; restore
	hw_ldq/p r3, PT__R3(p_temp)			; restore
	hw_ldq/p r8, PT__R8(p_temp)			; restore
	hw_ldq/p r9, PT__R9(p_temp)			; restore

	br	r31, call_pal__queue_setup_fault_nolock	; merge

;+
; call_pal__queue_fault_s
;
; A mm fault occurred while writing for queue. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; Otherwise we machine check.
;
; Current state:
;	p4		pte with <17:16>
;			^b00 => double
;			^b11 => invalid dpte
;			^b01 => dfault 
;	p5		mm_stat
;	p6		va
;	p23		fault pc (from PALcode flow)
;
;	r2		needs to be restored
;	r3		needs to be restored
;	r8		needs to be restored
;	r9		needs to be restored
;
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__FAULT_SCB	SCB offset
;	PT__CALL_PAL_PC	next pc
;
;	PT__R25		saved r25
;	PT__R26		saved r26
;-
	PVC_JSR	pal_mm_dispatch, dest=1
call_pal__queue_fault_s:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	srl	p4, #PTE__SOFT__S, p5		; get type of problem
	hw_ldq/p p7, PT__FAULT_SCB(p_temp)	; need to look at scb offset
	cmpeq	p7, #SCB__TNV, p7		; check for TNV
	and	p7, p5, p7			; TNV and 3rd level?
	blbc	p7, call_pal__queue_mchk_s	; take a machine if not
;
; This was a mulitprocessor case in which the valid bit has been cleared,
; but the pte is still valid. So write it into the TB.
;
	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p6, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p6, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	p4, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	p4, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

        hw_mtpr r31, <EV6__MM_STAT ! ^x80>      ; wait for pte write

	hw_ret	(p23)				; re-do the access

call_pal__queue_mchk_s:
	hw_ldq/p r2, PT__R2(p_temp)		; restore scratch
	hw_ldq/p r3, PT__R3(p_temp)		; restore scratch
	hw_ldq/p r8, PT__R8(p_temp)		; restore scratch
	hw_ldq/p r9, PT__R9(p_temp)		; restore scratch

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	br	r31, trap__pal_os_bugcheck	; take a mchk

	END_CALL_PAL

;+
; CALL_PAL__INSQUEQD
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of the address of the predecessor entry
;	r17		contains address of the new entry
;
; Function:
;	INSQUEQD inserts the entry specified in r17 into the absolute queue
;	following the entry specified by the predecessor, which is addressed
;	by the contents of the quadword addressed by r16.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed. Header and entries must be
;	OCTAWORD aligned. If not aligned, take an illegal operand exception.
;	Register r0 is UNPREDICTABLE if an exception occurs.
;
; Note:
;	Based on 3.5.1 of the SRM, we could conceivably get a 3rd level tnv
;	on a mp machine. In that case, we ignore the tnv and write the
;	otherwise good pte back into the tb and continue.
;
; Exit state:
;	r0	0	queue was not empty
;		1	queue was empty
;-
	START_CALL_PAL <INSQUEQD>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	ldah	p4,<<queue_setup_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	and	r16, #^xF, p20			; check r16 alignment
	bne	p20, call_pal__queue_addr_error	; branch if not octa-aligned
	ldq	r0, (r16)			; get P
;
; Validate P and E alignment.
;
call_pal__insqueqd_merge_from_insqueq:		; merge from insqueq
	bis	r0, r17, p20			; combine for alignment check
	and	p20, #^xF, p20			; check P/E alignment
	bne	p20, call_pal__queue_addr_error	; branch if not octa-aligned
;
; Probe P and E, fetching S and checking alignment of S.
;
	hw_ldq/w p20, 0(r0)			; probe P, fetch S
	hw_ldq/w r31, 0(r17)			; probe E

	and	p20, #^xF, p4			; check S alignment
	bne	p4, call_pal__queue_addr_error	; branch if not octa-aligned

	hw_ldq/w r31, 8(p20)			; probe S+8

	CONT_CALL_PAL <INSQUEQD>
;
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;

	ldah	p4,<<queue_fault_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	stq	p20, (r17)			; (E) 	<- S
	stq	r0, 8(r17)			; (E+8)	<- P
	stq	r17, 8(p20)			; (S+8)	<- E
	stq	r17, (r0)			; (P)	<- E

	cmpeq	p20, r0, r0			; if S=P => queue was empty

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

;+
; Something was not octa-aligned. Take an illegal operand
; exception via trap__post_km.
;
; Exit state:
;	PT__FAULT_PC	fault pc
;	PT__FAULT_SCB	scb offset
;-
call_pal__queue_addr_error:
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get pc back
	lda	p4, SCB__ILLOP(r31)		; illegal operand
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store fault pc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb offset
	br	r31, trap__post_km		; post

;+
; call_pal__queue_setup_fault_nolock
;
; Entered from trap__pal_mm_dispatch.
; Fault during checking of queue operands, nolock case. The memory operation
;	was either a read or a hw_ld with write check.
; Exit to trap__post_km_r45.
;
; Current state:
;	p5			mm_stat
;	p6			va
;
;	r25			saved, needs to be restored
;	r26			saved, needs to be restored
;
;	PT__FAULT_SCB		SCB offset
;	PT__CALL_PAL_PC		pc+4 of callpal
;	PT__TRAP		trap handler
;	PT__R25			saved r25
;	PT__R26			saved r26
;
; Exit State:
;	PT__FAULT_PC		pc of callpal
;	PT__FAULT_SCB		SCB offset
;	PT__FAULT_R4		fault va
;	PT__FAULT_R5		mmf
;	PT__TRAP		cleared
;-
	PVC_JSR	pal_mm_dispatch, dest=1
call_pal__queue_setup_fault_nolock:
	hw_ldq/p p4, PT__FAULT_SCB(p_temp)		; get scb offset back
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)		; get pc+4 of callpal

	hw_ldq/p r25, PT__R25(p_temp)			; restore r25
	hw_ldq/p r26, PT__R26(p_temp)			; restore r26
	hw_stq/p r31, PT__TRAP(p_temp)			; clear pt__trap

	cmpeq	p4, #SCB__FOR, p7			; p7=1 => FOR
	srl	p5, #EV6__MM_STAT__OPCODE__S, p20	; get opcode
	and	p20, #EV6__MM_STAT__OPCODE__M, p20	; clean opcode
	cmpeq	p20, #OPCODE__HW_LD, p20			; HW_LD => write

	bic	p20, p7, p20				; FOR => back to read
	sll	p20, #63, p20				; set up mmf
	hw_stq/p p20, PT__FAULT_R5(p_temp)		; save mmf

	subq	p23, #4, p23				; point to queue instr
	hw_stq/p p23, PT__FAULT_PC(p_temp)		; store fault pc
	hw_stq/p p6, PT__FAULT_R4(p_temp)		; store fault va
	br	r31, trap__post_km_r45			; post

;+
; call_pal__queue_fault
;
; A mm fault occurred while writing for queue. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; Otherwise we machine check.
;
; Current state:
;	p4		pte with <17:16>
;			^b00 => double
;			^b11 => invalid dpte
;			^b01 => dfault 
;	p5		mm_stat
;	p6		va
;	p23		fault pc (from PALcode flow)
;
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__FAULT_SCB	SCB offset
;	PT__CALL_PAL_PC	next pc
;
;	PT__R25		saved r25
;	PT__R26		saved r26
;-
	PVC_JSR	pal_mm_dispatch, dest=1
call_pal__queue_fault:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	srl	p4, #PTE__SOFT__S, p5		; get type of problem
	hw_ldq/p p7, PT__FAULT_SCB(p_temp)	; need to look at scb offset
	cmpeq	p7, #SCB__TNV, p7		; check for TNV
	and	p7, p5, p7			; TNV and 3rd level?
	blbc	p7, call_pal__queue_mchk	; take a machine if not
;
; This was a mulitprocessor case in which the valid bit has been cleared,
; but the pte is still valid. So write it into the TB.
;
	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p6, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p6, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	p4, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	p4, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

        hw_mtpr r31, <EV6__MM_STAT ! ^x80>      ; wait for pte write

	hw_ret	(p23)				; re-do the access

call_pal__queue_mchk:
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	br	r31, trap__pal_os_bugcheck	; take a mchk

	END_CALL_PAL

;+
; CALL_PAL__PROBER
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		base address
;	r17		signed offset
;	r18		access mode in <1:0>
;
; Function:
;	Check the read accessibility of the first and last byte specified by
;	the base address and the signed offset; the bytes in between are not
;	checked. If both bytes are accessible, PROBER returns the value 1 in
;	R0; otherwise PROBER returns a 0. The Fault on Read and Fault on Write
;	PTE bits are not checked. A TNV exception is signaled only if the
;	first or second level PTE is invalid.
;
;	The protection is checked against the less privileged of the modes
;	specified by R18<1:0> and the current mode.
;
; Register use:
;
; Exit state:
;	r0	1 if accessible, 0 if not
;
;-
	START_CALL_PAL <PROBER>

prober_offset = <call_pal__prober_alt_done - call_pal__prober_alt>

	and	r18, #3, p4				; clean access mode
	and	p_misc, #<3@P_MISC__CM__S>, p5		; current mode in <4:3>
	srl	p5, #P_MISC__CM__S, p5			; shift to <1:0>
	subq	p5, p4, p6				; current - specified
	cmovgt	p6, p5, p4				; use less privileged
	sll	p4, #EV6__DTB_ALT_MODE__MODE__S, p4	; shift into position

	CONT_CALL_PAL <PROBER>
;
; Load DTB_ALTMODE with the correct processor mode
;
	br	p7, call_pal__prober_alt
call_pal__prober_alt:
	addq	p7, #<prober_offset+1>, p7	; return past ret in palmode
	hw_mtpr	p4, EV6__DTB_ALT_MODE		; (6,0L) write mode
	bsr	r31, .				; push prediction stack
	PVC_JSR prober_alt
	hw_ret_stall (p7)			; pop prediction stack
	PVC_JSR prober_alt, dest=1

;
; Test the beginning and end bytes. We may take a miss, so we can
; only use p20 for scratch, and we save p23 away in PT__CALL_PAL_PC.
; If we have first or second level invalid PTE, the pal fault code
; will deal with this situation.
;
call_pal__prober_alt_done:
	addq	r16, r17, p20			; compute end byte address
	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p p4, PT__DTB_ALT_MODE(p_temp)	; save alt mode

call_pal__prober_ldl1:
	hw_ldl/a r31, 0(r16)			; test beginning byte
call_pal__prober_ldl2:
	hw_ldl/a r31, 0(p20)			; test end byte

	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get user pc back
	bis	r31, #1, r0			; indicate success

	hw_ret	(p23)				; return to user

	END_CALL_PAL

;+
; CALL_PAL__PROBEW
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		base address
;	r17		signed offset
;	r18		access mode
;
; Function:
;	Check the write accessibility of the first and last byte specified by
;	the base address and the signed offset; the bytes in between are not
;	checked. If both bytes are accessible, PROBEW returns the value 1 in
;	R0; otherwise PROBEW returns a 0. The Fault on Read and Fault on Write
;	PTE bits are not checked. A TNV exception is signaled only if the
;	first or second level PTE is invalid.
;
;	The protection is checked against the less privileged of the modes
;	specified by R18<1:0> and the current mode.
;
; Register use:
;
; Exit state:
;	r0	1 if accessible, 0 if not
;
;-
	START_CALL_PAL <PROBEW>

probew_offset = <call_pal__probew_alt_done - call_pal__probew_alt>

	and	r18, #3, p4				; clean access mode
	and	p_misc, #<3@P_MISC__CM__S>, p5		; current mode in <4:3>
	srl	p5, #P_MISC__CM__S, p5			; shift to <1:0>
	subq	p5, p4, p6				; current - specified
	cmovgt	p6, p5, p4				; use less privileged
	sll	p4, #EV6__DTB_ALT_MODE__MODE__S, p4	; shift into position

	CONT_CALL_PAL <PROBEW>
;
; Load DTB_ALTMODE with the correct processor mode
;
	br	p7, call_pal__probew_alt
call_pal__probew_alt:
	addq	p7, #<probew_offset+1>, p7	; return past ret in palmode
	hw_mtpr	p4, EV6__DTB_ALT_MODE		; (6,0L) write mode
	bsr	r31, .				; push prediction stack
	PVC_JSR probew_alt
	hw_ret_stall (p7)			; pop prediction stack
	PVC_JSR probew_alt, dest=1

;
; Test the beginning and end bytes. We may take a miss, so we can
; only use p20 for scratch, and we save p23 away in PT__CALL_PAL_PC.
; If we have first or second level invalid PTE, the pal fault code
; will deal with this situation.
;
call_pal__probew_alt_done:
	addq	r16, r17, p20			; compute end byte address
	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p p4, PT__DTB_ALT_MODE(p_temp)	; save alt mode

call_pal__probew_ldl1:
	hw_ldl/wa r31, 0(r16)			; test beginning byte
call_pal__probew_ldl2:
	hw_ldl/wa r31, 0(p20)			; test end byte

	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get user pc back
	bis	r31, #1, r0			; indicate success

	hw_ret	(p23)				; return to user

	END_CALL_PAL

;+
; CALL_PAL__RD_PS
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0 <- PS
;
; Exit state:
;	r0		PS
;-
	START_CALL_PAL <RD_PS>

	zapnot	p_misc, #^x3, r0		; just get PS
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__REI
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;
; Register use:
;
; Exit state:
;
;-
ASSUME EV6__PS__CM__S eq 3

	START_CALL_PAL <REI>

	hw_mtpr	r31, <EV6__MM_STAT ! ^xF0>	; 1.39 settle the tb

	and	r30, #^x3F, p6			; check stack alignment
	and	p_misc, #<3@P_MISC__CM__S>, p7	; get mode

	hw_stq/p p23, PT__FAULT_PC(p_temp)	; in case we fault

	bne	p6, call_pal__rei_stack		; branch on stack error
	bne	p7, call_pal__rei_from_nonkern	; branch on from nonkern

	CONT_CALL_PAL <REI>
;
; Do an rei from kernel.
;
; Touch the stack. Because it is naturally aligned, we can only trap on
; the first reference. A dtb miss is okay, a fault is fatal.
; Any load clears the lock.
;
call_pal__rei_ldq:
	ldq	p20, FRM__PS(r30)		; restore ps (could trap)
;
; Clear the interrupt flag.
;
	rc	r31				; clear interrupt flag
.if ne intercept_interrupt_return
	br	r31, call_pal__rei_intercept
call_pal__rei_intercept_return:
.endc
.if ne galaxy
	br	r31, call_pal__gal_intercept
call_pal__gal_intercept_return:
.endc
;
; Hack alert. We don't have visibility to r4-r7 because they are in our
; shadow range. So we need to peek under the covers.
;
; Current state:
;	r2	available
;	r3	available
;
; Turn off shadow mode. Do the loads. Then turn shadow mode back on.
;-
	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_mfpr	r3, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r3, #<1@EV6__I_CTL__SDE7__S>, r3; zap sde
	bis	r31, r31, r31			; separate blocks
	bis	r31, r31, r31			; separate blocks

	hw_mtpr	r3, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r3, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	ldq	r4, FRM__R4(r30)		; restore r4
	ldq	r5, FRM__R5(r30)		; restore r5
	ldq	r6, FRM__R6(r30)		; restore r6
	ldq	r7, FRM__R7(r30)		; restore r7

	bis	r3, #<1@EV6__I_CTL__SDE7__S>, r3; or in sde
	hw_mtpr	r3, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r3, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now back to business
;
	ldq	r2, FRM__R2(r30)		; restore r2
	ldq	r3, FRM__R3(r30)		; restore r3

	and	p20, #<3@PS__CM__S>, p4		; get new mode
	ldq	p23, FRM__PC(r30)		; get pc
	beq	p4, call_pal_rei__kern_to_kern	; branch on kern to kern
;
; Kernel to Non-Kernel. Assume new IPL=0.
; For now (??), we aren't optimizing ast/interrupts pending at IPL=0.
;
; Some platforms, such as those with tsunami, have a problem with the
; latency between clearing an interrupt and the interrupt being deasserted.
; Those platforms are opting to, on isum<device_irq> set, write to
; a tsunami csr to deassert the interrupt. A real interrupt will
; re-assert on the next polling loop.
;

.if ne	check_interrupt_pending
	hw_mfpr	p6, EV6__ISUM			; (0L) get ISUM
.endc

	hw_ldq/p p7, PT__PCBB(p_temp)		; get pcbb
	addq	p7, p4, p7			; new sp HWPCB offset

.if ne check_interrupt_pending
	xor	p6, p6, p5			; interlock isum and ps
	bis	p5, p4, p4			; interlock isum and ps
.endc

	hw_mtpr	p4, EV6__PS			; (4,0L) write new cm
	extbl	p20, #<<PS__SP_ALIGN__S>/8>, p5	; get ps<sp_align> bits
	zapnot	p20, #1, p20			; get ps<7:0> (ipl=0)
	zap	p_misc, #3, p_misc		; clear p_misc<15:0>
	bis	p_misc, p20, p_misc		; or in new ps

	hw_mfpr	p4, EV6__PAL_BASE		; (4,0L) get pal base
	bic	p23, #3, p23			; clean pc

	lda	p20, 64(r30)			; stack pointer beyond frame
	or	p20, p5, p20			; adjust by sp_align
	hw_ldq/p r30, PCB__KSP(p7)		; get new sp
	hw_stq/p p20, PT__KSP(p_temp)		; stash ksp

	lda	p4, ipl_offset(p4)		; pal base + table base
	hw_ldq/p p4, 0(p4)			; get new ier
	hw_mtpr p4, EV6__IER			; (4,0L) write new ier

.if ne	turbo_pcia_intr_fix
.if ne spinlock_hack
	hw_stq/p p4, ^x1f0(p_temp)		; save p4
.endc
	PVC_JSR	check_pcia, bsr=1
	bsr	p20, check_pcia_intr
.if ne spinlock_hack
	hw_ldq/p p4, ^x1f0(p_temp)		; restore p4
.endc
.endc

.if ne spinlock_hack				; 1.41

	hw_ldq/p p7, PT__PCTR_PEND(p_temp)	; check pending
	beq	p7, call_pal__rei_spin0		; 1.52 branch for none
	lda	p7, IPL__PERFMON(r31)		; ipl for perf counter
	subq	p7, p4, p7			; perfmon - current
	ble	p7, call_pal__rei_spin0		; branch if can't take now
;
; We can take the perfmon interrupt now because IPL is below the
; real PERFMON value.
;
	hw_ldq/p p6, PT__PCTR_R4(p_temp)	; get FAULT_R4 value
	hw_stq/p r31, PT__PCTR_PEND(p_temp)	; clear pending
	br	r31, sys__interrupt_pc_take_int	; now take interrupt

call_pal__rei_spin0:

.endc						; 1.41

.if eq	check_interrupt_pending
	hw_ret_stall (p23)			; rei to non-kern
.iff
	lda	p4, IRQ_DEV__M(r31)		; mask of IRQ(s) (IRQ1)
	sll	p4, #EV6__ISUM__EI__S, p4	; shift into position
	and	p6, p4, p4			; check for IRQ

	bne	p4, sys__deassert_interrupt	; go off to deassert
	hw_ret_stall (p23)			; otherwise finish the rei
.endc

;+
; Kernel to Kernel
;
; Current state:
;	p4		new mode (kernel)
;	p20		new ps
;	p23		new pc
;	r2 - r7 	restored
;
;	lock flag and interrupt flag both cleared
;
; Some platforms, such as those with tsunami, have a problem with the
; latency between clearing an interrupt and the interrupt being deasserted.
; Those platforms are opting to, on isum<device_irq> set, write to
; a tsunami csr to deassert the interrupt. A real interrupt will
; re-assert on the next polling loop.
;
;-
call_pal_rei__kern_to_kern:

.if ne	check_interrupt_pending
	hw_mfpr	p6, EV6__ISUM			; (0L) get ISUM
.endc

	extbl	p20, #<<PS__SP_ALIGN__S>/8>, p5	; get ps<sp_align> bits

	zapnot	p20, #3, p20			; get ps<15:0>
	zap	p_misc, #3, p_misc		; clear p_misc<15:0>
	extbl	p20, #<<PS__IPL__S>/8>, p7	; get IPL
	bis	p_misc, p20, p_misc		; or in new ps

	hw_mfpr	p4, EV6__PAL_BASE		; (4,0L) get pal base
	bic	p23, #3, p23			; clean pc

	lda	p20, 64(r30)			; stack pointer beyond frame
	or	p20, p5, r30			; adjust by sp_align

	s8addq	p7, p4, p4			; pal base + index
	lda	p4, ipl_offset(p4)		; pal base + table base + index
	hw_ldq/p p4, 0(p4)			; get new ier

.if ne	check_interrupt_pending
	xor	p6, p6, p5			; interlock isum and ier
	bis	p5, p4, p4			; interlock isum and ier
.endc

	hw_mtpr p4, EV6__IER			; (4,0L) write new ier

.if ne	turbo_pcia_intr_fix
.if ne spinlock_hack
	hw_stq/p p4, ^x1f0(p_temp)		; save p4
.endc
	PVC_JSR	check_pcia, bsr=1
	bsr	p20, check_pcia_intr
.if ne spinlock_hack
	hw_ldq/p p4, ^x1f0(p_temp)		; restore p4
.endc
.endc

.if ne spinlock_hack				; 1.41

	hw_ldq/p p7, PT__PCTR_PEND(p_temp)	; check pending
	beq	p7, call_pal__rei_spin1		; 1.52 branch for none
	lda	p7, IPL__PERFMON(r31)		; ipl for perf counter
	subq	p7, p4, p7			; perfmon - current
	ble	p7, call_pal__rei_spin1		; branch if can't take now
;
; We can take the perfmon interrupt now because IPL is below the
; real PERFMON value.
;
	hw_ldq/p p6, PT__PCTR_R4(p_temp)	; get FAULT_R4 value
	hw_stq/p r31, PT__PCTR_PEND(p_temp)	; clear pending
	br	r31, sys__interrupt_pc_take_int	; now take interrupt

call_pal__rei_spin1:

.endc						; 1.41

.if eq	check_interrupt_pending
	hw_ret_stall (p23)			; rei to non-kern
.iff
	lda	p4, IRQ_DEV__M(r31)		; mask of IRQ(s) (IRQ1)
	sll	p4, #EV6__ISUM__EI__S, p4	; shift into position
	and	p6, p4, p4			; check for IRQ

	bne	p4, sys__deassert_interrupt	; go off to deassert
	hw_ret_stall (p23)			; otherwise finish the rei
.endc

;+
; REI from non kernel.
;
; Stack alignment has been checked.
; Touch the stack. We can miss or fault.
; Any load clears the lock.
;- 
call_pal__rei_from_nonkern:
call_pal__rei_ldq_from_nonkern:
	ldq	p20, FRM__PS(r30)		; restore ps (could trap)
;
; Clear the interrupt flag.
;
	rc	r31				; clear interrupt flag

	ldq	p23, FRM__PC(r30)		; get pc
	and	p_misc, #<3@P_MISC__CM__S>, p7	; get current mode
	and	p20, #<3@PS__CM__S>, p4		; get new mode

	lda	p6, ^x3F(r31)				; build cleaning mask
	sll	p6, #<PS__SP_ALIGN__S>, p6		; shift to correct spot
	lda	p6, <<3@PS__CM__S>!<3@PS__SW__S>>(p6)	; sw, cm, align mask

	cmplt	p4, p7, p5			; check mode going wrong way
	bne	p5, call_pal__rei_illop		; branch on wrong way
	bic	p20, p6, p5			; test other ps bits
	bne	p5, call_pal__rei_illop		; branch on bad ps bits
;
; Hack alert. We don't have visibility to r4-r7 because they are in our
; shadow range. So we need to peek under the covers.
;
; Current state:
;	r2	available
;	r3	available
;
; Turn off shadow mode. Do the loads. Then turn shadow mode back on.
;-
	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_mfpr	r3, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r3, #<1@EV6__I_CTL__SDE7__S>, r3; zap sde
	bis	r31, r31, r31			; separate blocks
	bis	r31, r31, r31			; separate blocks

	hw_mtpr	r3, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r3, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	ldq	r4, FRM__R4(r30)		; restore r4
	ldq	r5, FRM__R5(r30)		; restore r5
	ldq	r6, FRM__R6(r30)		; restore r6
	ldq	r7, FRM__R7(r30)		; restore r7

	bis	r3, #<1@EV6__I_CTL__SDE7__S>, r3; or in sde
	hw_mtpr	r3, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r3, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now back to business
;
	ldq	r2, FRM__R2(r30)		; restore r2
	ldq	r3, FRM__R3(r30)		; restore r3

	hw_ldq/p p6, PT__PCBB(p_temp)		; get pcbb
	extbl	p20, #<<PS__SP_ALIGN__S>/8>, p5	; get ps<sp_align> bits
	addq	p6, p7, p7			; current sp HWPCB offset
	addq	p6, p4, p6			; new sp HWPCB offset

	zapnot	p20, #3, p20			; get ps<15:0> (ipl=0)
	zap	p_misc, #3, p_misc		; clear p_misc<15:0>
	hw_mtpr	p4, EV6__PS			; (4,0L) write new cm
	bis	p_misc, p20, p_misc		; or in new ps

	bic	p23, #3, p23			; clean pc

	lda	p20, 64(r30)			; stack pointer beyond frame
	or	p20, p5, p20			; adjust by sp_align
	hw_stq/p p20, PCB__KSP(p7)		; stash current mode sp
	hw_ldq/p r30, PCB__KSP(p6)		; get new sp

.if ne	turbo_pcia_intr_fix
	PVC_JSR	check_pcia, bsr=1
	bsr	p20, check_pcia_intr
.endc

	hw_ret_stall (p23)			; return with stall
;+
; Illegal operand.
;
; Either stack was not aligned, or the ps had mode and/or other
; validity problems.
;
; Take an illegal operand trap.
;
; Current state:
;	PT__FAULT_PC	next pc
;-
call_pal__rei_stack:
call_pal__rei_illop:
	lda	p4, SCB__ILLOP(r31)		; illegal operand
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb offset
	br	r31, trap__post_km

	END_CALL_PAL

;+
; CALL_PAL__REMQHIL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		address of the header
;
; Function:
;	If the secondary interlock is clear, REMQHIL removes from the
;	long self-relative queue the entry following the header, pointed to
;	by r16, and the address of the removed entry is returned in r1.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;	r2		saved and used as scratch
;	r3		saved and used as scratch
;	r8		saved and used as scratch
;	r9		saved and used as scratch
;
; Exit state:
;	r0	-1 	failed to obtain secondary interlock
;		 0	queue was empty
;		 1	queue now not empty
;		 2	queue now empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQHIL>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	addl	r16, r31, p4			; sign extend H
	xor	r16, p4, p4			; check H = sext H
	bne	p4, call_pal__queue_addr_error	; branch if bad sext H

	and	r16, #^x7, p4			; check H alignment
	bne	p4, call_pal__queue_addr_error	; branch if H not quad-aligned

	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

	ldah	p4,<<queue_setup_lock_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_lock_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	bis	r31, r31, r2			; flag that we don't have lock
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <REMQHIL>
;
; Now try to acquire the lock
;
call_pal__remqhil_lock:
	ldl_l	r0, (r16)			; try to get H, interlocked
	beq	r0, call_pal__queue_was_empty	; done, queue already empty
	blbs	r0, call_pal__queue_busy	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stl_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__remqhil_retry	; retry on failure

	mb					; per SRM

	bis	r2, #1, r2			; flag that we have the lock
;
; Current state:
;	r0	E-H
;	r2	1, lock acquired
;	r16	H
;
	addl	r16, r0, r1				; E
	and	r1, #^x7, p4				; check E alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment

	ldl	r3, (r1)				; S-E (checking access)
	addl	r3, r1, r9				; S
	and	r9, #^x7, p4				; check S alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment

	hw_ldl/w r31, 0(r9)				; check S access
;
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)			; offset for dfault
;
; Current state:
;	r0	E-H
;	r1	E
;	r2	1, lock acquired
;	r9	S
;	r16	H
;
	subl	r16, r9, r3			; H-S
	stl	r3, 4(r9)			; (S+4) <- H-S

	mb

	subl	r31, r3, r3			; S-H
	stl	r3, (r16)			; (H)	<- S-H, clear secondary

	cmpeq	r3, r31, r0			; if S-H=0 => queue now empty
	addq	r0, #1, r0			; 1=>not empty, 2=>now empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;+
; Check retry count and try again if not zero.
;-
call_pal__remqhil_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__remqhil_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

;+
; Queue already empty
;
; Return to user with status of r0=0.
; Current state:
;	r0	0=>queue already empty
;	r16	H
; Note: Previous implementations claimed stl_c not required.
;-
call_pal__queue_was_empty:
	stl_c	r0, (r16)			; release primary lock
	bis	r31, r31, r0			; reset r0=0
	bis	r16, r31, r1			; 'removed' entry = H

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__REMQTIL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		address of the header
;
; Function:
;	If the secondary interlock is clear, REMQTIL removes from the
;	long self-relative queue the entry preceding the header, pointed to
;	by r16, and the address of the removed entry is returned in r1.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;
; Exit state:
;	r0	-1 	failed to obtain secondary interlock
;		 0	queue was empty
;		 1	queue now not empty
;		 2	queue now empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQTIL>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	addl	r16, r31, p4			; sign extend H
	xor	r16, p4, p4			; check H = sext H
	bne	p4, call_pal__queue_addr_error	; branch if bad sext H

	and	r16, #^x7, p4			; check H alignment
	bne	p4, call_pal__queue_addr_error	; branch if H not quad-aligned

	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

	ldah	p4,<<queue_setup_lock_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_lock_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	bis	r31, r31, r2			; flag that we don't have lock
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <REMQTIL>

;
; Note: We do a ldq_l to get the backward link as well as the lock.
;
call_pal__remqtil_lock:
	ldq_l	r0, (r16)			; try to get H, interlocked
	beq	r0, call_pal__queue_was_empty	; done, queue already empty
	blbs	r0, call_pal__queue_busy	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stl_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__remqtil_retry	; retry on failure

	mb					; per SRM

	bis	r2, #1, r2			; flag that we have the lock
;
; Current state:
;	r0<31:0> 	S-H
;	r0<63:32>	E-H
;	r2	1, lock acquired
;	r16	H
;
	sra	r0, #32, r9				; E-H
	addl	r16, r9, r1				; E
	and	r1, #^x7, p4				; check E alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment

	ldl	r3, 4(r1)				; P-E
	addl	r1, r3, r9				; P
	and	r9, #^x7, p4				; check P alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment
	hw_ldl/w r31, 0(r9)				; check P access
;
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)			; offset for dfault
;
; Current state:
;	r0<31:0> 	S-H
;	r0<63:32>	E-H
;	r1		E
;	r2		1, lock acquired
;	r9		P
;	r16		H
;
	subl	r9, r16, r3			; P-H
	stl	r3, 4(r16)			; (H+4)	<- P-H
	subl	r16, r9, r3			; H-P
	beq	r3, call_pal__remqtil_now_empty	; branch if now empty

	stl	r3, (r9)			; (P)	<- H-P

	mb

	stl	r0, (r16)			; (H)	<- S-H, clear secondary

	bis	r31, #1, r0			; 1=>not empty
	br	r31, call_pal__remqtil_done	; done
;
; Last entry in the queue
;
call_pal__remqtil_now_empty:
	mb

	stl	r3, (r16)			; (H)	<- 0

	bis	r31, #2, r0			; 2=>queue now empty

call_pal__remqtil_done:
	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

;+
; Check retry count and try again if not zero.
;-
call_pal__remqtil_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__remqtil_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__REMQHIQ
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		address of the header
;
; Function:
;	If the secondary interlock is clear, REMQHIQ removes from the
;	quad self-relative queue the entry following the header, pointed to
;	by r16, and the address of the removed entry is returned in r1.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;
; Exit state:
;	r0	-1 	failed to obtain secondary interlock
;		 0	queue was empty
;		 1	queue now not empty
;		 2	queue now empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQHIQ>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	and	r16, #^xF, p4			; check H alignment
	bne	p4, call_pal__queue_addr_error	; branch if H not octa-aligned

	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

	ldah	p4,<<queue_setup_lock_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_lock_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	bis	r31, r31, r2			; flag that we don't have lock
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <REMQHIQ>
;
; Now try to acquire the lock
;
call_pal__remqhiq_lock:
	ldq_l	r0, (r16)			; try to get H, interlocked
	beq	r0, call_pal__queue_was_empty	; done, queue already empty
	blbs	r0, call_pal__queue_busy	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stq_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__remqhiq_retry	; retry on failure

	mb					; per SRM

	bis	r2, #1, r2			; flag that we have the lock
;
; Current state:
;	r0	E-H
;	r2	1, lock acquired
;	r16	H
;
	addq	r16, r0, r1				; E
	and	r1, #^xF, p4				; check E alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment

	ldq	r3, (r1)				; S-E
	addq	r3, r1, r9				; S
	and	r9, #^xF, p4				; check S alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment
	hw_ldq/w r31, 0(r9)				; check S access
;
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)			; offset for dfault
;
; Current state:
;	r0	E-H
;	r1	E
;	r2	1, lock acquired
;	r9	S
;	r16	H
;
	subq	r16, r9, r3			; H-S
	stq	r3, 8(r9)			; (S+8) <- H-S

	mb

	subq	r31, r3, r3			; S-H
	stq	r3, (r16)			; (H)	<- S-H, clear secondary

	cmpeq	r3, r31, r0			; if S-H=0 => queue now empty
	addq	r0, #1, r0			; 1=>not empty, 2=>now empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

;+
; Check retry count and try again if not zero.
;-
call_pal__remqhiq_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__remqhiq_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__REMQTIQ
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		address of the header
;
; Function:
;	If the secondary interlock is clear, REMQTIQ removes from the
;	quad self-relative queue the entry preceding the header, pointed to
;	by r16, and the address of the removed entry is returned in r1.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;
; Exit state:
;	r0	-1 	failed to obtain secondary interlock
;		 0	queue was empty
;		 1	queue now not empty
;		 2	queue now empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQTIQ>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	and	r16, #^xF, p4			; check H alignment
	bne	p4, call_pal__queue_addr_error	; branch if H not octa-aligned

	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

	ldah	p4,<<queue_setup_lock_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_lock_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	bis	r31, r31, r2			; flag that we don't have lock
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <REMQTIQ>

;
; Now try to acquire the lock
;
call_pal__remqtiq_lock:
	ldq_l	r0, (r16)			; try to get H, interlocked
	beq	r0, call_pal__queue_was_empty	; done, queue already empty
	blbs	r0, call_pal__queue_busy	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stq_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__remqtiq_retry	; retry on failure

	mb					; per SRM

	bis	r2, #1, r2			; flag that we have the lock
;
; Current state:
;	r0	S-H
;	r2	1, lock acquired
;	r16	H
;
	ldq	r3, 8(r16)				; E-H
	addq	r16, r3, r1				; E
	and	r1, #^xF, p4				; check E alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment

	ldq	r3, 8(r1)				; P-E
	addq	r1, r3, r9				; P
	and	r9, #^xF, p4				; check P alignment
	bne	p4, call_pal__queue_addr_error_unlock	; branch on bad alignment
	hw_ldq/w r31, 0(r9)				; check P access
;
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; For errors of any other kind, take a mchk.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)			; offset for dfault
;
; Current state:
;	r0	S-H
;	r1	E
;	r2	1, lock acquired
;	r9	P
;	r16	H
;
	subq	r9, r16, r3			; P-H
	stq	r3, 8(r16)			; (H+8)	<- P-H
	subq	r16, r9, r3			; H-P
	beq	r3, call_pal__remqtiq_now_empty	; branch if now empty

	stq	r3, (r9)			; (P)	<- H-P

	mb

	stq	r0, (r16)			; (H)	<- S-H, clear secondary

	bis	r31, #1, r0			; 1=>not empty
	br	r31, call_pal__remqtiq_done	; done
;
; Last entry in the queue
;
call_pal__remqtiq_now_empty:
	mb

	stq	r3, (r16)			; (H)	<- 0

	bis	r31, #2, r0			; 2=>queue now empty

call_pal__remqtiq_done:
	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

;+
; Check retry count and try again if not zero.
;-
call_pal__remqtiq_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__remqtiq_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__REMQUEL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of entry
;
; Function:
;	REMQUEL removes the entry addressed by r16 from the 
;	long absolute queue. The address of the removed entry is returned
;	in r1.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;	r16	address of entry to be removed
;
; Exit state:
;	r0	-1 	no entry to be removed
;		 0	queue now empty
;		 1	queue now not empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQUEL>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	hw_stq/p r2, PT__R2(p_temp)		; get scratch registers
	hw_stq/p r3, PT__R3(p_temp)		; get scratch registers
	hw_stq/p r8, PT__R8(p_temp)		; get scratch registers
	hw_stq/p r9, PT__R9(p_temp)		; get scratch registers

	ldah	p4,<<queue_setup_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	bis	r16, r31, r1			; return address of entry

	br	r31, call_pal__remqueld_merge_from_remquel

	END_CALL_PAL

;+
; CALL_PAL__REMQUEQ
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of entry
;
; Function:
;	REMQUEQ removes the entry addressed by r16 from the quadword
;	absolute queue. The address of the removed entry is returned
;	in r1.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
;	Note that r0 and r1 are UNPREDICTABLE if an exception occurs.
;
; Register use:
;	r16	address of entry to be removed
;
; Exit state:
;	r0	-1 	no entry to be removed
;		 0	queue now empty
;		 1	queue now not empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQUEQ>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	ldah	p4,<<queue_setup_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	bis	r16, r31, r1			; return address of entry

	br	r31, call_pal__remqueqd_merge_from_remqueq

	END_CALL_PAL

;+
; CALL_PAL__REMQUEQD
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains the address of the address of the entry
;
; Function:
;	REMQUEQD removes an entry from the absolute queue. Register r16
;	contains the address of the address of the entry in the
;	absolute queue. The address of the removed entry is returned
;	in r1.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
;	Note that r0 and r1 are UNPREDICTABLE if an exception occurs.
;
; Register use:
;	r16	address of the address of the entry to be removed
;
; Exit state:
;	r0	-1 	no entry to be removed
;		 0	queue now empty
;		 1	queue now not empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQUEQD>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	ldah	p4,<<queue_setup_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	and	r16, #^xF, p20			; check r16 alignment
	bne	p20, call_pal__queue_addr_error	; branch if not octa-aligned
	ldq	r1, (r16)			; get E
;
; Validate S and P.
;
call_pal__remqueqd_merge_from_remqueq:
	and	r1, #^xF, p20			; check entry alignment
	bne	p20, call_pal__queue_addr_error	; branch if not octa-aligned

	ldq	p20, (r1)			; get S
	ldq	r0, 8(r1)			; get P

	bis	p20, r0, p4			; combine for S/P alignment check
	and	p4, #^xF, p4			; check S/P alignment
	bne	p4, call_pal__queue_addr_error	; branch if not octa-aligned

	CONT_CALL_PAL <REMQUEQD>
;
; Probe S and P.
;
	hw_ldq/w r31, (p20)			; probe S
	hw_ldq/w r31, (r0)			; probe P
;
; Probes have completed and we are aligned. Watch out for 3rd level tnv on
; an mp machine.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	stq	p20, (r0)			; (P) <- S
	stq	r0, 8(p20)			; (S+8) <- P
;
; Now write r0.
; Current state:
;	r0	P
;	r1	E
;	p20	S
;
	bis	r0, r31, p6			; p6 <- P, since we need r0 now
	bis	r31, #1, r0			; assume r0=1 => queue not empty
	cmpeq	p20, p6, p4			; if S=P => queue now empty
	cmpeq	r1, p6, p5			; if E=P => queue was empty
	subq	r0, p4, r0			; r0=0  => queue now empty
	subq	r0, p5, r0			; r0=-1 => queue was empty

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__REMQUELD
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains the address of the address of the entry
;
; Function:
;	REMQUELD removes an entry from the absolute queue. Register r16
;	contains the address of the address of the entry in the
;	absolute queue. The address of the removed entry is returned
;	in r1.
;
;	Before performing any part of the insertion, the processor validates
;	that the entire operation can be completed.
;
; Register use:
;	r16	address of the address of the entry to be removed
;
; Exit state:
;	r0	-1 	no entry to be removed
;		 0	queue now empty
;		 1	queue now not empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQUELD>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	hw_stq/p r2, PT__R2(p_temp)		; get scratch registers
	hw_stq/p r3, PT__R3(p_temp)		; get scratch registers
	hw_stq/p r8, PT__R8(p_temp)		; get scratch registers
	hw_stq/p r9, PT__R9(p_temp)		; get scratch registers

	ldah	p4,<<queue_setup_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_setup_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	ldq_u	r2, (r16)			; first half
	ldq_u	r3, 3(r16)			; second half
	extll	r2, r16, r2			; extract low
	extlh	r3, r16, r3			; extract high
	or	r2, r3, r2			; combine
	addl	r2, r31, r1			; sign extend to get addr of E

	CONT_CALL_PAL <REMQUELD>
;
; Probe the addresses. We do non-trapping hw_ld, so probe both ends.
;
; Current state:
;	r1	address of E
;
call_pal__remqueld_merge_from_remquel:
	and	r1, #^x3, p4			; check entry alignment
	bne	p4, call_pal__remqueld_e_una	; entry is not aligned

	ldl	r9, (r1)			; get S
	ldl	r8, 4(r1)			; get P
	br	r31, call_pal__remqueld_probe	; continue with probe
;
; Entry is unaligned. Get an unaligned quadword with S in first longword
; and P in second longword, and sort it out.
;
call_pal__remqueld_e_una:
	ldq_u	r2, (r1)			; first half
	ldq_u	r3, 7(r1)			; second half
	extql	r2, r1, r2			; extract low
	extqh	r3, r1, r3			; extract high
	or	r2, r3, r9			; combine

	sra	r9, #32, r8			; (E+4) = P sign-extended
	addl	r31, r9, r9			; sign extend S
;
; Probe S and P.
; Current state:
;	r8	P
;	r9	S
;	r1	E
;
call_pal__remqueld_probe:			; continue with probe
	hw_ldl/w r31, 4(r9)			; probe first part of S blink
	hw_ldl/w r31, 7(r9)			; probe second part of S blink
	hw_ldl/w r31, 0(r8)			; probe first part of P flink
	hw_ldl/w r31, 3(r8)			; probe second part of P flink
;
; Probes have completed and we are aligned. Watch out for 3rd level tnv on
; an mp machine.
;
; NOTE: We need some mp debug code ???
;
	ldah	p4,<<queue_fault_s_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_s_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
;
; One shot at optimization -- If both aligned, do as aligned.
; Current state:
;	r8	P
;	r9	S
;	r1	E
;
	bis	r8, r9, r2			; combine r8 and r9
	and	r2, #^x3, r3			; check long alignment
	bne	r3, call_pal__remqueld_una	; do as unaligned

	stl	r9, (r8)			; (P) <- S
	stl	r8, 4(r9)			; (S+4) <- P
	br	r31, call_pal__remqueld_done
;
; Do both as unaligned.
; Use macro with arguments reg, disp, addr, scr1, scr2, scr3
;
call_pal__remqueld_una:

	QSTORE_UNALIGNED_LONG r9, 0, r8, r2, r3, p20 	; (P) 	<- S

	QSTORE_UNALIGNED_LONG r8, 4, r9, r2, r3, p20	; (S+4) <- P

;
; Done. Clear catcher, write r0, restore scratch registers, and return.
;	r8	P
;	r1	E
;	r9	S
;
call_pal__remqueld_done:
	bis	r31, #1, r0			; assume r0=1 => queue not empty
	cmpeq	r9, r8, p4			; if S=P => queue now empty
	cmpeq	r1, r8, p5			; if E=P => queue was empty
	subq	r0, p4, r0			; r0=0  => queue now empty
	subq	r0, p5, r0			; r0=-1 => queue was empty

	hw_ldq/p r2, PT__R2(p_temp)		; restore
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_ldq/p r8, PT__R8(p_temp)		; restore
	hw_ldq/p r9, PT__R9(p_temp)		; restore

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__SWASTEN
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16<0>		new enable bit for the current mode
;
; Function:
;	Swap the AST enable bit for the current mode.
;	r0 <- ZEXT(ASTEN<PS<CM>>)
;	ASTEN<PS<CM>> <- r16<0>
;
; Register use:
;
; Exit state:
;	r0		ZEXT(ASTEN<PS<CM>>)
;
;-
	START_CALL_PAL <SWASTEN>

	hw_mfpr	p4, EV6__PROCESS_CONTEXT	; (4,0L) get asten bits

	srl	p_misc, #P_MISC__CM__S, p5	; get current mode
	and	p5, #P_MISC__CM__M, p5		; clean it

	bis	r31, #1, p6			; get a 1
	addq	p5, #EV6__ASTER__ASTER__S, p5	; shift count = mode + __S
	sll	p6, p5, p6			; get 1 into position

	and	p4, p6, r0			; save current value
	and	r16, #1, p7			; get new value from bit 0
	bic	p4, p6, p4			; clear current value
	sll	p7, p5, p7			; new value into position
	bis	p4, p7, p4			; new aster

	hw_mtpr	p4, EV6__ASTER			; (4,0L) update aster
	srl	r0, p5, r0			; stick old value in bit 0

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__WR_PS_SW
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	PS<SW> <- r16<1:0>
;-
ASSUME P_MISC__SW__S eq 0

	START_CALL_PAL <WR_PS_SW>

	and	r16, #3, p4			; clean to <1:0>
	bic	p_misc, #P_MISC__SW__M, p_misc	; clear SW in p_misc
	bis	p_misc, p4, p_misc		; stash new SW
	NOP					; no hw_ret in 1st fetch block

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__RSCC
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Write register R0 with the value of the system cycle counter,
;	updating the system cycle counter from PCC.
;
; Register use:
;	r0<63:32>	high long of SCC
;	p4<31:0>	low long of PCC
;	p5<31:0>	low long of SCC
;	p6		1.0000.0000 (wrap) or 0 (nowrap)
;	p7		pcc<31:0> - scc<31:0>
;
; Exit state:
;	r0		system cycle counter
;
;-
	START_CALL_PAL <RSCC>

	hw_ldq/p r0, PT__SCC(p_temp)	; get SCC
	rpcc	p4			; get PCC

	bis	r31, #1, p6		; get a 1
	sll	p6, #32, p6		; now a 1.0000.0000 (for wrap)

	zap	p4, #^xF0, p4		; low long of PCC
	zap	r0, #^xF0, p5		; low long of SCC
	zap	r0, #^x0F, r0		; high long of SCC

	subq	p4, p5, p7		; PCC<31:0> - SCC<31:0>
	cmovge	p7, r31, p6		; if p7 >= 0, p6 <- 0 (for no wrap)
	addq	r0, p6, r0		; add wrap value to high SCC
	or	r0, p4, r0		; merge high SCC with low PCC

	hw_stq/p r0, PT__SCC(p_temp)	; update SCC

	hw_ret	(p23)			; return

	END_CALL_PAL

;+
; CALL_PAL__READ_UNQ
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Write into r0 the hardware process (thread) unique context value.
;
; Exit state:
;	r0		process unique context
;
;-
	START_CALL_PAL <READ_UNQ>

	hw_ldq/p r0, PT__PCBB(p_temp)	; get PCBB
	hw_ldq/p r0, PCB__UNQ(r0)	; get UNQ
	NOP				; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)			; return

	END_CALL_PAL

;+
; CALL_PAL__WRITE_UNQ
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Store the value of r16 in the hardware process (thread) unique
;	context value.
;-
	START_CALL_PAL <WRITE_UNQ>

	hw_ldq/p p4, PT__PCBB(p_temp)	; get PCBB
	hw_stq/p r16, PCB__UNQ(p4)	; write UNQ
	NOP				; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)			; return

	END_CALL_PAL

;+
; CALL_PAL__AMOVRR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		first source
;	r17		first destination
;	r18		first length encoding
;	r19		second source
;	r20		second destination
;	r21		second length encoding
;
; Function:
;
; Register use:
;
; Exit state:
;
;-
	START_CALL_PAL <AMOVRR>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	hw_stq/p r1, PT__R1(p_temp)		; get some scratch space
	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

	rc	r1				; check intr_flag
	beq	r1, call_pal__amovrr_fail	; if clear => fail

amovrx_probe_offset = <<call_pal__amovrx_probe_fault - trap__pal_base>>

	ldah	p4,<<amovrx_probe_offset>+32768>@-16(r31)
	lda	p4,<<amovrx_probe_offset> & ^xFFFF>(p4)
	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	CONT_CALL_PAL <AMOVRR>			; continue in free space
;
; Hack alert. We don't have visibility to r20 and r21 because they are in our
; shadow range. So we need to peek under the covers.
;
; Turn off shadow mode. Grab r20 and r21 and stick them in r8 and r9,
; respectively. Turn on shadow mode.
;-
	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_mfpr	r1, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r1, #<1@EV6__I_CTL__SDE7__S>, r1; zap sde
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r20, r31, r8			; grab r20
	bis	r21, r31, r9			; grab r21
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r1, #<1@EV6__I_CTL__SDE7__S>, r1; or in sde
	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now back to business.
; Current state:
;	r8		non-shadow r20
;	r9		non-shadow r21 (length)
;

	and	r18, #3, r18			; clean 1st length encoding
	and	r9, #3, r9			; clean 2nd length encoding
;
; Probe the beginning of both writes
;
	hw_ldl/w r1, 0(r17)			; probe beginning of 1st
	hw_ldl/w r2, 0(r8)			; probe beginning of 2nd

;
; Now check to see whether destinations are naturally aligned
; Length encoding (binary) = 00 (byte), 01 (word), 10 (long), 11 (quad)
; Current state:
;	r17	1st destination
;	r18	1st length encoding cleaned 
;	r8	2nd destination
;	r9	2nd length encoding cleaned
;
	bis	r31, #1, p20			; get a 1
	sll	p20, r18, p20			; produce mask of correct number
	subq	p20, #1, p20			;	of 1s for check
	and	r17, p20, r1			; check for natural alignment
	bne	r1, call_pal__amovrr_ua1	; branch for unaligned

	bis	r31, #1, p20			; get a 1
	sll	p20, r9, p20			; produce mask of correct number
	subq	p20, #1, p20			;	of 1s for check
	and	r8, p20, r2			; check for natural alignment
	bne	r2, call_pal__amovrr_ua2	; branch for unaligned
;+
; Probes have completed and we are aligned. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
;
; NOTE: ??? We need some mp test code here.
;-
amovrx_write_offset = <<call_pal__amovrx_write_fault - trap__pal_base>>

	ldah	p4,<<amovrx_write_offset>+32768>@-16(r31)
	lda	p4,<<amovrx_write_offset> & ^xFFFF>(p4)
	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
;
; Handle naturally aligned store. This code is executed twice, once for
; each store.
;
.iif ndf amovrx_retry_count, amovrx_retry_count = 32

call_pal__amovrr_aligned_store:
	lda	r3, amovrx_retry_count(r31)	; retry count
	beq	r18, call_pal__amovrrb		; branch for byte access
	subq	r18, #2, p20
	beq	p20, call_pal__amovrrl		; branch for long access 
	subq	r18, #1, p20
	beq	p20, call_pal__amovrrw		; branch for word access
;
; Access is aligned quadword.
; Note that ev4 and ev5 copied r16 to r18 first, but this is not necessary
; because we are not repositioning the data. A retry uses the same data.
;
call_pal__amovrrq:
	stq	r16, (r17)			; store, no lock necessary
	br	r31, call_pal__amovrr_a2	; on to next access
;
; Access is byte.
;
call_pal__amovrrb:
	bis	r31, #1, p20			; get a 1
	and	r17, #3, r1			; get low bits of address
	sll	p20, r1, p20			; byte zap value
	insbl	r16, r1, r18			; reposition data to r18
	bic	r17, #3, r2			; produce aligned long address

	ldl_l	r1, (r2)			; fetch data with lock
	zap	r1, p20, r1			; zap old data
	bis	r1, r18, r1			; merge new and old
;
; NOTE: ??? We should have some mp test code to cause occasional failures.
;
	stl_c	r1, (r2)			; store data with unlock
	blbc	r1, call_pal__amovrrb_retry	; failed => retry
	br	r31, call_pal__amovrr_a2	; on to next access
;
; Access is word.
;
call_pal__amovrrw:
	bis	r31, #3, p20			; get a 3
	and	r17, #3, r1			; get low bits of address
	sll	p20, r1, p20			; byte zap value
	inswl	r16, r1, r18			; reposition data to r18
	bic	r17, #3, r2			; produce aligned long address

	ldl_l	r1, (r2)			; fetch data with lock
	zap	r1, p20, r1			; zap old data
	bis	r1, r18, r1			; merge new and old
;
; NOTE: ??? We should have some mp test code to cause occasional failures.
;
	stl_c	r1, (r2)			; store data with unlock
	blbc	r1, call_pal__amovrrw_retry	; failed => retry
	br	r31, call_pal__amovrr_a2	; on to next access
;
; Access is aligned longword.
; Note that ev4 and ev5 copied r16 to r18 first, but this is not necessary
; because we are not repositioning the data. A retry uses the same data.
;
call_pal__amovrrl:
	stl	r16, (r17)			; store, no lock necessary
	br	r31, call_pal__amovrr_a2	; on to next access
;
; First store has completed. Now go on to the second. Make the second
; write look like the first.
;
call_pal__amovrr_a2:
	blt	r9, call_pal__amovrr_done	; done if second time

	bis	r19, r31, r16
	bis	r8, r31, r17
	bis	r9, r31, r18

	subq	r31, #1, r9			; 2nd time flag
	br	r31, call_pal__amovrr_aligned_store
;
; Do a retry on the byte store.
; Register use:
;	r3		counter
;
call_pal__amovrrb_retry:
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrrb		; try again
	bge	r9, call_pal__amovrr_done	; fail if not on 2nd
;
; Lock timer ran out on 2nd, but we are not allowed to fail. So do an
; ordinary load/store and return success.
;
	ldl	r1, (r2)			; load data
	zap	r1, p20, r1			; zap old data
	bis	r1, r18, r1			; merge new and old
	stl	r1, (r2)			; store data
	br	r31, call_pal__amovrr_done	; mark success (a little bogus)
;
; Do a retry on the word store.
; Register use:
;	r3		counter
;
call_pal__amovrrw_retry:
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrrw		; try again
	bge	r9, call_pal__amovrr_done	; fail if not on 2nd
;
; Lock timer ran out on 2nd, but we are not allowed to fail. So do an
; ordinary load/store and return success.
;
	ldl	r1, (r2)			; load data
	zap	r1, p20, r1			; zap old data
	bis	r1, r18, r1			; merge new and old
	stl	r1, (r2)			; store data
	br	r31, call_pal__amovrr_done	; mark success (a little bogus)

;+
; Unaligned amovrr.
;
; Current state:
;	p20	1 -- word
;		3 -- lw
;		7 -- qw
;	r16		first source
;	r17		first destination
;	r18		first length encoding
;	r19		second source
;	r8		second destination
;	r9		second length encoding
;
; We have probed the front ends of both, but because we are not naturally
; aligned, we need to probe the back ends.
;-
call_pal__amovrr_ua1:
	addq	r17, p20, r1			; get address of back end
	hw_ldl/w r1, (r1)			; probe

	bis	r31, #1, p20			; get a 1
	sll	p20, r9, p20			; produce mask of correct number
	subq	p20, #1, p20			;	of 1s for check

call_pal__amovrr_ua2:
	addq	r8, p20, r1			; get address of back end
	hw_ldl/w r1, (r1)			; probe	
;
; Probes have been completed. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
;
; NOTE: ??? We need some mp test code here.
;
	ldah	p4,<<amovrx_write_offset>+32768>@-16(r31)
	lda	p4,<<amovrx_write_offset> & ^xFFFF>(p4)
	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

call_pal__amovrr_ua_store:
	lda	r3, amovrx_retry_count(r31)	; retry count
	beq	r18, call_pal__amovrrbu		; branch for byte access
	subq	r18, #2, p20
	beq	p20, call_pal__amovrrlu		; branch for long access 
	subq	r18, #1, p20
	beq	p20, call_pal__amovrrwu		; branch for word access
;
; Access is unaligned quadword.
; Use macro with arguments reg, addr, scr1, scr2, scr3, lock, err, err1
;
call_pal__amovrrqu:
	STORE_UNALIGNED_QUAD r16, r17, r1, r2, p20, lock=1,-
		err=call_pal__amovrrqu_retry,-
		err1=call_pal__amovrrqu_retry1

	br	r31, call_pal__amovrr_ua_store2	; on to next acess
;
; Access is byte.
;
call_pal__amovrrbu:
	bis	r31, #1, p20			; get a 1
	and	r17, #3, r1			; get low bits of address
	sll	p20, r1, p20			; byte zap value
	insbl	r16, r1, r18			; resposition data to r18
	bic	r17, #3, r2			; produce aligned long address

	ldl_l	r1, (r2)			; fetch data with lock
	zap	r1, p20, r1			; zap old data
	bis	r1, r18, r1			; merge new and old
;
; NOTE: ??? We should have some mp test code to cause occasional failures.
;
	stl_c	r1, (r2)			; store data with unlock
	blbc	r1, call_pal__amovrrbu_retry	; failed => retry
	br	r31, call_pal__amovrr_ua_store2	; on to next access
;
; Access is unaligned word.
; Use macro with arguments reg, addr, scr1, scr2, scr3, lock, err, err1
;
call_pal__amovrrwu:
	STORE_UNALIGNED_WORD r16, r17, r1, r2, p20, lock=1,-
		err=call_pal__amovrrwu_retry,-
		err1=call_pal__amovrrwu_retry1

	br	r31, call_pal__amovrr_ua_store2	; on to next access
;
; Access is unaligned long.
; Use macro with arguments reg, addr, scr1, scr2, scr3, lock, err, err1
;
call_pal__amovrrlu:
	STORE_UNALIGNED_LONG r16, r17, r1, r2, p20, lock=1,-
		err=call_pal__amovrrlu_retry,-
		err1=call_pal__amovrrlu_retry1

	br	r31, call_pal__amovrr_ua_store2	; on to next access
;
; First store has completed. Now go on to second. Make the second
; write look like the  first.
;
call_pal__amovrr_ua_store2:
	blt	r9, call_pal__amovrr_done	; done if second time

	bis	r19, r31, r16
	bis	r8, r31, r17
	bis	r9, r31, r18

	subq	r31, #1, r9			; 2nd time flag
	br	r31, call_pal__amovrr_ua_store
;
; Do a retry on the unaligned quad store.
; Register use:
;	r3		counter
; There are two entries, one for the 1st half of the word, one for the 2nd.
;
call_pal__amovrrqu_retry:			; 1st half of the word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrrqu		; try again
	bge	r9, call_pal__amovrr_done	; fail if not on 2nd store
;
; Lock timer ran out on the 1st half of the 2nd store. We are not allowed
; to fail. So do an ordinary load/store and return success.
;
	STORE_UNALIGNED_QUAD r16, r17, r1, r2, p20

	br	r31, call_pal__amovrr_done	; mark success (a little bogus)

call_pal__amovrrqu_retry1:			; 2nd half of word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrrqu		; try again
;
; Lock timer ran out on the 2nd half of either store. We are not allowed
; to fail. So do an ordinary load/store.
;
	STORE_UNALIGNED_QUAD r16, r17, r1, r2, p20

	blt	r9, call_pal__amovrr_done	; done if second time
	br	r31, call_pal__amovrr_ua_store2	; go on to next access
;
; Do a retry on the byte store.
; Register use:
;	r3		counter
;
call_pal__amovrrbu_retry:
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrrbu		; try again
	bge	r9, call_pal__amovrr_done	; fail if not on 2nd
;
; Lock timer ran out on 2nd, but we are not allowed to fail. So do an
; ordinary load/store and return success.
;
	ldl	r1, (r2)			; load data
	zap	r1, p20, r1			; zap old data
	bis	r1, r18, r1			; merge new and old
	stl	r1, (r2)			; store data
	br	r31, call_pal__amovrr_done	; mark success (a little bogus)
;
; Do a retry on the word store.
; Register use:
;	r3		counter
; There are two entries, one for the 1st half of the word, one for the 2nd.
;
call_pal__amovrrwu_retry:			; 1st half of the word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrrwu		; try again
	bge	r9, call_pal__amovrr_done	; fail if not on 2nd store
;
; Lock timer ran out on the 1st half of the 2nd store. We are not allowed
; to fail. So do an ordinary load/store and return success.
;
	STORE_UNALIGNED_WORD r16, r17, r1, r2, p20

	br	r31, call_pal__amovrr_done	; mark success (a little bogus)

call_pal__amovrrwu_retry1:			; 2nd half of word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrrwu		; try again
;
; Lock timer ran out on the 2nd half of either store. We are not allowed
; to fail. So do an ordinary load/store.
;
	STORE_UNALIGNED_WORD r16, r17, r1, r2, p20

	blt	r9, call_pal__amovrr_done	; done if second time
	br	r31, call_pal__amovrr_ua_store2	; go on to next access
;
; Do a retry on the long store.
; Register use:
;	r3		counter
; There are two entries, one for the 1st half of the word, one for the 2nd.
;
call_pal__amovrrlu_retry:			; 1st half of the word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrrlu		; try again
	bge	r9, call_pal__amovrr_done	; fail if not on 2nd store
;
; Lock timer ran out on the 1st half of the 2nd store. We are not allowed
; to fail. So do an ordinary load/store and return success.
;
	STORE_UNALIGNED_LONG r16, r17, r1, r2, p20

	br	r31, call_pal__amovrr_done	; mark success (a little bogus)

call_pal__amovrrlu_retry1:			; 2nd half of word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrrlu		; try again
;
; Lock timer ran out on the 2nd half of either store. We are not allowed
; to fail. So do an ordinary load/store.
;
	STORE_UNALIGNED_LONG r16, r17, r1, r2, p20

	blt	r9, call_pal__amovrr_done	; done if second time
	br	r31, call_pal__amovrr_ua_store2	; go on to next access

;+
; Done. Either we did neither, or did both. Although note that if both
; were accessible and we succeeded on the first, we couldn't allow a conditional
; load/store timeout on the second. In that case, we did an ordinary load/store
; and flagged success anyway.
;
; Register use:
;	r9		<0 => we succeeded on the first,
;				and are on the second.
;-
call_pal__amovrr_fail:
	bis	r31, r31, r9			; clear 2nd time flag,
						;	which will set failure
call_pal__amovrr_done:
	cmplt	r9, r31, r18			; set success or failure

	hw_ldq/p r1, PT__R1(p_temp)		; restore r1
	hw_ldq/p r2, PT__R2(p_temp)		; restore r2
	hw_ldq/p r3, PT__R3(p_temp)		; restore r3
	hw_ldq/p r8, PT__R8(p_temp)		; restore r8
	hw_ldq/p r9, PT__R9(p_temp)		; restore r9

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret_stall (p23)			; return (stall for pvc)

;+
; call_pal__amovrx_probe_fault
;
; A mm fault occurred while probing for amovrx. Prepare to take a trap
; via trap__post_km_45.
;
; Current state:
;	p4	pte with <17:16>
;		^b00 => double
;		^b11 => invalid dpte
;		^b01 => dfault 
;	p5	mm_stat
;	p6	va
;
;	r1	needs to be restored
;	r2	needs to be restored
;	r3	needs to be restored
;	r8	needs to be restored
;	r9	needs to be restored
;
;	r25	needs to be restored
;	r26	needs to be restored
;
;	PT__FAULT_SCB	SCB offset
;	PT__CALL_PAL_PC	next pc
;
;	PT__R25		saved r25
;	PT__R26		saved r26
;
; On exit to trap__post_km_45
;	r1		restored
;	r2		restored
;	r3		restored
;	r8		restored
;	r9		restored
;
;	r25		restored
;	r26		restored
;
;	PT__FAULT_PC	fault pc
;	PT__FAULT_SCB	scb offset
;	PT__FAULT_R4	fault va
;	PT__FAULT_R5	mmf flags
;-
	PVC_JSR pal_mm_dispatch, dest=1
call_pal__amovrx_probe_fault:
	hw_ldq/p p7, PT__FAULT_SCB(p_temp)		; get SCB offset
	cmpeq	p7, #SCB__FOR, p7			; check for FOR

	srl	p5, #EV6__MM_STAT__OPCODE__S, p4	; get opcode
	and	p4, #EV6__MM_STAT__OPCODE__M, p4	; clean
	cmpeq	p4, #OPCODE__HW_LD, p4			; is it hw_ld?
	bic	p4, p7, p4				; clear mmf on FOR
	sll	p4, #63, p4				; set up mmf

	hw_stq/p p6, PT__FAULT_R4(p_temp)	; store va
	hw_stq/p p4, PT__FAULT_R5(p_temp)	; store mmf

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap handler
	hw_ldq/p p7, PT__CALL_PAL_PC(p_temp)	; get user pc + 4
	subq	p7, #4, p7			; point to amov instruction
	hw_stq/p p7, PT__FAULT_PC(p_temp)	; store fault pc

	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	hw_ldq/p r1, PT__R1(p_temp)		; restore r1
	hw_ldq/p r2, PT__R2(p_temp)		; restore r2
	hw_ldq/p r3, PT__R3(p_temp)		; restore r3
	hw_ldq/p r8, PT__R8(p_temp)		; restore r8
	hw_ldq/p r9, PT__R9(p_temp)		; restore r9

	br	r31, trap__post_km_r45		; post

;+
; call_pal__amovrx_write_fault
;
; A mm fault occurred while writing for amovrx. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
; Otherwise we machine check.
;
; Current state:
;	p4	pte with <17:16>
;		^b00 => double
;		^b11 => invalid dpte
;		^b01 => dfault 
;	p5	mm_stat
;	p6	va
;	p23	fault pc (from PALcode flow)
;
;	r1	needs to be restored on mchk
;	r2	needs to be restored on mchk
;	r3	needs to be restored on mchk
;	r8	needs to be restored on mchk
;	r9	needs to be restored on mchk
;
;	r25	needs to be restored
;	r26	needs to be restored
;
;	PT__FAULT_SCB	SCB offset
;	PT__CALL_PAL_PC	next pc
;
;	PT__R25		saved r25
;	PT__R26		saved r26
;-
	PVC_JSR pal_mm_dispatch, dest=1
call_pal__amovrx_write_fault:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	srl	p4, #PTE__SOFT__S, p5		; get type of problem
	hw_ldq/p p7, PT__FAULT_SCB(p_temp)	; need to look at scb offset
	cmpeq	p7, #SCB__TNV, p7		; check for TNV
	and	p7, p5, p7			; TNV and 3rd level?
	blbc	p7, call_pal__amovrr_mchk	; take a machine if not
;
; This was a mulitprocessor case in which the valid bit has been cleared,
; but the pte is still valid. So write it into the TB.
;
	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p6, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p6, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	p4, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	p4, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

        hw_mtpr r31, <EV6__MM_STAT ! ^x80>      ; wait for pte write

	hw_ret	(p23)				; re-do the access

call_pal__amovrr_mchk:
	hw_ldq/p r1, PT__R1(p_temp)		; restore scratch
	hw_ldq/p r2, PT__R2(p_temp)		; restore scratch
	hw_ldq/p r3, PT__R3(p_temp)		; restore scratch
	hw_ldq/p r8, PT__R8(p_temp)		; restore scratch
	hw_ldq/p r9, PT__R9(p_temp)		; restore scratch

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher

	br	r31, trap__pal_os_bugcheck	; take a mchk

	END_CALL_PAL

;+
; CALL_PAL__AMOVRM
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		first source
;	r17		first destination
;	r18		first length encoding
;	r19		second source
;	r8		second destination
;	r21		second length
;
; Function:
;
; Register use:
;
; Exit state:
;
;-
	START_CALL_PAL <AMOVRM>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc

	hw_stq/p r1, PT__R1(p_temp)		; get some scratch space
	hw_stq/p r2, PT__R2(p_temp)		; get some scratch space
	hw_stq/p r3, PT__R3(p_temp)		; get some scratch space
	hw_stq/p r8, PT__R8(p_temp)		; get some scratch space
	hw_stq/p r9, PT__R9(p_temp)		; get some scratch space

	rc	r1				; check intr_flag
	beq	r1, call_pal__amovrm_fail	; if clear => fail

	ldah	p4,<<amovrx_probe_offset>+32768>@-16(r31)
	lda	p4,<<amovrx_probe_offset> & ^xFFFF>(p4)
	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	CONT_CALL_PAL <AMOVRM>			; continue in free space

;
; Hack alert. We don't have visibility to r20 and r21 because they are in our
; shadow range. So we need to peek under the covers.
;
; Turn off shadow mode. Grab r20 and r21 and stick them in r8 and r9,
; respectively. Turn on shadow mode.
;-
	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_mfpr	r1, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r1, #<1@EV6__I_CTL__SDE7__S>, r1; zap sde
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31


	bis	r20, r31, r8			; grab r20
	bis	r21, r31, r9			; grab r21
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r1, #<1@EV6__I_CTL__SDE7__S>, r1; or in sde
	hw_mtpr	r1, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now back to business.
;
	and	r18, #3, r18			; clean 1st length encoding
	and	r9, #^x3F, r9			; clean 2nd length

;
; Probe the first write.
; Skip probe on second if length = 0.
; Perform longword alignment check on data mover addresses when length non-zero.
;
	hw_ldl/w r1, 0(r17)			; probe beginning of 1st
	beq	r9, call_pal__amovrm_check_ua	; skip probe if length=0

	bis	r19, r8, r1			; merge the two addresses
	and	r1, #3, r1			; get low bits for longword
	bne	r1, call_pal__amovrm_illop	; branch on alignment error
;
; Probe the memory-to-memory write.
;
	hw_ldl/w r1, (r8)			; probe beginning of move dest
	s4addq	r9, r8, r1			; calculate end of move dest
	hw_ldl/w r1, <-4&^xFFF>(r1)		; probe beginning of move
;
; Probe the memory-to-memory read.
;
	ldl	r1, (r19)			; probe beginning of move source
	s4addq	r9, r19, r1			; calculate end of move source
	ldl	r1, <-4&^xFFFF>(r1)		; probe beginning of move
;
; Now check to see whether the 1st destination is naturally aligned.
; Length encoding (binary) = 00 (byte), 01 (word), 10 (long), 11 (quad)
; Current state:
;	r17	1st destination
;	r18	1st length encoding cleaned 
;
call_pal__amovrm_check_ua:
	bis	r31, #1, p20			; get a 1
	sll	p20, r18, p20			; produce mask of correct number
	subq	p20, #1, p20			;	of 1s for check
	and	r17, p20, r1			; check for natural alignment
	bne	r1, call_pal__amovrm_ua		; branch for unaligned
;+
; Probes have completed and we are aligned. Based on 3.5.1. of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
;
; NOTE: ??? We need some mp test code here.
;-
	ldah	p4,<<amovrx_write_offset>+32768>@-16(r31)
	lda	p4,<<amovrx_write_offset> & ^xFFFF>(p4)
	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	lda	r3, amovrx_retry_count(r31)	; retry count
	beq	r18, call_pal__amovrmb		; branch for byte access
	subq	r18, #2, p20
	beq	p20, call_pal__amovrml		; branch for long access 
	subq	r18, #1, p20
	beq	p20, call_pal__amovrmw		; branch for word access
;
; Access is aligned quadword.
; Note that ev4 and ev5 copied r16 to r18 first, but this is not necessary
; because we are not repositioning the data. A retry uses the same data.
;
call_pal__amovrmq:
	stq	r16, (r17)			; store, no lock necessary
	br	r31, call_pal__amovrm_2nd	; on to next access
;
; Access is (aligned) byte (shouldn't really need an unaligned byte flow).
;
call_pal__amovrmb:
	bis	r31, #1, p20			; get a 1
	and	r17, #3, r1			; get low bits of address
	sll	p20, r1, p20			; byte zap value
	insbl	r16, r1, r18			; reposition data to r18
	bic	r17, #3, r2			; produce aligned long address

	ldl_l	r1, (r2)			; fetch data with lock
	zap	r1, p20, r1			; zap old data
	bis	r1, r18, r1			; merge new and old
;
; NOTE: ??? We should have some mp test code to cause occasional failures.
;
	stl_c	r1, (r2)			; store data with unlock
	blbc	r1, call_pal__amovrmb_retry	; failed => retry
	br	r31, call_pal__amovrm_2nd	; on to next access
;
; Access is aligned word.
;
call_pal__amovrmw:
	bis	r31, #3, p20			; get a 3
	and	r17, #3, r1			; get low bits of address
	sll	p20, r1, p20			; byte zap value
	inswl	r16, r1, r18			; reposition data to r18
	bic	r17, #3, r2			; produce aligned long address

	ldl_l	r1, (r2)			; fetch data with lock
	zap	r1, p20, r1			; zap old data
	bis	r1, r18, r1			; merge new and old
;
; NOTE: ??? We should have some mp test code to cause occasional failures.
;
	stl_c	r1, (r2)			; store data with unlock
	blbc	r1, call_pal__amovrmw_retry	; failed => retry
	br	r31, call_pal__amovrm_2nd	; on to next access
;
; Access is aligned longword.
; Note that ev4 and ev5 copied r16 to r18 first, but this is not necessary
; because we are not repositioning the data. A retry uses the same data.
;
call_pal__amovrml:
	stl	r16, (r17)			; store, no lock necessary
	br	r31, call_pal__amovrm_2nd	; on to next access
;
; Do a retry on the byte store.
; Register use:
;	r3		counter
;
call_pal__amovrmb_retry:
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrmb		; try again
	br	r31, call_pal__amovrm_fail	; fail -- count expired
;
; Do a retry on the word store.
; Register use:
;	r3		counter
;
call_pal__amovrmw_retry:
	subq	r31, #1, r3			; decrement the count
	bge	r3, call_pal__amovrmw		; try again
	br	r31, call_pal__amovrm_fail	; fail -- count expired
;+
; call_pal__amovrm_ua
;
; Unaligned amovrm
;
; We have probed the 1st half of the first datum. Now we need to probe
; the 2nd half.
;
; Current state:
;	p20	1 -- word
;		3 -- lw
;		7 -- qw
;	r16	source
;	r17	destination address
;	r18	length encoding
;-
call_pal__amovrm_ua:
	addq	r17, p20, r1			; get address of back end
	hw_ldl/w r1, (r1)			; probe
;
; Probes have been completed. Based on 3.5.1 of the SRM, we
; could conceivably get a 3rd level tnv on a mp machine. In that case,
; we ignore the tnv and write the otherwise good pte back into the tb.
;
; NOTE: ??? We need some mp test code here.
;
	ldah	p4,<<amovrx_write_offset>+32768>@-16(r31)
	lda	p4,<<amovrx_write_offset> & ^xFFFF>(p4)
	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault

	lda	r3, amovrx_retry_count(r31)	; retry count
	beq	r18, call_pal__amovrmbu		; branch for byte access
	subq	r18, #2, p20
	beq	p20, call_pal__amovrmlu		; branch for long access 
	subq	r18, #1, p20
	beq	p20, call_pal__amovrmwu		; branch for word access
;
; Access is unaligned quadword.
; Use macro with arguments reg, addr, scr1, scr2, scr3, lock, err, err1
;
call_pal__amovrmqu:
	STORE_UNALIGNED_QUAD r16, r17, r1, r2, p20, lock=1,-
		err=call_pal__amovrmqu_retry,-
		err1=call_pal__amovrmqu_retry1

	br	r31, call_pal__amovrm_2nd	; on to next acess
;
; Access is byte. (probably should never get here)
;
call_pal__amovrmbu:
	bis	r31, #1, p20			; get a 1
	and	r17, #3, r1			; get low bits of address
	sll	p20, r1, p20			; byte zap value
	insbl	r16, r1, r18			; resposition data to r18
	bic	r17, #3, r2			; produce aligned long address

	ldl_l	r1, (r2)			; fetch data with lock
	zap	r1, p20, r1			; zap old data
	bis	r1, r18, r1			; merge new and old
;
; NOTE: ??? We should have some mp test code to cause occasional failures.
;
	stl_c	r1, (r2)			; store data with unlock
	blbc	r1, call_pal__amovrmbu_retry	; failed => retry
	br	r31, call_pal__amovrm_2nd	; on to next access
;
; Access is unaligned word.
; Use macro with arguments reg, addr, scr1, scr2, scr3, lock, err, err1
;
call_pal__amovrmwu:
	STORE_UNALIGNED_WORD r16, r17, r1, r2, p20, lock=1,-
		err=call_pal__amovrmwu_retry,-
		err1=call_pal__amovrmwu_retry1

	br	r31, call_pal__amovrm_2nd	; on to next access
;
; Access is unaligned long.
; Use macro with arguments reg, addr, scr1, scr2, scr3, lock, err, err1
;
call_pal__amovrmlu:
	STORE_UNALIGNED_LONG r16, r17, r1, r2, p20, lock=1,-
		err=call_pal__amovrmlu_retry,-
		err1=call_pal__amovrmlu_retry1

	br	r31, call_pal__amovrm_2nd	; on to next access
;
; Do a retry on the unaligned quad store.
; Register use:
;	r3		counter
; There are two entries, one for the 1st half of the word, one for the 2nd.
;
call_pal__amovrmqu_retry:			; 1st half of the word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrmqu		; try again
	br	r31, call_pal__amovrm_fail	; fail -- count expired

call_pal__amovrmqu_retry1:			; 2nd half of word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrmqu		; try again
;
; Lock timer ran out on the 2nd half of store. We are not allowed
; to fail. So do an ordinary load/store.
;
	STORE_UNALIGNED_QUAD r16, r17, r1, r2, p20

	br	r31, call_pal__amovrm_2nd	; go on to next access
;
; Do a retry on the byte store.
; Register use:
;	r3		counter
;
call_pal__amovrmbu_retry:
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrmbu		; try again
	br	r31, call_pal__amovrm_fail	; fail -- count expired
;
; Do a retry on the unaligned word store.
; Register use:
;	r3		counter
; There are two entries, one for the 1st half of the word, one for the 2nd.
;
call_pal__amovrmwu_retry:			; 1st half of the word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrmwu		; try again
	br	r31, call_pal__amovrm_fail	; fail -- count expired

call_pal__amovrmwu_retry1:			; 2nd half of word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrmwu		; try again
;
; Lock timer ran out on the 2nd half of store. We are not allowed
; to fail. So do an ordinary load/store.
;
	STORE_UNALIGNED_WORD r16, r17, r1, r2, p20

	br	r31, call_pal__amovrm_2nd	; go on to next access
;
; Do a retry on the unaligned long store.
; Register use:
;	r3		counter
; There are two entries, one for the 1st half of the word, one for the 2nd.
;
call_pal__amovrmlu_retry:			; 1st half of the word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrmlu		; try again
	br	r31, call_pal__amovrm_fail	; fail -- count expired

call_pal__amovrmlu_retry1:			; 2nd half of word
	subq	r3, #1, r3			; decrement the count
	bge	r3, call_pal__amovrmlu		; try again
;
; Lock timer ran out on the 2nd half of store. We are not allowed
; to fail. So do an ordinary load/store.
;
	STORE_UNALIGNED_LONG r16, r17, r1, r2, p20

	br	r31, call_pal__amovrm_2nd	; go on to next access
;+
; First store has completed. Now go on to the memory-to-memory move.
; We know we have a transfer of 0 to 63 aligned longwords.
;-
call_pal__amovrm_2nd:
	beq	r9, call_pal__amovrm_done	; done when r9 reaches 0

	s4addq	r9, r31, r9			; convert long count to byte
	bis	r19, r8, r1			; merge both addresses
	bis	r1, r9, r1			; and the length
	and	r1, #7, r1			; see if both addrs and count
	bne	r1, call_pal__amovrm_long	;	are quad-aligned
;
; Data and count are multiples of quad, so we can move data as quads.
;
call_pal__amovrm_quad:
	ldq	r18, (r19)			; get source
	subq	r9, #8, r9			; decrement count
	lda	r19, 8(r19)			; bump source address
	stq	r18, (r8)			; store data
	lda	r8, 8(r8)			; bump destination addr
	bgt	r9, call_pal__amovrm_quad	; branch back if more to do
	br	r31, call_pal__amovrm_done	; otherwise, we are done
;
; Data or count not a multiple of quad, so move as longs.
;
call_pal__amovrm_long:
	ldl	r18, (r19)			; get source
	subq	r9, #4, r9			; decrement count
	lda	r19, 4(r19)			; bump source address
	stl	r18, (r8)			; store data
	lda	r8, 4(r8)			; bump destination addr
	bgt	r9, call_pal__amovrm_long	; branch back if more to do
	br	r31, call_pal__amovrm_done	; otherwise, we are done
;+
; Done. Either we did neither, or did both.
;
; Register use:
;	r9		0 => we succeeded, not 0 => we failed
;-
call_pal__amovrm_fail:
	subq	r31, #1, r9			; set failure

call_pal__amovrm_done:
	cmpeq	r9, r31, r18			; set success or failure

	hw_ldq/p r1, PT__R1(p_temp)		; restore r1
	hw_ldq/p r2, PT__R2(p_temp)		; restore r2
	hw_ldq/p r3, PT__R3(p_temp)		; restore r3
	hw_ldq/p r8, PT__R8(p_temp)		; restore r8
	hw_ldq/p r9, PT__R9(p_temp)		; restore r9

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret_stall (p23)			; return (stall for pvc)
;
; call_pal__amovrm_illop
;
; One/both of the addresses in r19 and r20 was not an aligned address.
;
call_pal__amovrm_illop:
	hw_ldq/p r1, PT__R1(p_temp)		; restore register
	hw_ldq/p r2, PT__R2(p_temp)		; restore register
	hw_ldq/p r3, PT__R3(p_temp)		; restore register
	hw_ldq/p r8, PT__R8(p_temp)		; restore r8
	hw_ldq/p r9, PT__R9(p_temp)		; restore r9

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap

	lda	p4, SCB__ILLOP(r31)		; illegal operand
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get pc back
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store fault pc (next pc)
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb offset
	br	r31, trap__post_km

	END_CALL_PAL

;+
; CALL_PAL__INSQHILR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of header
;	r17		contains address of entry
;
; Function:
;	If the secondary interlock is clear, INSQHILR inserts the entry
;	specified in r17 into the long self-relative queue following the header
;	specified in r16.
;
;	This instruction requires that the queue be memory resident, and
;	that the queue header and elements are quad-aligned. No checks
;	are made. If any of these requirements are not met, the
;	queue may be left in an unpredictable state and an illegal operand
;	fault may be reported.
;
; 	Faults go through call_pal__queue_fault_res. Unaligns go through
;	the unalign exception and call_pal__queue_unaligned_from_pal.
;
; Register use:
;
; Exit state:
;	r0	-1	failed to get secondary interlock
;		 0	queue was not empty
;		 1	queue was empty
;-
	START_CALL_PAL <INSQHILR>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p r3, PT__R3(p_temp)		; need 1 temp

queue_fault_res_offset = <<call_pal__queue_fault_res - trap__pal_base>>

	ldah	p4,<<queue_fault_res_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_res_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	lda	p20, queue_retry_count(r31)	; set retry count
;
; Now try to acquire the lock
;
call_pal__insqhilr_lock:
	ldl_l	r0, (r16)			; try to get H, interlocked
	blbs	r0, call_pal__queue_busy_res	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stl_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__insqhilr_retry	; retry on failure

	mb					; per SRM

	CONT_CALL_PAL <INSQHILR>
;
; Got the lock. Now do the queue operation.
; Current state:
;	r0	S-H
;	r16	H
;	r17	E
;
	subl	r16, r17, r3			; H-E
	stl	r3, 4(r17)			; (E+4) <- H-E
	addl	r3, r0, r3			; S-E
	stl	r3, (r17)			; (E)	<- S-E

	subl	r31, r3, r3			; E-S
	addl	r16, r0, p20			; S
	stl	r3, 4(p20)			; (S+4)	<- E-S

	mb

	subl	r17, r16, r3			; E-H
	stl	r3, (r16)			; (H)	<- E-H, clear secondary

	cmpeq	r0, #0, r0			; if S-H=0 => queue was empty

	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;+
; Check retry count and try again if not zero.
;-
call_pal__insqhilr_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__insqhilr_lock	; branch to try again
	br	r31, call_pal__queue_busy_res	; branch for queue busy

;+
; Queue busy. Resident queue.
; Enter for secondary lock set and for retry exceeded.
; Clear lock for secondary lock failure case, and return r0 = -1 to user.
; Also restore saved register.
;-
call_pal__queue_busy_res:
	stl_c	r0, (r16)			; release primary lock

	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get pc back
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p r3, PT__R3(p_temp)		; restore register
	subq	r31, #1, r0			; return with -1
	hw_ret	(p23)				; return

;+
; call_pal__queue_fault_res
;
; A mm fault occurred while writing for queue resident instructions.
; We leave the queue in what partial state it may be in, and post an
; invalid operand trap.
; Current state:
;	p5		mm_stat
;	p6		va
;	p23		fault pc (from PALcode flow)
;
;	r3		needs to be restored
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__FAULT_SCB	SCB offset
;	PT__CALL_PAL_PC	next pc
;
;	PT__R25		saved r25
;	PT__R26		saved r26
;-
	PVC_JSR	pal_mm_dispatch, dest=1
call_pal__queue_fault_res:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26
	hw_ldq/p r3, PT__R3(p_temp)		; restore r3

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get pc back
	lda	p4, SCB__ILLOP(r31)		; illegal operand
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store fault pc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb offset
	br	r31, trap__post_km		; post

;+
; call_pal__queue_unaligned_from_pal
;
; Entered for RESIDENT QUEUE operations that are unaligned. These are
; the only operations that can cause an unalign exception. We will
; turn this into an ILLOP and post. The state of the queue is unknown.
;
; Current state:
;	p23	exc_addr
;	p7	exc_sum
;	r3	needs to be restored
;-
call_pal__queue_unaligned_from_pal:
	hw_ldq/p r3, PT__R3(p_temp)		; restore r3

	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get pc back
	lda	p4, SCB__ILLOP(r31)		; illegal operand
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store fault pc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb offset
	br	r31, trap__post_km		; post

	END_CALL_PAL

;+
; CALL_PAL__INSQTILR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of header
;	r17		contains address of entry
;
; Function:
;	If the secondary interlock is clear, INSQTILR inserts the entry
;	specified in r17 into the long self-relative queue preceding the header
;	specified in r16.
;
;	This instruction requires that the queue be memory resident, and
;	that the queue header and elements are quad-aligned. No checks
;	are made. If any of these requirements are not met, the
;	queue may be left in an unpredictable state and an illegal operand
;	fault may be reported.
;
; 	Faults go through call_pal__queue_fault_res. Unaligns go through
;	the unalign exception and call_pal__queue_unaligned_from_pal.
;
; Register use:
;
; Exit state:
;	r0	-1	failed to get secondary interlock
;		 0	queue was not empty
;		 1	queue was empty
;-
	START_CALL_PAL <INSQTILR>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p r3, PT__R3(p_temp)		; need 1 temp

	ldah	p4,<<queue_fault_res_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_res_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	lda	p20, queue_retry_count(r31)	; set retry count
;
; Note: We do a ldq_l to get the backward link as well as the lock.
;
call_pal__insqtilr_lock:
	ldq_l	r0, (r16)			; try to get H, interlocked
	blbs	r0, call_pal__queue_busy_res	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stl_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__insqtilr_retry	; retry on failure

	mb					; per SRM

	CONT_CALL_PAL <INSQTILR>

	beq	r0, call_pal__insqtilr_was_empty; branch on queue empty
;
; Current state:
;	r0<31:0> 	S-H
;	r0<63:32>	P-H
;	r16	H
;	r17	E
;
	sra	r0, #32, p20			; P-H
	addl	r16, p20, p20			; P

	subl	r16, r17, r3			; H-E
	stl	r3, (r17)			; (E)	<- H-E
	subl	p20, r17, r3			; P-E
	stl	r3, 4(r17)			; (E+4)	<- P-E
	subl	r31, r3, r3			; E-P
	stl	r3, (p20)			; (P)	<- E-P
	subl	r17, r16, r3			; E-H
	stl	r3, 4(r16)			; (H+4)	<- E-H

	mb

	stl	r0, (r16)			; (H)	<- S-H, clear secondary

	bis	r31, #0, r0			; queue was not empty

	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;
; Flow for queue was empty.
;
; Current state:
;	r0<31:0> 	S-H = 0
;	r0<63:32>	P-H = 0
;	r16		H
;	r17		E
;
; NOTE: We need some mp debug code ???
;
call_pal__insqtilr_was_empty:
	subl	r16, r17, r3			; H-E
	stl	r3, (r17)			; (E)	<- H-E	
	stl	r3, 4(r17)			; (E+4)	<- H-E
	subl	r17, r16, r3			; E-H
	stl	r3, 4(16)			; (H+4)	<- E-H

	mb

	stl	r3, (r16)			; (H)	<- E-H, clear secondary

	bis	r31, #1, r0			; queue was empty

	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;+
; Check retry count and try again if not zero.
;-
call_pal__insqtilr_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__insqtilr_lock	; branch to try again
	br	r31, call_pal__queue_busy_res	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__INSQHIQR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of header
;	r17		contains address of entry
;
; Function:
;	If the secondary interlock is clear, INSQHIQR inserts the entry
;	specified in r17 into the quad self-relative queue following the header
;	specified in r16.
;
;	This instruction requires that the queue be memory resident, and
;	that the queue header and elements are octa-aligned. No checks
;	are made. If any of these requirements are not met, the
;	queue may be left in an unpredictable state and an illegal operand
;	fault may be reported.
;
; 	Faults go through call_pal__queue_fault_res. Unaligns go through
;	the unalign exception and call_pal__queue_unaligned_from_pal.
;
; Register use:
;
; Exit state:
;	r0	-1	failed to get secondary interlock
;		 0	queue was not empty
;		 1	queue was empty
;-
	START_CALL_PAL <INSQHIQR>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p r3, PT__R3(p_temp)		; need 1 temp

	ldah	p4,<<queue_fault_res_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_res_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	lda	p20, queue_retry_count(r31)	; set retry count
;
; Now try to acquire the lock
;
call_pal__insqhiqr_lock:
	ldq_l	r0, (r16)			; try to get H, interlocked
	blbs	r0, call_pal__queue_busy_res	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stq_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__insqhiqr_retry	; retry on failure

	mb					; per SRM

	CONT_CALL_PAL <INSQHIQR>
;
; Got the lock. Now do the queue operation.
; Current state:
;	r0	S-H
;	r16	H
;	r17	E
;
	subq	r16, r17, r3			; H-E
	stq	r3, 8(r17)			; (E+8) <- H-E
	addq	r3, r0, r3			; S-E
	stq	r3, (r17)			; (E)	<- S-E

	subq	r31, r3, r3			; E-S
	addq	r16, r0, p20			; S
	stq	r3, 8(p20)			; (S+8)	<- E-S

	mb

	subq	r17, r16, r3			; E-H
	stq	r3, (r16)			; (H)	<- E-H, clear secondary

	cmpeq	r0, #0, r0			; if S-H=0 => queue was empty

	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;+
; Check retry count and try again if not zero.
;-
call_pal__insqhiqr_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__insqhiqr_lock	; branch to try again
	br	r31, call_pal__queue_busy_res	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__INSQTIQR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		contains address of header
;	r17		contains address of entry
;
; Function:
;	If the secondary interlock is clear, INSQTIQR inserts the entry
;	specified in r17 into the quad self-relative queue preceding the header
;	specified in r16.
;
;	This instruction requires that the queue be memory resident, and
;	that the queue header and elements are octa-aligned. No checks
;	are made. If any of these requirements are not met, the
;	queue may be left in an unpredictable state and an illegal operand
;	fault may be reported.
;
; 	Faults go through call_pal__queue_fault_res. Unaligns go through
;	the unalign exception and call_pal__queue_unaligned_from_pal.
;
; Register use:
;
; Exit state:
;	r0	-1	failed to get secondary interlock
;		 0	queue was not empty
;		 1	queue was empty
;-
	START_CALL_PAL <INSQTIQR>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p r3, PT__R3(p_temp)		; need 1 temp

	ldah	p4,<<queue_fault_res_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_res_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	lda	p20, queue_retry_count(r31)	; set retry count
;
; Now try to acquire the lock
;
call_pal__insqtiqr_lock:
	ldq_l	r0, (r16)			; try to get H, interlocked
	blbs	r0, call_pal__queue_busy_res	; entry already locked
	bis	r0, #1, r3			; set low bit for secondary lock
	stq_c	r3, (r16)			; try to set secondary lock
	blbc	r3, call_pal__insqtiqr_retry	; retry on failure

	mb					; per SRM

	CONT_CALL_PAL <INSQTIQR>

	beq	r0, call_pal__insqtiqr_was_empty; branch on queue empty
;
; Current state:
;	r0	S-H
;	r16	H
;	r17	E
;
	ldq	p20, 8(r16)			; P-H
	addq	r16, p20, p20			; P

	subq	r16, r17, r3			; H-E
	stq	r3, (r17)			; (E)	<- H-E
	subq	p20, r17, r3			; P-E
	stq	r3, 8(r17)			; (E+8)	<- P-E
	subq	r31, r3, r3			; E-P
	stq	r3, (p20)			; (P)	<- E-P
	subq	r17, r16, r3			; E-H
	stq	r3, 8(r16)			; (H+8)	<- E-H

	mb

	stq	r0, (r16)			; (H)	<- S-H, clear secondary

	bis	r31, #0, r0			; queue was not empty

	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;
; Flow for queue was empty.
;
; Current state:
;	r0	S-H
;	r16	H
;	r17	E
;
; NOTE: We need some mp debug code ???
;
call_pal__insqtiqr_was_empty:
	subq	r16, r17, r3			; H-E
	stq	r3, (r17)			; (E)	<- H-E	
	stq	r3, 8(r17)			; (E+8)	<- H-E
	subq	r17, r16, r3			; E-H
	stq	r3, 8(16)			; (H+8)	<- E-H

	mb

	stq	r3, (r16)			; (H)	<- E-H, clear secondary

	bis	r31, #1, r0			; queue was empty

	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;+
; Check retry count and try again if not zero.
;-
call_pal__insqtiqr_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__insqtiqr_lock	; branch to try again
	br	r31, call_pal__queue_busy_res	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__REMQHILR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		address of the header
;
; Function:
;	If the secondary interlock is clear, REMQHILR removes from the
;	long self-relative queue the entry following the header, pointed to
;	by r16, and the address of the removed entry is returned in r1.
;
;	This instruction requires that the queue be memory resident, and
;	that the queue header and elements are octa-aligned. No checks
;	are made. If any of these requirements are not met, the
;	queue may be left in an unpredictable state and an illegal operand
;	fault may be reported.
;
; 	Faults go through call_pal__queue_fault_res. Unaligns go through
;	the unalign exception and call_pal__queue_unaligned_from_pal.
;
; Register use:
;
; Exit state:
;	r0	-1 	failed to obtain secondary interlock
;		 0	queue was empty
;		 1	queue now not empty
;		 2	queue now empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQHILR>
	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p r3, PT__R3(p_temp)		; need 1 temp

	ldah	p4,<<queue_fault_res_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_res_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <REMQHILR>
;
; Now try to acquire the lock
;
call_pal__remqhilr_lock:
	ldl_l	r0, (r16)				; try to get H, locked
	beq	r0, call_pal__queue_was_empty_res	; done, queue was empty
	blbs	r0, call_pal__queue_busy_res		; entry already locked
	bis	r0, #1, r3				; for secondary lock
	stl_c	r3, (r16)				; try to set secondary
	blbc	r3, call_pal__remqhilr_retry		; retry on failure

	mb						; per SRM
;
; Current state:
;	r0	E-H
;	r16	H
;
	addl	r16, r0, r1			; E
	ldl	r3, (r1)			; S-E
	addl	r3, r1, p20			; S
	subl	r16, p20, r3			; H-S
	stl	r3, 4(p20)			; (S+4) <- H-S

	mb

	subl	r31, r3, r3			; S-H
	stl	r3, (r16)			; (H)	<- S-H, clear secondary

	cmpeq	r3, r31, r0			; if S-H=0 => queue now empty
	addq	r0, #1, r0			; 1=>not empty, 2=>now empty

	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;+
; Check retry count and try again if not zero.
;-
call_pal__remqhilr_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__remqhilr_lock	; branch to try again
	br	r31, call_pal__queue_busy_res	; branch for queue busy

;+
; Queue already empty. Resident queue.
;
; Return to user with status of r0=0.
; Current state:
;	r0	0=>queue already empty
;	r16	H
; Note: Previous implementations claimed stl_c not required.
;-
call_pal__queue_was_empty_res:
	stl_c	r0, (r16)			; release primary lock
	bis	r31, r31, r0			; reset r0=0
	bis	r16, r31, r1			; 'removed' entry = H

	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__REMQTILR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		address of the header
;
; Function:
;	If the secondary interlock is clear, REMQTILR removes from the
;	long self-relative queue the entry preceding the header, pointed to
;	by r16, and the address of the removed entry is returned in r1.
;
;	This instruction requires that the queue be memory resident, and
;	that the queue header and elements are octa-aligned. No checks
;	are made. If any of these requirements are not met, the
;	queue may be left in an unpredictable state and an illegal operand
;	fault may be reported.
;
; 	Faults go through call_pal__queue_fault_res. Unaligns go through
;	the unalign exception and call_pal__queue_unaligned_from_pal.
;
; Register use:
;
; Exit state:
;	r0	-1 	failed to obtain secondary interlock
;		 0	queue was empty
;		 1	queue now not empty
;		 2	queue now empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQTILR>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p r3, PT__R3(p_temp)		; need 1 temp

	ldah	p4,<<queue_fault_res_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_res_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <REMQTILR>

;
; Note: We do a ldq_l to get the backward link as well as the lock.
;
call_pal__remqtilr_lock:
	ldq_l	r0, (r16)				; try to get H, locked
	beq	r0, call_pal__queue_was_empty		; done, queue was empty
	blbs	r0, call_pal__queue_busy		; entry already locked
	bis	r0, #1, r3				; for secondary lock
	stl_c	r3, (r16)				; try to get secondary
	blbc	r3, call_pal__remqtilr_retry		; retry on failure

	mb						; per SRM
;
; Current state:
;	r0<31:0> 	S-H
;	r0<63:32>	E-H
;	r16		H
;
	sra	r0, #32, p20			; E-H
	addl	r16, p20, r1			; E
	ldl	r3, 4(r1)			; P-E
	addl	r1, r3, p20			; P
	subl	p20, r16, r3			; P-H
	stl	r3, 4(r16)			; (H+4)	<- P-H
	subl	r16, p20, r3			; H-P
	beq	r3, call_pal__remqtilr_now_empty; branch if now empty

	stl	r3, (p20)			; (P)	<- H-P

	mb

	stl	r0, (r16)			; (H)	<- S-H, clear secondary

	bis	r31, #1, r0			; 1=>not empty
	br	r31, call_pal__remqtilr_done	; done
;
; Last entry in the queue
;
call_pal__remqtilr_now_empty:
	mb

	stl	r3, (r16)			; (H)	<- 0

	bis	r31, #2, r0			; 2=>queue now empty

call_pal__remqtilr_done:
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;+
; Check retry count and try again if not zero.
;-
call_pal__remqtilr_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__remqtilr_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__REMQHIQR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		address of the header
;
; Function:
;	If the secondary interlock is clear, REMQHIQR removes from the
;	quad self-relative queue the entry following the header, pointed to
;	by r16, and the address of the removed entry is returned in r1.
;
;	This instruction requires that the queue be memory resident, and
;	that the queue header and elements are octa-aligned. No checks
;	are made. If any of these requirements are not met, the
;	queue may be left in an unpredictable state and an illegal operand
;	fault may be reported.
;
; 	Faults go through call_pal__queue_fault_res. Unaligns go through
;	the unalign exception and call_pal__queue_unaligned_from_pal.
;
; Register use:
;
; Exit state:
;	r0	-1 	failed to obtain secondary interlock
;		 0	queue was empty
;		 1	queue now not empty
;		 2	queue now empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQHIQR>

	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p r3, PT__R3(p_temp)		; need 1 temp

	ldah	p4,<<queue_fault_res_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_res_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <REMQHIQR>
;
; Now try to acquire the lock
;
call_pal__remqhiqr_lock:
	ldq_l	r0, (r16)				; try to get H, locked
	beq	r0, call_pal__queue_was_empty_res	; done, queue was empty
	blbs	r0, call_pal__queue_busy_res		; entry already locked
	bis	r0, #1, r3				; for secondary lock
	stq_c	r3, (r16)				; try to set secondary
	blbc	r3, call_pal__remqhiqr_retry		; retry on failure

	mb						; per SRM
;
; Current state:
;	r0	E-H
;	r2	1, lock acquired
;	r16	H
;
	addq	r16, r0, r1			; E
	ldq	r3, (r1)			; S-E
	addq	r3, r1, p20			; S
	subq	r16, p20, r3			; H-S
	stq	r3, 8(p20)			; (S+8) <- H-S

	mb

	subq	r31, r3, r3			; S-H
	stq	r3, (r16)			; (H)	<- S-H, clear secondary

	cmpeq	r3, r31, r0			; if S-H=0 => queue now empty
	addq	r0, #1, r0			; 1=>not empty, 2=>now empty

	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;+
; Check retry count and try again if not zero.
;-
call_pal__remqhiqr_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__remqhiqr_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy
	END_CALL_PAL

;+
; CALL_PAL__REMQTIQR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
;	r16		address of the header
;
; Function:
;	If the secondary interlock is clear, REMQTIQR removes from the
;	quad self-relative queue the entry preceding the header, pointed to
;	by r16, and the address of the removed entry is returned in r1.
;
;	This instruction requires that the queue be memory resident, and
;	that the queue header and elements are octa-aligned. No checks
;	are made. If any of these requirements are not met, the
;	queue may be left in an unpredictable state and an illegal operand
;	fault may be reported.
;
; 	Faults go through call_pal__queue_fault_res. Unaligns go through
;	the unalign exception and call_pal__queue_unaligned_from_pal.
;
; Register use:
;
; Exit state:
;	r0	-1 	failed to obtain secondary interlock
;		 0	queue was empty
;		 1	queue now not empty
;		 2	queue now empty
;	r1		address of entry removed
;-
	START_CALL_PAL <REMQTIQR>


	hw_stq/p p23, PT__CALL_PAL_PC(p_temp)	; save user pc
	hw_stq/p r3, PT__R3(p_temp)		; need 1 temp

	ldah	p4,<<queue_fault_res_offset>+32768>@-16(r31)
	lda	p4,<<queue_fault_res_offset> & ^xFFFF>(p4)

	hw_stq/p p4, PT__TRAP(p_temp)		; offset for dfault
	lda	p20, queue_retry_count(r31)	; set retry count

	CONT_CALL_PAL <REMQTIQR>
;
; Now try to acquire the lock
;
call_pal__remqtiqr_lock:
	ldq_l	r0, (r16)				; try to get H, locked
	beq	r0, call_pal__queue_was_empty_res	; done, queue was empty
	blbs	r0, call_pal__queue_busy_res		; entry already locked
	bis	r0, #1, r3				; for secondary lock
	stq_c	r3, (r16)				; try to set secondary
	blbc	r3, call_pal__remqtiqr_retry		; retry on failure

	mb						; per SRM
;
; Current state:
;	r0	S-H
;	r16	H
;
	ldq	r3, 8(r16)			; E-H
	addq	r16, r3, r1			; E
	ldq	r3, 8(r1)			; P-E
	addq	r1, r3, p20			; P
	subq	p20, r16, r3			; P-H
	stq	r3, 8(r16)			; (H+8)	<- P-H
	subq	r16, p20, r3			; H-P
	beq	r3, call_pal__remqtiqr_now_empty; branch if now empty

	stq	r3, (p20)			; (P)	<- H-P

	mb

	stq	r0, (r16)			; (H)	<- S-H, clear secondary

	bis	r31, #1, r0			; 1=>not empty
	br	r31, call_pal__remqtiqr_done	; done
;
; Last entry in the queue
;
call_pal__remqtiqr_now_empty:
	mb

	stq	r3, (r16)			; (H)	<- 0

	bis	r31, #2, r0			; 2=>queue now empty

call_pal__remqtiqr_done:
	hw_ldq/p r3, PT__R3(p_temp)		; restore
	hw_stq/p r31, PT__TRAP(p_temp)		; clear trap catcher
	hw_ldq/p p23, PT__CALL_PAL_PC(p_temp)	; get back pc

	hw_ret	(p23)				; return
;+
; Check retry count and try again if not zero.
;-
call_pal__remqtiqr_retry:
	subq	p20, #1, p20			; decrement count
	bne	p20, call_pal__remqtiqr_lock	; branch to try again
	br	r31, call_pal__queue_busy	; branch for queue busy

	END_CALL_PAL

;+
; CALL_PAL__GENTRAP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The GENTRAP instruction is provided for run-time software conditions.
;	Initiate gentrap exception with new_mode=kernel. Switch to
;	kernel mode, push r2..r7, the update PC, and PS on the kernel stack.
;	Dispatch to the address in Gentrap SCB vector.
;
; Exit state:
;	On exit to trap__post_km
;	PT__FAULT_PC		nextpc
;	PT__FAULT_SCB		SCB vector
;-
	START_CALL_PAL <GENTRAP>

	lda	p4, SCB__TRAP(r31)		; SCB vector
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; save nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; save SCB vector
	br	r31, trap__post_km		; post the trap

	END_CALL_PAL

.if ne clrmap
;+
; CALL_PAL__CLRMAP
;
; Entry:
;       p23             pc of instruction following the call_pal instruction
;
; Function:
;	Clear the register map
;
; Current state:
;	p21	p_temp
;	p22	p_misc
;	p23	linkage register
;
; Exit state:
;       restored
;-

PAL_FUNC__CLRMAP        = ^xAC

        START_CALL_PAL <CLRMAP>

  .if eq ev6_p1
	hw_ret	(p23)				; just return if not p1
  .iff

        hw_mfpr p7, EV6__PAL_BASE		; (4,0L) pal base
	hw_ldq/p p6, PT__WHAMI(p_temp)		; get my cpu number
	s8addq	p6, p7, p7			; (cpu*8)+base
	hw_stq/p p_temp, ^x30(p7)		; p_temp at ^x30+(cpu*8)+base

        hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
        hw_stq/p p_misc, CNS__P_MISC(p4)	; store p_misc
        hw_stq/p p23, CNS__P23(p4)		; store call_pal linkage
	hw_mfpr	p5, EV6__SIRR			; (4,0L) get sirr

	hw_stq/p p5, CNS__SIRR(p4)		; store sirr
	sll	p6, #EV6__SIRR__SIR__S, p6	; cpu number into place
	hw_mtpr	p6, EV6__SIRR			; (4,0L) save cpu number
	bis	r31, r31, r31			; pad fetch block

        hw_mfpr  p5, EV6__I_CTL                 ; (4,0L) get i_ctl

	CONT_CALL_PAL <CLRMASK>

        ASSUME_FETCH_BLOCK

	GET_32CONS p20, <^x804000>, r31		; hack for ev6isp
	bic	p5, p20, p5			; clear unpredictables
        hw_stq/p p5, CNS__I_CTL(p4)		; store i_ctl

        hw_stq/p r0, CNS__R0(p4)           	; save r0
        hw_stq/p r1, CNS__R1(p4)           	; save r1
        hw_stq/p r2, CNS__R2(p4)           	; save r2
        hw_mfpr r2, EV6__I_CTL                  ; (4,0L) get i_ctl

        bis     p4, r31, r1                     ; impure base into r1
        bic     r2, #<2@EV6__I_CTL__SDE__S>, r2 ; zap sde
        hw_stq/p r3, CNS__R3(p4)           	; save r3
        hw_stq/p r8, CNS__R8(r1)           	; save gpr

        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) write i_ctl
        hw_stq/p r9, CNS__R9(r1)           	; save gpr
        hw_stq/p r10, CNS__R10(r1)         	; save gpr
        hw_stq/p r11, CNS__R11(r1)         	; save gpr

        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) stall outside IQ
        hw_stq/p r12, CNS__R12(r1)         	; save gpr
        hw_stq/p r13, CNS__R13(r1)         	; save gpr
        hw_stq/p r14, CNS__R14(r1)         	; save gpr

        hw_stq/p r15, CNS__R15(r1)         	; buffer block 1 -- save gpr
        hw_stq/p r16, CNS__R16(r1)         	; save gpr
        hw_stq/p r17, CNS__R17(r1)         	; save gpr
        hw_stq/p r18, CNS__R18(r1)         	; save gpr

        hw_stq/p r19, CNS__R19(r1)         	; buffer block 2 -- save gpr
        hw_stq/p r24, CNS__R24(r1)         	; save gpr
        hw_stq/p r25, CNS__R25(r1)         	; save gpr
        hw_stq/p r26, CNS__R26(r1)         	; save gpr

        hw_stq/p r27, CNS__R27(r1)         	; buffer block 3 -- save gpr
        hw_stq/p r28, CNS__R28(r1)         	; save gpr
        hw_stq/p r29, CNS__R29(r1)         	; save gpr
        hw_stq/p r30, CNS__R30(r1)         	; save gpr

        hw_stq/p r4, CNS__R4(r1)           	; save gpr
        hw_stq/p r5, CNS__R5(r1)           	; save gpr
        hw_stq/p r6, CNS__R6(r1)           	; save gpr
        hw_stq/p r7, CNS__R7(r1)           	; save gpr

        hw_stq/p r20, CNS__R20(r1)         	; save gpr
        hw_stq/p r21, CNS__R21(r1)         	; save gpr
        hw_stq/p r22, CNS__R22(r1)         	; save gpr
        hw_stq/p r23, CNS__R23(r1)         	; save gpr

        hw_stq/p r31, CNS__R31(r1)         	; save gpr
	bis	r31, r31, r31
	bis	r31, r31, r31
	mb					; dll -- add mb

        hw_mfpr r0, EV6__I_CTL			; (4,0L) get i_ctl
        or	r31, #EV6__I_CTL__SBE__M, r3	; form SBE mask
        sll 	r3, #EV6__I_CTL__SBE__S, r3
        bic	r0, r3, r0			; zap SBE bits

clr_map_1_offset = <clr_map_1_done - clr_map_1>

        hw_mtpr r0, EV6__I_CTL			; (4,0L) turn off stream buffers
	br	r2, clr_map_1
clr_map_1:
	addq	r2, #<clr_map_1_offset+1>, r2	; jump past in palmode
	bsr	r31, .				; push prediction stack

        hw_mtpr r31, EV6__IC_FLUSH		; flush icache
	bne	r31, .
	PVC_JSR	clr_map_1			; synch up
	hw_ret_stall (r2)			; pop prediction stack
	PVC_JSR	clr_map_1, dest=1

        .ALIGN 6 ,<^X47FF041F>
clr_map_1_done:
        hw_mtpr r0, EV6__CLR_MAP      		; (4-7,0L) clear map
	NOP
	NOP
	NOP

	NOP					; dll - add quiet stuff
	NOP
	NOP
	NOP

clr_map_2_offset = <real_code_1 - clr_map_2>

	NOP
	NOP
	bsr	r31, .				; push prediction stack
	lda	r3, <clr_map_2_offset>(r31)

	br	r2, clr_map_2
clr_map_2:
	addq	r2, r3, r2
	addq	r2, #<1>, r2			; jump past in palmode
	PVC_JSR	clr_map_2			; synch up
	hw_ret_stall (r2)			; pop prediction stack
	PVC_JSR	clr_map_2, dest=1

	NOP					; dll - add more quiet stuff
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP
;
; Now do the mapper again
;
        .ALIGN 6 ,<^X47FF041F>
real_code_1:
        addq    r31,r31,r0
        addq    r31,r31,r1
        addq    r31,r31,r2
        addq    r31,r31,r3

        addq    r31,r31,r4
        addq    r31,r31,r5
        addq    r31,r31,r6
        addq    r31,r31,r7

        addq    r31,r31,r8
        addq    r31,r31,r9
        addq    r31,r31,r10
        addq    r31,r31,r11

        addq    r31,r31,r12
        addq    r31,r31,r13
        addq    r31,r31,r14
        addq    r31,r31,r15

        addq    r31,r31,r16
        addq    r31,r31,r17
        addq    r31,r31,r18
        addq    r31,r31,r19

        addq    r31,r31,r20
        addq    r31,r31,r21
        addq    r31,r31,r22
        addq    r31,r31,r23

        addq    r31,r31,r24
        addq    r31,r31,r25
        addq    r31,r31,r26
        addq    r31,r31,r27

        addq    r31,r31,r28
        addq    r31,r31,r29
        addq    r31,r31,r30
        addq    r31,r31,r0			; done 32

        addq    r31,r31,r0
        addq    r31,r31,r1
        addq    r31,r31,r2
        addq    r31,r31,r3

        addq    r31,r31,r4
        addq    r31,r31,r5
        addq    r31,r31,r6
        addq    r31,r31,r7

        addq    r31,r31,r8
        addq    r31,r31,r9
        addq    r31,r31,r10
        addq    r31,r31,r11

        addq    r31,r31,r12
        addq    r31,r31,r13
        addq    r31,r31,r14
        addq    r31,r31,r15

        addq    r31,r31,r16
        addq    r31,r31,r17
        addq    r31,r31,r18
        addq    r31,r31,r19

        addq    r31,r31,r20
        addq    r31,r31,r21
        addq    r31,r31,r22
        addq    r31,r31,r23

        addq    r31,r31,r24
        addq    r31,r31,r25
        addq    r31,r31,r26
        addq    r31,r31,r27

        addq    r31,r31,r28
        addq    r31,r31,r29
        addq    r31,r31,r30
	addq	r31,r31,r0			; done 64

        addq    r31,r31,r0
        addq    r31,r31,r1
        addq    r31,r31,r2
        addq    r31,r31,r3

        addq    r31,r31,r4
        addq    r31,r31,r5
        addq    r31,r31,r6
        addq    r31,r31,r7

        addq    r31,r31,r8
        addq    r31,r31,r9
        addq    r31,r31,r10
        addq    r31,r31,r11

        addq    r31,r31,r12
        addq    r31,r31,r13
        addq    r31,r31,r14
        addq    r31,r31,r15			; done 80
;
; Now turn on mapper source enables
;
        hw_mtpr r31,EV6__ITB_IA         ; (4,0L) flush ITB, enable source map
        hw_mtpr r31,EV6__DTB_IA         ; (7,1L) flush DTB
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
;
; Create a stall outside the IQ until the mtpr EV6__ITB_IA retires.
;
	PVC_VIOLATE <21>
        hw_mtpr r31,<EV6__MM_STAT ! ^x90>    ; (4&7,0L) IQ stall.
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; 1st buffer fetch block. IMAP stall
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; 2nd buffer fetch block. FMAP stall
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
;
; Now map the shadow registers
;
        lda     r0,^x0086(r31)          ; load I_CTL.....
	bis	r0, r0, r0
	bis	r0, r0, r0
	bis	r0, r0, r0

        hw_mtpr r0,EV6__I_CTL           ; .....SDE=2, IC_EN=3 (SCRBRD=4)
	bis	r0, r0, r0
	bis	r0, r0, r0
	bis	r0, r0, r0

        hw_mtpr r0,EV6__I_CTL           ; .....SDE=2, IC_EN=3 (SCRBRD=4)
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; 1st buffer fetch block for above map-stall
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; 2nd buffer fetch block for above map-stall
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; need 3rd buffer fetch block to get sde bit
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
;
; map shadow registers
;
        addq    r31,r31,r4
        addq    r31,r31,r5
        addq    r31,r31,r6
        addq    r31,r31,r7

        addq    r31,r31,r20
        addq    r31,r31,r21
        addq    r31,r31,r22
        addq    r31,r31,r23
;
; Now restore stuff
;	p21 = p_temp
;	p22 = p_misc
;	p23 = linkage
;
;	(pal_base+^x30+(8*cpu)) = p_temp
;	CNS__P_MISC =  p_misc
;	CNS__P23    = linkage
;	CNS__I_CTL  = i_ctl
;	CNS__SIRR   = sirr value
;
	ASSUME_FETCH_BLOCK

	hw_mfpr	p6, EV6__SIRR			; (4,0L) get cpu number
	srl	p6, #EV6__SIRR__SIR__S, p6	; shift into place
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mfpr	r2, EV6__PAL_BASE		; (4,0L) pal base
	s8addq	p6, r2, r2			; (cpu*8)+base
	hw_ldq/p p_temp, ^x30(r2)		; restore p_temp
	hw_ldq/p r1, PT__IMPURE(p_temp)		; get base of impure area

        hw_ldq/p p_misc, CNS__P_MISC(r1)	; restore p_misc
        hw_ldq/p p23, CNS__P23(r1)		; restore call_pal linkage
	hw_ldq/p p5, CNS__SIRR(r1)		; get SIRR back
	hw_mtpr	p5, EV6__SIRR			; (4,0L) restore it

        hw_ldq/p r0, CNS__I_CTL(r1)		; get saved i_ctl
        bic     r0, #<2@EV6__I_CTL__SDE__S>, r2 ; zap sde
        hw_ldq/p r8, CNS__R8(r1)           	; restore gpr
        hw_ldq/p r9, CNS__R9(r1)           	; restore gpr

        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) write i_ctl
        hw_ldq/p r10, CNS__R10(r1)         	; restore gpr
        hw_ldq/p r11, CNS__R11(r1)         	; restore gpr
        hw_ldq/p r12, CNS__R12(r1)         	; restore gpr

        hw_mtpr r2, EV6__I_CTL                  ; stall outside IQ
        hw_ldq/p r13, CNS__R13(r1)         	; restore gpr
        hw_ldq/p r14, CNS__R14(r1)         	; restore gpr
        hw_ldq/p r15, CNS__R15(r1)         	; restore gpr

        hw_ldq/p r16, CNS__R16(r1)         	; buffer block 1 -- restore gpr
        hw_ldq/p r17, CNS__R17(r1)         	; restore gpr
        hw_ldq/p r18, CNS__R18(r1)         	; restore gpr
        hw_ldq/p r19, CNS__R19(r1)         	; restore gpr

        hw_ldq/p r24, CNS__R24(r1)         	; buffer block 2 -- restore gpr
        hw_ldq/p r25, CNS__R25(r1)         	; restore gpr
        hw_ldq/p r26, CNS__R26(r1)         	; restore gpr
        hw_ldq/p r27, CNS__R27(r1)         	; restore gpr

        hw_ldq/p r28, CNS__R28(r1)         	; buffer block 3 -- restore gpr
        hw_ldq/p r29, CNS__R29(r1)         	; restore gpr
        hw_ldq/p r30, CNS__R30(r1)         	; restore gpr
        bis     r31, r31, r31

        hw_ldq/p r4, CNS__R4(r1)           	; restore gpr
        hw_ldq/p r5, CNS__R5(r1)           	; restore gpr
        hw_ldq/p r6, CNS__R6(r1)           	; restore gpr
        hw_ldq/p r7, CNS__R7(r1)           	; restore gpr

        hw_ldq/p r20, CNS__R20(r1)         	; restore gpr
        hw_ldq/p r21, CNS__R21(r1)         	; restore gpr
        hw_ldq/p r22, CNS__R22(r1)         	; restore gpr
        hw_ldq/p r23, CNS__R23(r1)         	; restore gpr
;
; Now turn shadow mode back on.
;
        bis     r2, #<2@EV6__I_CTL__SDE__S>, r2 ; enable sde
        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) write i_ctl
        bis     r31, r31, r31
        bis     r31, r31, r31

        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) stall outside IQ
        bis     r31, r31, r31
        bis     r31, r31, r31
        bis     r31, r31, r31

        bis     r0, r0, r0                      ; buffer block 1
        bis     r31, r31, r31
        bis     r31, r31, r31
        bis     r31, r31, r31

        bis     r0, r0, r0                      ; buffer block 2
        bis     r31, r31, r31
        bis     r31, r31, r31
        bis     r31, r31, r31

        bis     r0, r0, r0                      ; buffer block 3
        bis     r31, r31, r31
        bis     r31, r31, r31
        bis     r31, r31, r31

        bis     r1, #0, p4                      ; impure pointer
        hw_ldq/p r0, CNS__R0(p4)           	; restore r0
        hw_ldq/p r1, CNS__R1(p4)           	; restore r1
        hw_ldq/p r2, CNS__R2(p4)           	; restore r2

        hw_ldq/p r3, CNS__R3(p4)           	; restore r3
        hw_ret_stall  (p23)                     ; return

  .endc						; if/iff eq ev6_p1

        END_CALL_PAL
.endc						; if ne clrmap

.if ne ev6_p1
    .if ne fp_count

;+
; CALL_PAL__FP_COUNT
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0 <- fp counter
;
;-

PAL_FUNC__00AD = ^xAD

	START_CALL_PAL <00AD>

	hw_ldq/p r0, PT__RSV_FOR_PAL(p_temp)	; read fp counter
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return to user

	END_CALL_PAL

    .endc
.endc


.if df pal$quilt_offset
;+
; CALL_PAL__QUILT -- PALcode for Quilt instruction
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;
; Function:
;	This pal call is used by quilt to instrument console images.  Quilt
;	copies the console image into a cloned area, and for subroutines that
;	are instrumented, replaces each instruction with a pal call to this
;	routine.  When this routine gets control, it replaces the instruction
;	from the cloned area.  The instruction to replace with is obtained by
;	adding the pc to an offset (located in pal common area at +40).
;
;	If we're running the console firmware, swap in the real
;	instruction for the quiltpoint. Otherwise, it's an illegal
;	opcode exception.
;
; Exit state:
;	(p23)		original opcode
;	p23		pc of the new instruction
;-
PAL_FUNC__QUILTPOINT = ^x3c

	START_CALL_PAL <QUILTPOINT>

	subq	p23, #4, p23			; adjust pc

	;
	; If we are in console mode, let this opcode work. Otherwise,
	; fail the opcode.
	;
	
	bge	p_misc, call_pal__quilt_fail

	;+
	; get the real instruction
	;-
	lda	p4, pal$quilt_offset(r31)	; quilt offset adr
	hw_ldq/p p4, (p4)
	addq	p23, p4, p4			; address of cloned instruction
	hw_ldl/p p5, (p4)			; p5 = cloned instruction
	cmpeq	p5, #PAL_FUNC__QUILTPOINT, p4	; check for quiltpoint
	cmovne	p4, r31, p5			; if it is, use a halt (00)

	hw_stl/p p5, (p23)			; restore instruction

	;+
	; The instruction stream has been modified,
	; synchronize by flushing the cache.
	; (cloned from IMB)
	;-
	hw_mtpr	r31, EV6__IC_FLUSH		; (4,0L) flush the icache
	bne	r31, .				; pvc #24 -- separate flush and stall
	hw_ret_stall (p23)			; return with stall

call_pal__quilt_fail:
	lda	p4, SCB__OPCDEC(r31)		; scb
	hw_stq/p p23, PT__FAULT_PC(p_temp)	; store nextpc
	hw_stq/p p4, PT__FAULT_SCB(p_temp)	; store scb
	br	r31, trap__post_km		; post

	END_CALL_PAL
.endc

;+
; CALL_PAL__CLRFEN
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The clear floating-point enable (CLRFEN) instruction writes a
;	zero to the floating-point enable register and to the PCB
;	at offset (PCBB+FEN)<0>.
;
;	FEN 	   <- 0
;	(HWPCB+56) <- FEN
;-
	START_CALL_PAL <CLRFEN>

	hw_ldq/p p4, PT__PCBB(p_temp)		; get PCBB
	hw_mtpr	r31, EV6__FPE			; (4,0L) write new fpe
	hw_stl/p r31, PCB__FEN(p4)		; store cleared FEN in PCB
	bis	r31, r31, r31

	hw_mtpr	r31, EV6__FPE			; (4,0L) force retire
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	CONT_CALL_PAL <CLRFEN>

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

.if ne ev6_p1
	hw_ldq/p p5, PT__IMPURE(p_temp)		; get impure pointer
	hw_stq/p r31, CNS__FPE_STATE(p5)	; clear FPE state	
.endc

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

	GOTO_FREE_CODE

;+
; call_pal__post_xm
;
; Entry:
;	Entered from CHME, CHMS, CHMU to post a trap. Build a
;	stack frame on selected mode stack called from any
;	mode.
;
; Current state:
;	p5		old mode
;	p6		new mode
;	p20		1 => mode hasn't changed
;	p23		nextpc
;	PT__FAULT_PC	nextpc
;	PT__FAULT_SCB	SCB vector
;	PT__CH_MODE	old mode
;	PT__CH_SP	old sp
;
;-
ASSUME EV6__PS__CM__S eq 3
xm_cm_offset = <call_pal__post_xm_sp - call_pal__post_xm_cm>

call_pal__post_xm:
	hw_mtpr	r31, <EV6__MM_STAT ! ^xF0>	; 1.39 settle the tb

	bne	p20, call_pal__post_xm_cont	; skip if mode hasn't changed
;
; Change to new mode. Save stack pointer and get new one.
; We are either going from supr/user to exec, or user to supr.
;
	br	p7, call_pal__post_xm_cm	; change mode
call_pal__post_xm_cm:
	addq	p7, #<xm_cm_offset+1>, p7	; set up to jump past in palmode
	hw_mtpr	p6, EV6__PS			; (4,0L) change mode
	bsr	r31, .				; push prediction stack
	PVC_JSR post_xm				; synch up
	hw_ret_stall (p7)			; pop prediction stack
	PVC_JSR post_xm, dest=1

;
; Swap stacks. Put new mode in p_misc.
;
call_pal__post_xm_sp:
	hw_ldq/p p20, PT__PCBB(p_temp)		; get PCBB
	addq	p20, p6, p7			; point to new mode sp
	addq	p20, p5, p20			; point to old mode sp
	hw_stq/p r30, PCB__KSP(p20)		; save old mode sp
	hw_ldq/p r30, PCB__KSP(p7)		; get new mode sp

	bic	p_misc,#<3@P_MISC__CM__S>, p_misc	; clear old mode
	bis	p_misc, p6, p_misc			; update mode
;
; Enter here from call_pal__chmu.
; Round down stack and write to it. First store can miss/fault.
; The others are in the same page, so we're all set after that.
;
; Current state:
;	p_misc		updated with new mode
;
;	PT__FAULT_PC	nextpc
;	PT__FAULT_SCB	SCB vector
;	PT__CH_MODE	old mode
;	PT__CH_SP	old sp
;
call_pal__post_xm_cont:
	bic	r30, #63, p20				; round down stack

call_pal__post_xm_store:
	stq	r2, <<FRM__R2-64>&^xFFFF>(p20)	; write R2 to the stack
	stq	r3, <<FRM__R3-64>&^xFFFF>(p20)	; write R3 to the stack
;
; Hack alert. We don't have visibility to r4-r7 because they are in our
; shadow range. So we need to peek under the covers.
;
; Current state:
;	r2	available
;	r3	available
;
; Get some scratch space. Then save p20 to r0 so we can access the stack.
; Turn off shadow mode. Do the stores. Then turn shadow mode back on and
; restore the scratch space.
;-
	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_stq/p r0, PT__R0(p_temp)		; save r0
	bis	p20, r31, r0			; save stack value
	hw_mfpr	r3, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r3, #<1@EV6__I_CTL__SDE7__S>, r3; zap sde

	hw_mtpr	r3, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r3, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	stq	r4, <<FRM__R4-64>&^xFFFF>(r0)	; write R4 to the stack
	stq	r5, <<FRM__R5-64>&^xFFFF>(r0)	; write R5 to the stack
	stq	r6, <<FRM__R6-64>&^xFFFF>(r0)	; write R6 to the stack
	stq	r7, <<FRM__R7-64>&^xFFFF>(r0)	; write R7 to the stack

	bis	r3, #<1@EV6__I_CTL__SDE7__S>, r3; or in sde
	hw_mtpr	r3, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r3, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_ldq/p r0, PT__R0(p_temp)		; restore r0
;
; Now back to business
;
	hw_ldq/p p4, PT__SCBB(p_temp)		; get SCBB
	hw_ldq/p p6, PT__FAULT_SCB(p_temp)	; get SCB offset
	hw_ldq/p p5, PT__CH_MODE(p_temp)	; get old mode back

	zap	p_misc, #^xFC, p7			; reconstruct old ps
	bic	p7, #<3@P_MISC__CM__S>, p7		; clear new mode
	bis	p7, p5, p7				; or back in old mode
	bic	p_misc, #<3@P_MISC__SW__S>, p_misc	; now can clear new SW

	addq	p4, p6, p4			; get address of vector
	hw_ldq/p r2, 0(p4)			; get SCBV
	hw_ldq/p r3, 8(p4)			; get SCBP

	and	r30, #63, p4			; isolate SP unalign bits
	subq	p20, #64, r30			; decrement stack pointer
	sll	p4, #PS__SP_ALIGN__S, p4	; get alignment bits in place
	bis	p7, p4, p7			; or alignment into old PS

	hw_ldq/p p6, PT__FAULT_PC(p_temp)	; get fault PC
	bic	r2, #3, r2			; clean new pc
	stq	p7, FRM__PS(r30)		; write old PS to the stack
	stq	p6, FRM__PC(r30)		; write old PC to the stack

	hw_ret_stall (r2)			; return (stall for pvc)

	END_FREE_CODE


